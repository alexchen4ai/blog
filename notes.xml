<?xml version="1.0" encoding="UTF-8"?>
<rss  xmlns:atom="http://www.w3.org/2005/Atom" 
      xmlns:media="http://search.yahoo.com/mrss/" 
      xmlns:content="http://purl.org/rss/1.0/modules/content/" 
      xmlns:dc="http://purl.org/dc/elements/1.1/" 
      version="2.0">
<channel>
<title>Alex Chen&#39;s Blog</title>
<link>https://alexchen4ai.github.io/blog/notes.html</link>
<atom:link href="https://alexchen4ai.github.io/blog/notes.xml" rel="self" type="application/rss+xml"/>
<description>Personal summaries and insights gathered from reading various research papers and articles.</description>
<generator>quarto-1.4.549</generator>
<lastBuildDate>Sat, 20 Apr 2024 07:00:00 GMT</lastBuildDate>
<item>
  <title>Optimization for Inference of Large Language Model</title>
  <dc:creator>Alex Chen</dc:creator>
  <link>https://alexchen4ai.github.io/blog/notes/Large Language Model/inference_optimize.html</link>
  <description><![CDATA[ 





<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Tip
</div>
</div>
<div class="callout-body-container callout-body">
<p>To run the language model faster and especially on the edge devices, we need to optimize the model. This optimization can be done in different ways. In this article, we will discuss some of the optimization techniques. In this blog, I will introduce different methods and the existing solutions to enable the language model to run faster. Note that my focus is for the on-device language model.</p>
</div>
</div>
<section id="overview-of-the-optimization" class="level2">
<h2 class="anchored" data-anchor-id="overview-of-the-optimization">Overview of the optimization</h2>
<p>To optimize the inference for language model, we mainly have the following methods:</p>
<ol type="1">
<li><strong>Quantization</strong>: Quantization is the process of reducing the precision of the weights and activations of the model. This reduces the memory footprint and increases the speed of the model.</li>
<li><strong>Pruning</strong>: Pruning is the process of removing the weights that are close to zero. This reduces the number of parameters in the model and increases the speed of the model.</li>
<li><strong>Lower level implementation</strong>: Implementing the model in a lower level language like C++ or Rust can increase the speed of the model.</li>
<li><strong>KV Cache</strong>: Key-Value cache is a technique to cache the intermediate results of the model. This reduces the computation and increases the speed of the model. For some certain devices, we may need to support the KV cache specially.</li>
<li><strong>Optimization based on hardware</strong>: Like the flash attention for NVIDIA GPU, we can optimize the model based on the hardware. The main method would be to use the memory-access pattern method to optimize the model.</li>
</ol>
</section>
<section id="quantization" class="level2">
<h2 class="anchored" data-anchor-id="quantization">Quantization</h2>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Tip
</div>
</div>
<div class="callout-body-container callout-body">
<p>Quantization is a model compression technique that converts the weights and activations within an LLM from a high-precision data representation to a lower-precision data representation, i.e., from a data type that can hold more information to one that holds less. A typical example of this is the conversion of data from a <code>32-bit</code> floating-point number (<code>FP32</code>) to an <code>8-bit</code> or <code>4-bit</code> integer (<code>INT4</code> or <code>INT8</code>). A good blog from internet is <a href="https://symbl.ai/developers/blog/a-guide-to-quantization-in-llms/">here</a>. We note that the conversion will decrease the memory and disk usage considerably. We note that for real calculation, we still need to <strong>dequantize</strong> the data to the original data type like <code>float32</code> or <code>bfloat16</code>. The trick is that we only dequantize the data when we need to calculate the data while keeping the most of data in the quantized format. Therefore, we still save the memory and disk usage.</p>
</div>
</div>
<p>Let’s first revisit the representation of data in computer. We mainly study the <code>float32</code>, <code>float16</code> and <code>bfloat16</code> type.</p>
<ul>
<li><strong>float32</strong>: 32 bits. We have 1 bit for the sign, 8 bits for the exponent and 23 bits for the mantissa. To form a float number in computer, we need the sign, the number before the exponent and the exponent number over 2. For example, we have <img src="https://latex.codecogs.com/png.latex?6.75=+1.1011%5Ctimes%202%5E2">. Thus, we can conclude that the range of the representation is between <img src="https://latex.codecogs.com/png.latex?1%5Ctimes%2010%5E%7B-38%7D"> and <img src="https://latex.codecogs.com/png.latex?3%5Ctimes%2010%5E%7B38%7D"> (you can add sign freely, though).</li>
<li><strong>float16</strong>: 16 bits. We have 1 bit for the sign, 5 bits for the exponent and 10 bits for the mantissa. The range of the representation is between <img src="https://latex.codecogs.com/png.latex?6%5Ctimes%2010%5E%7B-8%7D"> and <img src="https://latex.codecogs.com/png.latex?6%5Ctimes%2010%5E%7B4%7D">.</li>
<li><strong>bfloat16</strong>: 16 bits. We have 1 bit for the sign, 8 bits for the exponent and 7 bits for the mantissa. The range of the representation is between <img src="https://latex.codecogs.com/png.latex?1%5Ctimes%2010%5E%7B-38%7D"> and <img src="https://latex.codecogs.com/png.latex?3%5Ctimes%2010%5E%7B38%7D">.</li>
</ul>
<p>We can see that <code>float16</code> and <code>bfloat16</code> take up the same memory space. But they are different in the bits allocation. The <code>float16</code> has better precision than <code>bfloat16</code>, but the <code>bfloat16</code> has better range than <code>float16</code>. For deep neural network, we may need to consider the use of the <code>bfloat16</code> type since the range is more important than the precision for the deep neural network. The common quantization type are <code>INT8</code> and <code>INT4</code>. Note that <code>INT8</code> and <code>INT4</code> can only represent the integer numbers, not for the float numbers. Thus, <code>INT8</code> can only represent the numbers between <img src="https://latex.codecogs.com/png.latex?-128"> and <img src="https://latex.codecogs.com/png.latex?127">, and <code>INT4</code> can only represent the numbers between <img src="https://latex.codecogs.com/png.latex?-8"> and <img src="https://latex.codecogs.com/png.latex?7">.</p>
<p>We use the <em>affine quantization scheme</em> to convert the model:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0Ax_q%20=%20%5Coperatorname%7Bround%7D%5Cleft(x/S%20+%20Z%5Cright)%0A"></p>
<p>where we have: - <img src="https://latex.codecogs.com/png.latex?x_q">: the quantized value - <img src="https://latex.codecogs.com/png.latex?x">: the original value - <img src="https://latex.codecogs.com/png.latex?S">: the scale factor - <img src="https://latex.codecogs.com/png.latex?Z">: the zero point - <img src="https://latex.codecogs.com/png.latex?%5Coperatorname%7Bround%7D">: the rounding function.</p>
<p>Usually, we will set multiple blocks to quantize the model. It means that we need multiple scale factors and zero points. Note that not all layers are quantized. For some important layers, we still consider the use of the float32 type.</p>
<p>For LLM quantization, we have two different methods called post-training quantization and quantization-aware training. If we finally use the quantization model, quantization-aware training is better.</p>
<section id="exisiting-solutions" class="level3">
<h3 class="anchored" data-anchor-id="exisiting-solutions">Exisiting solutions</h3>
<p>We can use quantization library provied in huggingface transformers. For more foundamental optimization, we should consider to use <code>GGML</code> (GPT-Generated Model Language) and <code>GGUF</code> (GPT-Generated Unified Format). For on-device deployment, we should consider the usage of <code>GGUF</code> since it is more efficient. Refer to <a href="https://github.com/ggerganov/llama.cpp">github</a> to use it. We can consider another library called <a href="https://github.com/ollama/ollama">ollama</a> which is built based on the llama cpp.</p>


</section>
</section>

 ]]></description>
  <category>Large Language Models</category>
  <guid>https://alexchen4ai.github.io/blog/notes/Large Language Model/inference_optimize.html</guid>
  <pubDate>Sat, 20 Apr 2024 07:00:00 GMT</pubDate>
</item>
<item>
  <title>Optimization in machine learning</title>
  <dc:creator>Alex Chen</dc:creator>
  <link>https://alexchen4ai.github.io/blog/notes/Math Theories/ml_optimizer.html</link>
  <description><![CDATA[ 





<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Tip
</div>
</div>
<div class="callout-body-container callout-body">
<p>In this paper, we will give a thorough of optimizer used in the machine learning. Specially, the pytorch model will be used to illustrate the code implementation of the optimizer. We will also explain some latest optimizer effort related to the large language model.</p>
</div>
</div>
<section id="heuristic-for-the-optimization" class="level2">
<h2 class="anchored" data-anchor-id="heuristic-for-the-optimization">Heuristic for the optimization</h2>
<p>The machine learning training starts from the convex optimization problems. In deep learning, we usually deal with the tensor operations. Thus, the optimization is about the multivariate calculus. For this problem, we know that the derivative is in terms of the vector and matrix. To know about the concept of the optimization, we starts with the 1D optimization.</p>
<section id="d-optimization" class="level3">
<h3 class="anchored" data-anchor-id="d-optimization">1D optimization</h3>
<p>Suppose we have a nonlinear function to be solved: <img src="https://latex.codecogs.com/png.latex?%0Af(x)%20=%200%0A"></p>
<p>Now, we need to find the solution. And we can use the tangent line to approximate the solution and then use the Newton’s method to solve the problem. The Newton’s method is given by:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0Ax_%7Bn+1%7D%20=%20x_n%20-%20%5Cfrac%7Bf(x_n)%7D%7Bf'(x_n)%7D,%5Cquad%20f'(x_n)%20%5Cneq%200%0A"></p>
<p>After some iterations, we can get close to our solution. In terms of the convex optimization problem, the local minimum has the properties of <img src="https://latex.codecogs.com/png.latex?f'(x)%20=%200"> and <img src="https://latex.codecogs.com/png.latex?f''(x)%20%3E%200">. Thus, we can use the Newton’s method to solve the optimization problem:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0Ax_%7Bn+1%7D%20=%20x_n%20-%20%5Calpha%5Cfrac%7Bf'(x_n)%7D%7Bf''(x_n)%7D%0A"></p>
<p>Here, we have two important concepts. The path is defined by the path <img src="https://latex.codecogs.com/png.latex?%5C%7Bx_0,x_1,...%5C%7D">. The magnitude of the step from <img src="https://latex.codecogs.com/png.latex?x_n"> to <img src="https://latex.codecogs.com/png.latex?x_%7Bn+1%7D"> is defined by the value of</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cleft%7C%5Calpha%20%5Cfrac%7Bf'(x)%7D%7Bf''(x)%7D%5Cright%7C.%0A"></p>
</section>
<section id="tensor-optimization" class="level3">
<h3 class="anchored" data-anchor-id="tensor-optimization">Tensor optimization</h3>
<p>In the case of tensors, the variables becomes the tensors and matrices, and we can note the gradient and the Hessian matrix. The gradient is the first order derivative and the Hessian matrix is the second order derivative. Suppose that we need to solve the following optimization problems:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cmin_%7B%5Cboldsymbol%5Ctheta%7D%20F(%5Cboldsymbol%5Ctheta).%0A"></p>
<p>Then, the gradient is given by:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cnabla_%5Cboldsymbol%7B%5Ctheta%7D%20F(%5Cboldsymbol%5Ctheta)%20=%20%5Cleft%5B%5Cfrac%7B%5Cpartial%20F(%5Cboldsymbol%5Ctheta)%7D%7B%5Cpartial%20%5Ctheta_1%7D,%5Cfrac%7B%5Cpartial%20F(%5Cboldsymbol%5Ctheta)%7D%7B%5Cpartial%20%5Ctheta_2%7D,...,%5Cfrac%7B%5Cpartial%20F(%5Cboldsymbol%5Ctheta)%7D%7B%5Cpartial%20%5Ctheta_n%7D%5Cright%5D.%0A"></p>
<p>If the <img src="https://latex.codecogs.com/png.latex?%5Cboldsymbol%5Ctheta"> is a matrix of size <img src="https://latex.codecogs.com/png.latex?m%5Ctimes%20n">, then the gradient is a matrix of size <img src="https://latex.codecogs.com/png.latex?m%5Ctimes%20n">. And we have</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cnabla_%5Cboldsymbol%7B%5Ctheta%7D%20F(%5Cboldsymbol%5Ctheta)%20=%0A%5Cbegin%7Bbmatrix%7D%0A%5Cfrac%7B%5Cpartial%20F(%5Cboldsymbol%5Ctheta)%7D%7B%5Cpartial%20%5Ctheta_%7B11%7D%7D%20&amp;%20%5Cfrac%7B%5Cpartial%20F(%5Cboldsymbol%5Ctheta)%7D%7B%5Cpartial%20%5Ctheta_%7B12%7D%7D%20&amp;%20%5Ccdots%20&amp;%20%5Cfrac%7B%5Cpartial%20F(%5Cboldsymbol%5Ctheta)%7D%7B%5Cpartial%20%5Ctheta_%7B1n%7D%7D%5C%5C%0A%5Cfrac%7B%5Cpartial%20F(%5Cboldsymbol%5Ctheta)%7D%7B%5Cpartial%20%5Ctheta_%7B21%7D%7D%20&amp;%20%5Cfrac%7B%5Cpartial%20F(%5Cboldsymbol%5Ctheta)%7D%7B%5Cpartial%20%5Ctheta_%7B22%7D%7D%20&amp;%20%5Ccdots%20&amp;%20%5Cfrac%7B%5Cpartial%20F(%5Cboldsymbol%5Ctheta)%7D%7B%5Cpartial%20%5Ctheta_%7B2n%7D%7D%5C%5C%0A%5Cvdots%20&amp;%20%5Cvdots%20&amp;%20%5Cddots%20&amp;%20%5Cvdots%5C%5C%0A%5Cfrac%7B%5Cpartial%20F(%5Cboldsymbol%5Ctheta)%7D%7B%5Cpartial%20%5Ctheta_%7Bm1%7D%7D%20&amp;%20%5Cfrac%7B%5Cpartial%20F(%5Cboldsymbol%5Ctheta)%7D%7B%5Cpartial%20%5Ctheta_%7Bm2%7D%7D%20&amp;%20%5Ccdots%20&amp;%20%5Cfrac%7B%5Cpartial%20F(%5Cboldsymbol%5Ctheta)%7D%7B%5Cpartial%20%5Ctheta_%7Bmn%7D%7D%0A%5Cend%7Bbmatrix%7D.%0A"></p>
<p>After we construct the loss function, we can begin to search for the optimal solution. The most popular optimizer is the gradient descent. The gradient descent is given by:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cboldsymbol%5Ctheta_%7Bn+1%7D%20=%20%5Cboldsymbol%5Ctheta_n%20-%20%5Calpha%20%5Cnabla_%5Cboldsymbol%7B%5Ctheta%7D%20F(%5Cboldsymbol%5Ctheta_n).%0A"></p>
<p>Here, we have <img src="https://latex.codecogs.com/png.latex?%5Calpha"> as the learning rate. Note that what we use here is the first order derivative. It is not directly originated from the Newton’s method. The gradient descent is a first order optimization method. The second order optimization method is the Newton’s method. The Newton’s method is given by:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cboldsymbol%7B%5Ctheta%7D_%7Bn+1%7D=%5Cboldsymbol%7B%5Ctheta%7D_n-%5Calpha%5Cleft%5B%5Cnabla%5E2%20F%5Cleft(%5Cboldsymbol%7B%5Ctheta%7D_n%5Cright)%5Cright%5D%5E%7B-1%7D%20%5Cnabla%20F%5Cleft(%5Cboldsymbol%7B%5Ctheta%7D_n%5Cright).%0A"></p>
<p>It is usually faster for the convergence, but it is more expensive for the computation in each step. Thus, we need to have a tradeoff. We can also consider to use quasi-Newton’s method, and there are algorithms like BFGS and L-BFGS. The BFGS is the Broyden-Fletcher-Goldfarb-Shanno algorithm. The L-BFGS is the limited memory BFGS.</p>
</section>
</section>
<section id="the-adam-optimizer" class="level2">
<h2 class="anchored" data-anchor-id="the-adam-optimizer">The Adam optimizer</h2>
<p>There are many important concept of the Adam optimizer. We first introduce the concepts:</p>
<ul>
<li>First momentum: This is the mean of the gradients. Instead of using the current gradient, we choose to use the mean of the gradients. And we use the exponential moving average to calculate the mean;</li>
<li>Second momentum: This is the variance of the gradients. The second moment in Adam captures the variance of the gradients, providing information about their variability. This is used to adaptively adjust the learning rate for each parameter, allowing parameters with large gradients to have smaller updates and vice versa. This adaptive learning rate mechanism helps in stabilizing the optimization process, especially in the presence of noisy gradients or sparse data.</li>
</ul>
<p>Now for one step of the Adam optimizer, we have the following steps: 1. Compute the gradient, and this can be done by pytorch autograd already; 2. First momentum: <img src="https://latex.codecogs.com/png.latex?%0A%20%20%20%20m_t%20=%20%5Cbeta_1%20m_%7Bt-1%7D%20+%20(1-%5Cbeta_1)g_t%0A%20%20%20%20"> 3. Second momentum: <img src="https://latex.codecogs.com/png.latex?%0A%20%20%20%20v_t%20=%20%5Cbeta_2%20v_%7Bt-1%7D%20+%20(1-%5Cbeta_2)g_t%5E2%0A%20%20%20%20"> 4. Bias correction: <img src="https://latex.codecogs.com/png.latex?%0A%20%20%20%20%5Chat%7Bm%7D_t%20=%20%5Cfrac%7Bm_t%7D%7B1-%5Cbeta_1%5Et%7D%0A%20%20%20%20"> <img src="https://latex.codecogs.com/png.latex?%0A%20%20%20%20%5Chat%7Bv%7D_t%20=%20%5Cfrac%7Bv_t%7D%7B1-%5Cbeta_2%5Et%7D%0A%20%20%20%20"> 5. Update the parameters: <img src="https://latex.codecogs.com/png.latex?%0A%20%20%20%20%5Ctheta_%7Bt+1%7D%20=%20%5Ctheta_t%20-%20%5Calpha%20%5Cfrac%7B%5Chat%7Bm%7D_t%7D%7B%5Csqrt%7B%5Chat%7Bv%7D_t%7D+%5Cepsilon%7D%0A%20%20%20%20"></p>
<p>And here the hyperparameters are:</p>
<ul>
<li><img src="https://latex.codecogs.com/png.latex?%5Calpha">: the learning rate;</li>
<li><img src="https://latex.codecogs.com/png.latex?%5Cbeta_1">: the parameter for the first momentum;</li>
<li><img src="https://latex.codecogs.com/png.latex?%5Cbeta_2">: the parameter for the second momentum;</li>
<li><img src="https://latex.codecogs.com/png.latex?%5Cepsilon">: the small value to avoid the division by zero.</li>
</ul>
</section>
<section id="pytorch-implementation" class="level2">
<h2 class="anchored" data-anchor-id="pytorch-implementation">Pytorch implementation</h2>
<section id="the-autograd-in-pytorch" class="level3">
<h3 class="anchored" data-anchor-id="the-autograd-in-pytorch">The autograd in pytorch</h3>
<p>To learn how the pytorch optimizer works, we should be aware that the pytorch can use the automatic differentiation to compute the gradient. The automatic differentiation is given by the backpropagation. Thus, we don’t need bother to calculate the gradient, analytically. Note that all the optimizers uses this technique. A very simple example would be:</p>
<div class="sourceCode" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> torch</span>
<span id="cb1-2"></span>
<span id="cb1-3">x <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> torch.tensor([<span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">1.0</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">2.0</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">3.0</span>], requires_grad<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>)</span>
<span id="cb1-4">y <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> x <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span></span>
<span id="cb1-5">z <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> y <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> y <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span></span>
<span id="cb1-6">out <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> z.mean()</span>
<span id="cb1-7"></span>
<span id="cb1-8">out.backward()</span>
<span id="cb1-9"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(x.grad)</span></code></pre></div>
</section>
<section id="the-adamw-optimizer-in-pytorch" class="level3">
<h3 class="anchored" data-anchor-id="the-adamw-optimizer-in-pytorch">The AdamW optimizer in pytorch</h3>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Tip
</div>
</div>
<div class="callout-body-container callout-body">
<p>AdamW and Adam are different, since they have different weight decay procedure. AdamW’s weight decay is not scaled by adaptative learning rate.</p>
</div>
</div>
<p>Refer to the <a href="https://pytorch.org/docs/stable/generated/torch.optim.Adam.html">official documentation</a>. The math formulas to be implemented are:</p>
<img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Baligned%7D%0A%20%20%20%20&amp;%5Crule%7B180mm%7D%7B0.4pt%7D%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%5C%5C%0A%20%20%20%20&amp;%5Ctextbf%7Binput%7D%20%20%20%20%20%20:%20%5Cgamma%20%5Ctext%7B(lr)%7D,%20%5C:%20%5Cbeta_1,%20%5Cbeta_2%0A%20%20%20%20%20%20%20%20%5Ctext%7B(betas)%7D,%20%5C:%20%5Ctheta_0%20%5Ctext%7B(params)%7D,%20%5C:%20f(%5Ctheta)%20%5Ctext%7B(objective)%7D,%0A%20%20%20%20%20%20%20%20%5C:%20%5Cepsilon%20%5Ctext%7B%20(epsilon)%7D%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%5C%5C%0A%20%20%20%20&amp;%5Chspace%7B13mm%7D%20%20%20%20%20%20%5Clambda%20%5Ctext%7B(weight%20decay)%7D%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%5C%5C%0A%20%20%20%20&amp;%5Ctextbf%7Binitialize%7D%20:%20m_0%20%5Cleftarrow%200%20%5Ctext%7B%20(first%20moment)%7D,%20v_0%20%5Cleftarrow%200%0A%20%20%20%20%20%20%20%20%5Ctext%7B%20(%20second%20moment)%7D%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%5C%5C%5B-1.ex%5D%0A%20%20%20%20&amp;%5Crule%7B180mm%7D%7B0.4pt%7D%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%5C%5C%0A%20%20%20%20&amp;%5Ctextbf%7Bfor%7D%20%5C:%20t=1%20%5C:%20%5Ctextbf%7Bto%7D%20%5C:%20%5Cldots%20%5C:%20%5Ctextbf%7Bdo%7D%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%5C%5C%0A%20%20%20%20&amp;%5Chspace%7B5mm%7D%5Ctext%7BGet%20a%20minibatch%20and%20calculate%20the%20loss%20$f_t(%5Ctheta_%7Bt-1%7D)$%7D%20%20%20%20%20%20%20%20%20%20%20%5C%5C%0A%20%20%20%20&amp;%5Chspace%7B5mm%7Dg_t%20%20%20%20%20%20%20%20%20%20%20%5Cleftarrow%20%20%20%5Cnabla_%7B%5Ctheta%7D%20f_t%20(%5Ctheta_%7Bt-1%7D)%20%20%20%20%20%20%20%20%20%20%20%5C%5C%0A%20%20%20%20&amp;%5Chspace%7B5mm%7D%20%5Ctheta_t%20%5Cleftarrow%20%5Ctheta_%7Bt-1%7D%20-%20%5Cgamma%20%5Clambda%20%5Ctheta_%7Bt-1%7D%20%5Cquad%20%5Ctextit%7Bweight%20decay%7D%20%20%20%20%20%20%20%5C%5C%0A%20%20%20%20&amp;%5Chspace%7B5mm%7Dm_t%20%20%20%20%20%20%20%20%20%20%20%5Cleftarrow%20%20%20%5Cbeta_1%20m_%7Bt-1%7D%20+%20(1%20-%20%5Cbeta_1)%20g_t%20%20%20%20%20%20%20%20%20%20%5C%5C%0A%20%20%20%20&amp;%5Chspace%7B5mm%7Dv_t%20%20%20%20%20%20%20%20%20%20%20%5Cleftarrow%20%20%20%5Cbeta_2%20v_%7Bt-1%7D%20+%20(1-%5Cbeta_2)%20g%5E2_t%20%20%20%20%20%20%20%20%20%20%5C%5C%0A%20%20%20%20&amp;%5Chspace%7B5mm%7D%5Cwidehat%7Bm_t%7D%20%5Cleftarrow%20%20%20m_t/%5Cbig(1-%5Cbeta_1%5Et%20%5Cbig)%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%5C%5C%0A%20%20%20%20&amp;%5Chspace%7B5mm%7D%5Cwidehat%7Bv_t%7D%20%5Cleftarrow%20%20%20v_t/%5Cbig(1-%5Cbeta_2%5Et%20%5Cbig)%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%5C%5C%0A%20%20%20%20&amp;%5Chspace%7B5mm%7D%5Ctheta_t%20%5Cleftarrow%20%5Ctheta_t%20-%20%5Cgamma%20%5Cwidehat%7Bm_t%7D/%0A%20%20%20%20%20%20%20%20%5Cbig(%5Csqrt%7B%5Cwidehat%7Bv_t%7D%7D%20+%20%5Cepsilon%20%5Cbig)%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%5C%5C%0A%20%20%20%20&amp;%5Crule%7B180mm%7D%7B0.4pt%7D%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%5C%5C%5B-1.ex%5D%0A%20%20%20%20&amp;%5Cbf%7Breturn%7D%20%5C:%20%20%5Ctheta_t%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%5C%5C%5B-1.ex%5D%0A%20%20%20%20&amp;%5Crule%7B180mm%7D%7B0.4pt%7D%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%5C%5C%5B-1.ex%5D%0A%5Cend%7Baligned%7D">
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Tip
</div>
</div>
<div class="callout-body-container callout-body">
<p>The momentum is like the momentum in Physics, if we look at the directional arrow between different solution points. This first momentum will be served as the direction, and magnitude of the difference. The second momentum is like the variance of the gradients. It is used to adjust the learning rate for each parameter. When the gradient variance is large (this is suddenly a large variance, we decrease the learning rate, vice versa). The <img src="https://latex.codecogs.com/png.latex?%5Cepsilon"> is used to avoid the division by zero.</p>
<p>Bias corrections are needed for <img src="https://latex.codecogs.com/png.latex?m_t"> and <img src="https://latex.codecogs.com/png.latex?v_t"> in the Adam optimizer because they are initialized at 0, and they use exponential moving averages which are biased towards 0 at the start of training. This can cause the estimates to be too low at the beginning of optimization. Note that <img src="https://latex.codecogs.com/png.latex?%5Cbeta_1"> and <img src="https://latex.codecogs.com/png.latex?%5Cbeta_2"> are usually selected as very close to 1, in pytorch the default values are 0.9 and 0.999. The <img src="https://latex.codecogs.com/png.latex?%5Cepsilon"> is usually set to a small value like <img src="https://latex.codecogs.com/png.latex?10%5E%7B-8%7D">.</p>
</div>
</div>
<p>The associated code implementation without special setup would be:</p>
<div class="sourceCode" id="cb2" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">def</span> _single_tensor_adamw(</span>
<span id="cb2-2">    params: List[Tensor],</span>
<span id="cb2-3">    grads: List[Tensor],</span>
<span id="cb2-4">    exp_avgs: List[Tensor],</span>
<span id="cb2-5">    exp_avg_sqs: List[Tensor],</span>
<span id="cb2-6">    max_exp_avg_sqs: List[Tensor],</span>
<span id="cb2-7">    state_steps: List[Tensor],</span>
<span id="cb2-8">    grad_scale: Optional[Tensor],</span>
<span id="cb2-9">    found_inf: Optional[Tensor],</span>
<span id="cb2-10">    <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span>,</span>
<span id="cb2-11">    amsgrad: <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">bool</span>,</span>
<span id="cb2-12">    beta1: <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">float</span>,</span>
<span id="cb2-13">    beta2: <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">float</span>,</span>
<span id="cb2-14">    lr: Union[Tensor, <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">float</span>],</span>
<span id="cb2-15">    weight_decay: <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">float</span>,</span>
<span id="cb2-16">    eps: <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">float</span>,</span>
<span id="cb2-17">):</span>
<span id="cb2-18"></span>
<span id="cb2-19">    <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">assert</span> grad_scale <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">is</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span> <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">and</span> found_inf <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">is</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span></span>
<span id="cb2-20"></span>
<span id="cb2-21">    <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">if</span> torch.jit.is_scripting():</span>
<span id="cb2-22">        <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># this assert is due to JIT being dumb and not realizing that the ops below</span></span>
<span id="cb2-23">        <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># have overloads to handle both float and Tensor lrs, so we just assert it's</span></span>
<span id="cb2-24">        <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># a float since most people using JIT are using floats</span></span>
<span id="cb2-25">        <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">assert</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">isinstance</span>(lr, <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">float</span>)</span>
<span id="cb2-26"></span>
<span id="cb2-27">    <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">for</span> i, param <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">enumerate</span>(params):</span>
<span id="cb2-28">        grad <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> grads[i] <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">if</span> <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">not</span> maximize <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">else</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span>grads[i]</span>
<span id="cb2-29">        exp_avg <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> exp_avgs[i]</span>
<span id="cb2-30">        exp_avg_sq <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> exp_avg_sqs[i]</span>
<span id="cb2-31">        step_t <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> state_steps[i]</span>
<span id="cb2-32">        </span>
<span id="cb2-33">        <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># update step</span></span>
<span id="cb2-34">        step_t <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span></span>
<span id="cb2-35"></span>
<span id="cb2-36">        <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Perform stepweight decay</span></span>
<span id="cb2-37">        param.mul_(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> lr <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> weight_decay)</span>
<span id="cb2-38"></span>
<span id="cb2-39">        <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Decay the first and second moment running average coefficient</span></span>
<span id="cb2-40">        exp_avg.lerp_(grad, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> beta1)</span>
<span id="cb2-41">        exp_avg_sq.mul_(beta2).addcmul_(grad, grad, value<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> beta2)</span>
<span id="cb2-42"></span>
<span id="cb2-43">        step <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> _get_value(step_t)</span>
<span id="cb2-44"></span>
<span id="cb2-45">        bias_correction1 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> beta1 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">**</span> step</span>
<span id="cb2-46">        bias_correction2 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> beta2 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">**</span> step</span>
<span id="cb2-47"></span>
<span id="cb2-48">        step_size <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> lr <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span> bias_correction1</span>
<span id="cb2-49"></span>
<span id="cb2-50">        bias_correction2_sqrt <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> _dispatch_sqrt(bias_correction2)</span>
<span id="cb2-51"></span>
<span id="cb2-52">        <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">if</span> amsgrad:</span>
<span id="cb2-53">            <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Maintains the maximum of all 2nd moment running avg. till now</span></span>
<span id="cb2-54">            torch.maximum(max_exp_avg_sqs[i], exp_avg_sq, out<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>max_exp_avg_sqs[i])</span>
<span id="cb2-55"></span>
<span id="cb2-56">            <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Use the max. for normalizing running avg. of gradient</span></span>
<span id="cb2-57">            denom <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> (max_exp_avg_sqs[i].sqrt() <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span> bias_correction2_sqrt).add_(eps)</span>
<span id="cb2-58">        <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">else</span>:</span>
<span id="cb2-59">            denom <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> (exp_avg_sq.sqrt() <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span> bias_correction2_sqrt).add_(eps)</span>
<span id="cb2-60"></span>
<span id="cb2-61">        param.addcdiv_(exp_avg, denom, value<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=-</span>step_size)</span></code></pre></div>


</section>
</section>

 ]]></description>
  <category>Math Theories</category>
  <guid>https://alexchen4ai.github.io/blog/notes/Math Theories/ml_optimizer.html</guid>
  <pubDate>Fri, 15 Mar 2024 07:00:00 GMT</pubDate>
</item>
<item>
  <title>Large language model distributed training</title>
  <dc:creator>Alex Chen</dc:creator>
  <link>https://alexchen4ai.github.io/blog/notes/Large Language Model/llm_train.html</link>
  <description><![CDATA[ 





<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Tip
</div>
</div>
<div class="callout-body-container callout-body">
<p>The AWS sagemaker is a service to support the automatic training for the models. And the price is 1.5x of the normal elastic container. Thus, the distributed learning is important and expensive.</p>
</div>
</div>
<section id="distributed-learning-introduction-in-pytorch" class="level2">
<h2 class="anchored" data-anchor-id="distributed-learning-introduction-in-pytorch">Distributed learning introduction in <code>Pytorch</code></h2>
<p>We need to be aware of what kind of distributed learning we can use, and there are <code>DDP</code>, <code>RPC</code> and <code>Collective communication</code> from the <a href="https://pytorch.org/tutorials/beginner/dist_overview.html">pytorch documentation</a> (read the documentation for the detail).</p>
<section id="data-parallel" class="level3">
<h3 class="anchored" data-anchor-id="data-parallel">Data Parallel</h3>
<p>DistributedDataParallel is better than the DataParallel (DP), since DP is limited by the <a href="https://wiki.python.org/moin/GlobalInterpreterLock">GIL</a>. For <code>DP</code>, it is to split the dataset into multiple machine, and compute them then reduce them. Suppose you have a forward computation with batch size as 16, and the number of the GPU is 4. Then, you basically calculate batch size 4 in each GPU. To apply it, we just need to add a few code:</p>
<div class="sourceCode" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">if</span> torch.cuda.device_count() <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">&gt;</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>:</span>
<span id="cb1-2">    model <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> nn.DataParallel(model)</span></code></pre></div>
<p>We don’t need to do any other operation to let it run.</p>
</section>
<section id="distributed-data-parallel-ddp" class="level3">
<h3 class="anchored" data-anchor-id="distributed-data-parallel-ddp">Distributed Data Parallel (DDP)</h3>
<p>We need to use the specific module to let it work. This trick can overcome the <code>GIL</code>. A code example can be</p>
<div class="sourceCode" id="cb2" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> torch</span>
<span id="cb2-2"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> torch.distributed <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> dist</span>
<span id="cb2-3"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> torch.multiprocessing <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> mp</span>
<span id="cb2-4"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> torch.nn <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> nn</span>
<span id="cb2-5"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> torch.optim <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> optim</span>
<span id="cb2-6"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> os</span>
<span id="cb2-7"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> torch.nn.parallel <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> DistributedDataParallel <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> DDP</span>
<span id="cb2-8"></span>
<span id="cb2-9"><span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">class</span> SimpleCNN(nn.Module):</span>
<span id="cb2-10">    <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">pass</span> </span>
<span id="cb2-11"></span>
<span id="cb2-12"><span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">def</span> example(rank, world_size):</span>
<span id="cb2-13">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># create default process group, nccl means running on GPU</span></span>
<span id="cb2-14">    dist.init_process_group(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"nccl"</span>, rank<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>rank, world_size<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>world_size)</span>
<span id="cb2-15">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># create local model and move it to the current device (GPU/CPU)</span></span>
<span id="cb2-16">    model <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> SimpleCNN().to(rank)</span>
<span id="cb2-17">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># construct DDP model</span></span>
<span id="cb2-18">    ddp_model <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> DDP(model, device_ids<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>[rank])</span>
<span id="cb2-19">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># define loss function and optimizer</span></span>
<span id="cb2-20">    loss_fn <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> nn.CrossEntropyLoss()</span>
<span id="cb2-21">    optimizer <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> optim.SGD(ddp_model.parameters(), lr<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.001</span>)</span>
<span id="cb2-22"></span>
<span id="cb2-23">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># forward pass</span></span>
<span id="cb2-24">    outputs <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> ddp_model(torch.randn(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">64</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">28</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">28</span>).to(rank))  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Example input size for MNIST</span></span>
<span id="cb2-25">    labels <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> torch.randint(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10</span>, (<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">64</span>,)).to(rank)  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Example labels for 64 samples</span></span>
<span id="cb2-26">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># backward pass</span></span>
<span id="cb2-27">    loss_fn(outputs, labels).backward()</span>
<span id="cb2-28">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># update parameters</span></span>
<span id="cb2-29">    optimizer.step()</span>
<span id="cb2-30"></span>
<span id="cb2-31"><span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">def</span> main():</span>
<span id="cb2-32">    world_size <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span></span>
<span id="cb2-33">    mp.spawn(example,</span>
<span id="cb2-34">             args<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>(world_size,),</span>
<span id="cb2-35">             nprocs<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>world_size,</span>
<span id="cb2-36">             join<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>)</span>
<span id="cb2-37"></span>
<span id="cb2-38"><span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">if</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">__name__</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">==</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"__main__"</span>:</span>
<span id="cb2-39">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Environment variables for distributed training</span></span>
<span id="cb2-40">    os.environ[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"MASTER_ADDR"</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"localhost"</span></span>
<span id="cb2-41">    os.environ[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"MASTER_PORT"</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"29500"</span></span>
<span id="cb2-42">    main()</span></code></pre></div>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Tip
</div>
</div>
<div class="callout-body-container callout-body">
<p><code>rank</code> and <code>world_size</code> are two special concept in the distributed learning. When we launch multiple processes to learn the model, the total number of processes is <code>world_size</code>. For each process, we can define it as <code>rank</code>. You can imagine <code>rank</code> is like a small device, so we put the model or data to the <code>rank</code> like we put them in <code>cuda</code>.</p>
</div>
</div>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Tip
</div>
</div>
<div class="callout-body-container callout-body">
<p>We still need to define the env value for the distributed learning code, since the framework needs to setup a communication network.</p>
</div>
</div>
</section>
<section id="use-the-zeroredundancyoptimizer" class="level3">
<h3 class="anchored" data-anchor-id="use-the-zeroredundancyoptimizer">Use the ZeroRedundancyOptimizer</h3>
<p>Since some optimizer like <code>Adam</code> will keep many states, usually twice the model size, OOM error can occur. Therefore, we consider to deepspeed optimizer. In pytorch, it is already implemented!</p>
<div class="sourceCode" id="cb3" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> torch.distributed.optim <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> ZeroRedundancyOptimizer</span></code></pre></div>
<p>If we want to use it, just add a flag called <code>use_zero</code>:</p>
<div class="sourceCode" id="cb4" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">if</span> use_zero:</span>
<span id="cb4-2">    optimizer <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> ZeroRedundancyOptimizer(</span>
<span id="cb4-3">        ddp_model.parameters(),</span>
<span id="cb4-4">        optimizer_class<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>torch.optim.Adam,</span>
<span id="cb4-5">        lr<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.01</span></span>
<span id="cb4-6">    )</span>
<span id="cb4-7"><span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">else</span>:</span>
<span id="cb4-8">    optimizer <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> torch.optim.Adam(ddp_model.parameters(), lr<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.01</span>)</span></code></pre></div>
<p>This technique is mainly used to distribute the optimizer to multiple machine to avoid the OOM. All other code is similar to the DDP part.</p>
</section>
<section id="fully-sharded-data-parallel" class="level3">
<h3 class="anchored" data-anchor-id="fully-sharded-data-parallel">Fully sharded data parallel</h3>
<p>This FSDP will distribute the model and data across all process, and it is good especially for the model that can’t be fitted to one GPU. For the example script, refer to this <a href="https://pytorch.org/tutorials/intermediate/FSDP_tutorial.html">code example</a>.</p>
</section>
<section id="torchrun" class="level3">
<h3 class="anchored" data-anchor-id="torchrun">torchrun</h3>
<p><code>torchrun</code> is a method to execute the distributed learning in a way of elastic running. It can deal with the case that some node may fail. And it can handle the restart automatically.</p>
<p>We should set the checkpoint so that we will at most lose one epoch of training. The code is like</p>
<pre><code>def main():
     args = parse_args(sys.argv[1:])
     state = load_checkpoint(args.checkpoint_path)
     initialize(state)

     # torch.distributed.run ensures that this will work
     # by exporting all the env vars needed to initialize the process group
     torch.distributed.init_process_group(backend=args.backend)

     for i in range(state.epoch, state.total_num_epochs)
          for batch in iter(state.dataset)
              train(batch, state.model)

          state.epoch += 1
          save_checkpoint(state)</code></pre>
<p>For more usage about the <code>torchrun</code>, refer to <a href="https://pytorch.org/docs/stable/elastic/train_script.html">this page</a>. Here is another script that can be runned by the <a href="https://github.com/pytorch/elastic/blob/master/examples/imagenet/main.py">torchrun command</a>. If we want to run the <code>torchrun</code>, we should firstly make sure the script can adapt to the <code>torchrun</code>. The code is to run it is:</p>
<div class="sourceCode" id="cb6" style="background: #f1f3f5;"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb6-1"><span class="ex" style="color: null;
background-color: null;
font-style: inherit;">torchrun</span></span>
<span id="cb6-2">   <span class="ex" style="color: null;
background-color: null;
font-style: inherit;">--nnodes=NUM_NODES</span></span>
<span id="cb6-3">   <span class="ex" style="color: null;
background-color: null;
font-style: inherit;">--nproc-per-node=TRAINERS_PER_NODE</span></span>
<span id="cb6-4">   <span class="ex" style="color: null;
background-color: null;
font-style: inherit;">--max-restarts=NUM_ALLOWED_FAILURES</span></span>
<span id="cb6-5">   <span class="ex" style="color: null;
background-color: null;
font-style: inherit;">--rdzv-id=JOB_ID</span></span>
<span id="cb6-6">   <span class="ex" style="color: null;
background-color: null;
font-style: inherit;">--rdzv-backend=c10d</span></span>
<span id="cb6-7">   <span class="ex" style="color: null;
background-color: null;
font-style: inherit;">--rdzv-endpoint=HOST_NODE_ADDR</span></span>
<span id="cb6-8">   <span class="ex" style="color: null;
background-color: null;
font-style: inherit;">YOUR_TRAINING_SCRIPT.py</span> <span class="er" style="color: #AD0000;
background-color: null;
font-style: inherit;">(</span><span class="ex" style="color: null;
background-color: null;
font-style: inherit;">--arg1</span> ... train script args...<span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">)</span></span></code></pre></div>
<p>For more complicated case, pytorch also provide some solution to use multiple container with communication by <a href="https://github.com/pytorch/elastic/blob/master/examples/imagenet/main.py">docker example</a> or <a href="https://github.com/pytorch/elastic/tree/master/kubernetes">k8s example</a>.</p>


</section>
</section>

 ]]></description>
  <category>Large Language Models</category>
  <guid>https://alexchen4ai.github.io/blog/notes/Large Language Model/llm_train.html</guid>
  <pubDate>Sat, 02 Mar 2024 08:00:00 GMT</pubDate>
</item>
<item>
  <title>Complex analysis for machine learning</title>
  <dc:creator>Alex Chen</dc:creator>
  <link>https://alexchen4ai.github.io/blog/notes/Math Theories/complexanalysis.html</link>
  <description><![CDATA[ 





<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Tip
</div>
</div>
<div class="callout-body-container callout-body">
<p>The real functional analysis is used a lot in the ML. There is also the case where the complex analysis is used. We are not talking about the very detail of the complexy analysis, we just mention its application in the era of ML.</p>
</div>
</div>
<section id="basics-or-formulas-required." class="level2">
<h2 class="anchored" data-anchor-id="basics-or-formulas-required.">Basics or formulas required.</h2>
<p>In this section, we just mention some critical formulas. In the case of complex number, we have</p>
<p><img src="https://latex.codecogs.com/png.latex?%0Ae%5E%7Bi%20%5Ctheta%7D=%5Ccos%20%5Ctheta+i%20%5Csin%20%5Ctheta%0A"></p>
<p>The equation can be proved if we use the Taylor series of <img src="https://latex.codecogs.com/png.latex?e%5Ex">, <img src="https://latex.codecogs.com/png.latex?%5Coperatorname%7Bcos%7D%20x"> and <img src="https://latex.codecogs.com/png.latex?%5Coperatorname%7Bsin%7D%20x"> to prove it. This formula will be highlighted when we use complex analysis in the ML.</p>
<p>Additionally, we note that the complex number can be viewed as a special dimension so that we have <img src="https://latex.codecogs.com/png.latex?i%5E2=-1">. This <img src="https://latex.codecogs.com/png.latex?i"> will be helpful for many special computation.</p>
</section>
<section id="consider-the-rotary-embedding-using-complex-analysis" class="level2">
<h2 class="anchored" data-anchor-id="consider-the-rotary-embedding-using-complex-analysis">Consider the rotary embedding using complex analysis</h2>
<p>The token positional embedding is used to capture the features of token because of its position in the sequence. To put it simple, the token in the position 0 has different contribution from the token to the position 10.</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Tip
</div>
</div>
<div class="callout-body-container callout-body">
<p>📝 <strong>Paper</strong>: <a href="https://arxiv.org/pdf/2104.09864.pdf">ROFORMER: ENHANCED TRANSFORMER WITH ROTARY POSITION EMBEDDING</a></p>
</div>
</div>
<p>The present research indicate that we wish to add the absolute embedding and the relative embedding to the token based on its position. The absolute embedding is just decided by the position of the token. And the relative embedding is decided by the relative position of the token. Note that for the embedding computation, we need to compute the attendence score between the token at the position of <img src="https://latex.codecogs.com/png.latex?m"> and <img src="https://latex.codecogs.com/png.latex?n">:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Baligned%7D%0A%5Cboldsymbol%7Bq%7D_m%20&amp;%20=f_q%5Cleft(%5Cboldsymbol%7Bx%7D_m,%20m%5Cright)%20%5C%5C%0A%5Cboldsymbol%7Bk%7D_n%20&amp;%20=f_k%5Cleft(%5Cboldsymbol%7Bx%7D_n,%20n%5Cright)%20%5C%5C%0A%5Cboldsymbol%7Bv%7D_n%20&amp;%20=f_v%5Cleft(%5Cboldsymbol%7Bx%7D_n,%20n%5Cright).%0A%5Cend%7Baligned%7D%0A"></p>
<p>When you consider this problem, try to simulate the LLM inference (not training). The query is the new token to be predicted, and the key and value are the existing tokens. Now, we also need to consider the position info between them. The original transformer paper uses the absolute position embedding:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0Af_%7Bt:%20t%20%5Cin%5C%7Bq,%20k,%20v%5C%7D%7D%5Cleft(%5Cboldsymbol%7Bx%7D_i,%20i%5Cright):=%5Cboldsymbol%7BW%7D_%7Bt:%20t%20%5Cin%5C%7Bq,%20k,%20v%5C%7D%7D%5Cleft(%5Cboldsymbol%7Bx%7D_i+%5Cboldsymbol%7Bp%7D_i%5Cright),%0A"></p>
<p>and</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Bcases%7D%5Cboldsymbol%7Bp%7D_%7Bi,%202%20t%7D%20&amp;%20=%5Csin%20%5Cleft(k%20/%2010000%5E%7B2%20t%20/%20d%7D%5Cright)%20%5C%5C%20%5Cboldsymbol%7Bp%7D_%7Bi,%202%20t+1%7D%20&amp;%20=%5Ccos%20%5Cleft(k%20/%2010000%5E%7B2%20t%20/%20d%7D%5Cright)%5Cend%7Bcases%7D%0A"></p>
<p>If we think about this structure further, we found that the <code>sin</code> and <code>cos</code> function is the periodic functions, which means for the same relative distance, we could observe similar embedding.</p>
<p>Another relative positional embedding is to note that the relative position between the token <img src="https://latex.codecogs.com/png.latex?m"> and <img src="https://latex.codecogs.com/png.latex?n"> is <img src="https://latex.codecogs.com/png.latex?m-n">, and the embedding is dependent on the <img src="https://latex.codecogs.com/png.latex?m-n">, the difference. Note that we need to use <img src="https://latex.codecogs.com/png.latex?%5Cboldsymbol%7Bq%7D_m%5ET%5Cboldsymbol%7Bk%7D_n">, and this should be able to reflect the relative position information between the two tokens. And the <strong>current research indicates that the relative position embedding is important for the positional information</strong>. We wish the inner product encodes position information by:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cleft%5Clangle%20f_q%5Cleft(%5Cboldsymbol%7Bx%7D_m,%20m%5Cright),%20f_k%5Cleft(%5Cboldsymbol%7Bx%7D_n,%20n%5Cright)%5Cright%5Crangle=g%5Cleft(%5Cboldsymbol%7Bx%7D_m,%20%5Cboldsymbol%7Bx%7D_n,%20m-n%5Cright)%20.%0A"></p>
<p>The idea here is that we express the relative position as the information of angle rather than the position in a linear segment. And we can use complex analysis for it. It is like the signal processing where each signal has the frequency and the magnitude. Suppose <img src="https://latex.codecogs.com/png.latex?d=2">, and we can assume the embedding information as:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Baligned%7D%0Af_q%5Cleft(%5Cboldsymbol%7Bx%7D_q,%20m%5Cright)%20&amp;%20=R_q%5Cleft(%5Cboldsymbol%7Bx%7D_q,%20m%5Cright)%20e%5E%7Bi%20%5CTheta_q%5Cleft(%5Cboldsymbol%7Bx%7D_q,%20m%5Cright)%7D%20%5C%5C%0Af_k%5Cleft(%5Cboldsymbol%7Bx%7D_k,%20n%5Cright)%20&amp;%20=R_k%5Cleft(%5Cboldsymbol%7Bx%7D_k,%20n%5Cright)%20e%5E%7Bi%20%5CTheta_k%5Cleft(%5Cboldsymbol%7Bx%7D_k,%20n%5Cright)%7D%20%5C%5C%0Ag%5Cleft(%5Cboldsymbol%7Bx%7D_q,%20%5Cboldsymbol%7Bx%7D_k,%20n-m%5Cright)%20&amp;%20=R_g%5Cleft(%5Cboldsymbol%7Bx%7D_q,%20%5Cboldsymbol%7Bx%7D_k,%20n-m%5Cright)%20e%5E%7Bi%20%5CTheta_g%5Cleft(%5Cboldsymbol%7Bx%7D_q,%20%5Cboldsymbol%7Bx%7D_k,%20n-m%5Cright)%7D%0A%5Cend%7Baligned%7D%0A"></p>
<p>Thus, using this information, we have</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Baligned%7D%0AR_q%5Cleft(%5Cboldsymbol%7Bx%7D_q,%20m%5Cright)%20R_k%5Cleft(%5Cboldsymbol%7Bx%7D_k,%20n%5Cright)%20&amp;%20=R_g%5Cleft(%5Cboldsymbol%7Bx%7D_q,%20%5Cboldsymbol%7Bx%7D_k,%20n-m%5Cright),%20%5C%5C%0A%5CTheta_k%5Cleft(%5Cboldsymbol%7Bx%7D_k,%20n%5Cright)-%5CTheta_q%5Cleft(%5Cboldsymbol%7Bx%7D_q,%20m%5Cright)%20&amp;%20=%5CTheta_g%5Cleft(%5Cboldsymbol%7Bx%7D_q,%20%5Cboldsymbol%7Bx%7D_k,%20n-m%5Cright),%0A%5Cend%7Baligned%7D%0A"></p>
<p>After derivation, we found that if we choose the following expression, we can satisfy the condition above:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Baligned%7D%0Af_q%5Cleft(%5Cboldsymbol%7Bx%7D_m,%20m%5Cright)%20&amp;%20=%5Cleft(%5Cboldsymbol%7BW%7D_q%20%5Cboldsymbol%7Bx%7D_m%5Cright)%20e%5E%7Bi%20m%20%5Ctheta%7D%20%5C%5C%0Af_k%5Cleft(%5Cboldsymbol%7Bx%7D_n,%20n%5Cright)%20&amp;%20=%5Cleft(%5Cboldsymbol%7BW%7D_k%20%5Cboldsymbol%7Bx%7D_n%5Cright)%20e%5E%7Bi%20n%20%5Ctheta%7D%20%5C%5C%0Ag%5Cleft(%5Cboldsymbol%7Bx%7D_m,%20%5Cboldsymbol%7Bx%7D_n,%20m-n%5Cright)%20&amp;%20=%5Coperatorname%7BRe%7D%5Cleft%5B%5Cleft(%5Cboldsymbol%7BW%7D_q%20%5Cboldsymbol%7Bx%7D_m%5Cright)%5Cleft(%5Cboldsymbol%7BW%7D_k%20%5Cboldsymbol%7Bx%7D_n%5Cright)%5E*%20e%5E%7Bi(m-n)%20%5Ctheta%7D%5Cright%5D%0A%5Cend%7Baligned%7D%0A"></p>
<p>The derivation is as the following (<strong>This is not shown in the paper, you can use the derivation below to understand the paper better</strong>):</p>
<p>Note that if we express two vectors as <img src="https://latex.codecogs.com/png.latex?z_1%20=%20a%20+%20bi"> and <img src="https://latex.codecogs.com/png.latex?z_2%20=%20c%20+%20di">, the inner product is <img src="https://latex.codecogs.com/png.latex?ac%20+%20bd">. How would this be related to the multiplication of the two vectors? We actually have: <img src="https://latex.codecogs.com/png.latex?ac+bd%20=%20%5Coperatorname%7BRe%7D(z_1%20*%20%5Coverline%7Bz_2%7D)">.</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Baligned%7D%0A%5Clangle%20f_q,%20f_k%5Crangle%20&amp;=%20%5Coperatorname%7BRe%7D(f_q%20*%20%5Coverline%7Bf_k%7D)%20%5C%5C%0A%20%20%20%20%20%20&amp;=%20%5Cleft(%5Cboldsymbol%7BW%7D_q%20%5Cboldsymbol%7Bx%7D_m%5Cright)%20e%5E%7Bi%20m%20%5Ctheta%7D%20%5Coverline%7B%5Cleft(%5Cboldsymbol%7BW%7D_k%20%5Cboldsymbol%7Bx%7D_n%5Cright)%20e%5E%7Bi%20n%20%5Ctheta%7D%7D%20%5C%5C%0A%20%20%20%20%20%20&amp;=%20%5Cleft(%5Cboldsymbol%7BW%7D_q%20%5Cboldsymbol%7Bx%7D_m%5Cright)%20%5Coverline%7B%5Cleft(%5Cboldsymbol%7BW%7D_k%20%5Cboldsymbol%7Bx%7D_n%5Cright)%7D%20e%5E%7Bi(n-m)%20%5Ctheta%7D%0A%5Cend%7Baligned%7D%0A"></p>
<p>From the expression of <img src="https://latex.codecogs.com/png.latex?f_q%5Cleft(%5Cboldsymbol%7Bx%7D_m,%20m%5Cright)%20=%5Cleft(%5Cboldsymbol%7BW%7D_q%20%5Cboldsymbol%7Bx%7D_m%5Cright)%20e%5E%7Bi%20m%20%5Ctheta%7D">, we can design the rotary embedding setup in the llama2. It is important to note here that we introduce the complex number since we wish to integrate the meaning of the magnitude and the angle. The embedding itself represents the magnitude, and the angle is from the position. In real matrix multiplication, we can only do the real calculation, thus, we need the mapping above.</p>
<p>We can put it another way. The real and imaginary part of the complex number are useful information for us. We can express the vectors using complex theory, so that we can incorporate the angle or phase information from the vectors. Finally, we still need to map back to the real operations and proceed it with useful information. It is like a auxiliary method to help process information using some intermediate state.</p>


</section>

 ]]></description>
  <category>Math Theories</category>
  <guid>https://alexchen4ai.github.io/blog/notes/Math Theories/complexanalysis.html</guid>
  <pubDate>Sun, 25 Feb 2024 08:00:00 GMT</pubDate>
</item>
<item>
  <title>Large language model evaluation</title>
  <dc:creator>Alex Chen</dc:creator>
  <link>https://alexchen4ai.github.io/blog/notes/Large Language Model/llm_eval.html</link>
  <description><![CDATA[ 





<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Tip
</div>
</div>
<div class="callout-body-container callout-body">
<p>Today, the landscape of large language models (LLMs) is rich with diverse evaluation benchmarks. In this blog post, we’ll explore the various benchmarks used to assess language models and guide you through the process of obtaining these benchmarks after running a language model.</p>
</div>
</div>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Tip
</div>
</div>
<div class="callout-body-container callout-body">
<p>Benchmarks for evaluating large language models come in various forms, each serving a unique purpose. They can be broadly categorized into general benchmarks, which assess overall performance, and specialized benchmarks, designed to evaluate the model’s proficiency in specific areas such as understanding the Chinese language or its ability to perform function calls.</p>
</div>
</div>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Tip
</div>
</div>
<div class="callout-body-container callout-body">
<p>Consider to learn Stanford CS224U if you want to learn more fundamental knowledge about the LLM evaluation.</p>
</div>
</div>
<section id="llm-leaderboard" class="level2">
<h2 class="anchored" data-anchor-id="llm-leaderboard">LLM leaderboard</h2>
<p>Numerous leaderboards exist for Large Language Models (LLMs), each compiled based on the benchmarks of these models. By examining these leaderboards, we can identify which benchmarks are particularly effective and informative for evaluating the capabilities of LLMs.</p>
<ol type="1">
<li><a href="https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard">Huggingface LLM Leaderboard</a>;</li>
<li><a href="https://llm-leaderboard.streamlit.app/">Streamlit Leaderboard</a>;</li>
<li><a href="https://chat.lmsys.org/">LMSYS Leaderboard</a>;</li>
<li><a href="https://huggingface.co/spaces/mike-ravkine/can-ai-code-results">Can AI code leaderboard</a>.</li>
</ol>
<p>Also, we can get more benchmark from the papers for sure. 📝 <strong>Paper</strong>: <a href="https://arxiv.org/pdf/2310.19736.pdf">Evaluating Large Language Models: A Comprehensive Survey</a> can provide a full explanation about it.</p>
</section>
<section id="classification-of-llm-evaluation" class="level2">
<h2 class="anchored" data-anchor-id="classification-of-llm-evaluation">Classification of LLM evaluation</h2>
<p>There are different benchmarks for the LLM evaluation. The general classification can be found in Figure&nbsp;1.</p>
<div id="fig-classification" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-classification-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<p align="center">
<img src="https://alexchen4ai.github.io/blog/images/LLM_eval.png" width="80%" class="figure-img">
</p>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-classification-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1: The classification of LLM evaluation.
</figcaption>
</figure>
</div>
<p>For each aspect of the model, we will have different methods to evaluate it.</p>
<p>The knowledge and capability evaluation can be seen in Figure&nbsp;2.</p>
<div id="fig-knowledge-capability" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-knowledge-capability-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<p align="center">
<img src="https://alexchen4ai.github.io/blog/images/knowledge_capability_evaluation.png" width="100%" class="figure-img">
</p>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-knowledge-capability-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2: The progress of the LLM knowledge capability evaluation.
</figcaption>
</figure>
</div>
<p>The commonsense reasoning datasets can be seen below:</p>
<table class="table">
<caption>The details of commonsense reasoning datasets.</caption>
<colgroup>
<col style="width: 19%">
<col style="width: 12%">
<col style="width: 8%">
<col style="width: 37%">
<col style="width: 22%">
</colgroup>
<thead>
<tr class="header">
<th>Dataset</th>
<th>Domain</th>
<th>Size</th>
<th>Source</th>
<th>Task</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>ARC</td>
<td>science</td>
<td>7,787</td>
<td>a variety of sources</td>
<td>multiple-choice QA</td>
</tr>
<tr class="even">
<td>QASC</td>
<td>science</td>
<td>9,980</td>
<td>human-authored</td>
<td>multiple-choice QA</td>
</tr>
<tr class="odd">
<td>MCTACO</td>
<td>temporal</td>
<td>1,893</td>
<td>MultiRC</td>
<td>multiple-choice QA</td>
</tr>
<tr class="even">
<td>TRACIE</td>
<td>temporal</td>
<td>-</td>
<td>ROCStories, Wikipedia</td>
<td>multiple-choice QA</td>
</tr>
<tr class="odd">
<td>TIMEDIAL</td>
<td>temporal</td>
<td>1.1K</td>
<td>DailyDialog</td>
<td>multiple-choice QA</td>
</tr>
<tr class="even">
<td>HellaSWAG</td>
<td>event</td>
<td>20K</td>
<td>ActivityNet, WikiHow</td>
<td>multiple-choice QA</td>
</tr>
<tr class="odd">
<td>PIQA</td>
<td>physical</td>
<td>21K</td>
<td>human-authored</td>
<td>2-choice QA</td>
</tr>
<tr class="even">
<td>Pep-3k</td>
<td>physical</td>
<td>3,062</td>
<td>human-authored</td>
<td>2-choice QA</td>
</tr>
<tr class="odd">
<td>Social IQA</td>
<td>social</td>
<td>38K</td>
<td>human-authored</td>
<td>multiple-choice QA</td>
</tr>
<tr class="even">
<td>CommonsenseQA</td>
<td>generic</td>
<td>12,247</td>
<td>CONCEPTNET, human-authored</td>
<td>multiple-choice QA</td>
</tr>
<tr class="odd">
<td>OpenBookQA</td>
<td>generic</td>
<td>6K</td>
<td>WorldTree</td>
<td>multiple-choice QA</td>
</tr>
</tbody>
</table>
<p>And the multi-hop reasoning dataset is:</p>
<table class="table">
<colgroup>
<col style="width: 15%">
<col style="width: 12%">
<col style="width: 9%">
<col style="width: 11%">
<col style="width: 32%">
<col style="width: 18%">
</colgroup>
<thead>
<tr class="header">
<th>Dataset</th>
<th>Domain</th>
<th>Size</th>
<th># hops</th>
<th>Source</th>
<th>Answer type</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>HotpotQA</td>
<td>generic</td>
<td>112,779</td>
<td>1/2/3</td>
<td>Wikipedia</td>
<td>span</td>
</tr>
<tr class="even">
<td>HybridQA</td>
<td>generic</td>
<td>69,611</td>
<td>2/3</td>
<td>Wikitables, Wikipedia</td>
<td>span</td>
</tr>
<tr class="odd">
<td>MultiRC</td>
<td>generic</td>
<td>9,872</td>
<td>2.37</td>
<td>Multiple</td>
<td>MCQ</td>
</tr>
<tr class="even">
<td>NarrativeQA</td>
<td>fiction</td>
<td>46,765</td>
<td>-</td>
<td>Multiple</td>
<td>generative</td>
</tr>
<tr class="odd">
<td>Medhop</td>
<td>medline</td>
<td>2,508</td>
<td>-</td>
<td>Medline</td>
<td>MCQ</td>
</tr>
<tr class="even">
<td>Wikihop</td>
<td>generic</td>
<td>51,318</td>
<td>-</td>
<td>Wikipedia</td>
<td>MCQ</td>
</tr>
</tbody>
</table>
<p>Like the knowledge and capability, there are datasets prepared for other benchmark as well.</p>
</section>
<section id="benchmarks" class="level2">
<h2 class="anchored" data-anchor-id="benchmarks">Benchmarks</h2>
<p>Once we’ve acquired the dataset to assess the Large Language Model (LLM), we introduce a crucial concept known as a <strong>benchmark</strong>—a tool that quantitatively evaluates the LLM’s performance. Let’s delve deeper into the benchmarks and their significance.</p>
<table class="table">
<caption>Benchmarks for Knowledge and Reasoning</caption>
<colgroup>
<col style="width: 17%">
<col style="width: 13%">
<col style="width: 26%">
<col style="width: 18%">
<col style="width: 24%">
</colgroup>
<thead>
<tr class="header">
<th>Benchmarks</th>
<th># Tasks</th>
<th>Language</th>
<th># Instances</th>
<th>Evaluation Form</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>MMLU</td>
<td>57</td>
<td>English</td>
<td>15,908</td>
<td>Local</td>
</tr>
<tr class="even">
<td>MMCU</td>
<td>51</td>
<td>Chinese</td>
<td>11,900</td>
<td>Local</td>
</tr>
<tr class="odd">
<td>C-Eval</td>
<td>52</td>
<td>Chinese</td>
<td>13,948</td>
<td>Online</td>
</tr>
<tr class="even">
<td>AGIEval</td>
<td>20</td>
<td>English, Chinese</td>
<td>8,062</td>
<td>Local</td>
</tr>
<tr class="odd">
<td>M3KE</td>
<td>71</td>
<td>Chinese</td>
<td>20,477</td>
<td>Local</td>
</tr>
<tr class="even">
<td>M3Exam</td>
<td>4</td>
<td>English and others</td>
<td>12,317</td>
<td>Local</td>
</tr>
<tr class="odd">
<td>CMMLU</td>
<td>67</td>
<td>Chinese</td>
<td>11,528</td>
<td>Local</td>
</tr>
<tr class="even">
<td>LucyEval</td>
<td>55</td>
<td>Chinese</td>
<td>11,000</td>
<td>Online</td>
</tr>
</tbody>
</table>
<p>Also, there are some benchmark for the holistic evaluation.</p>
<table class="table">
<caption>Holistic benchmarks</caption>
<colgroup>
<col style="width: 16%">
<col style="width: 18%">
<col style="width: 29%">
<col style="width: 20%">
<col style="width: 15%">
</colgroup>
<thead>
<tr class="header">
<th>Benchmarks</th>
<th>Language</th>
<th>benchmark</th>
<th>Evaluation Form</th>
<th>Expandability</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>HELM</td>
<td>English</td>
<td>Automatic</td>
<td>Local</td>
<td>Supported</td>
</tr>
<tr class="even">
<td>BIG-bench</td>
<td>English and others</td>
<td>Automatic</td>
<td>Local</td>
<td>Supported</td>
</tr>
<tr class="odd">
<td>OpenCompass</td>
<td>English and others</td>
<td>Automatic and LLMs-based</td>
<td>Local</td>
<td>Supported</td>
</tr>
<tr class="even">
<td>Huggingface</td>
<td>English</td>
<td>Automatic</td>
<td>Local</td>
<td>Unsupported</td>
</tr>
<tr class="odd">
<td>FlagEval</td>
<td>English and others</td>
<td>Automatic and Manual</td>
<td>Local and Online</td>
<td>Unsupported</td>
</tr>
<tr class="even">
<td>OpenEval</td>
<td>Chinese</td>
<td>Automatic</td>
<td>Local</td>
<td>Supported</td>
</tr>
<tr class="odd">
<td>Chatbot Arena</td>
<td>English and others</td>
<td>Manual</td>
<td>Online</td>
<td>Supported</td>
</tr>
</tbody>
</table>
</section>
<section id="how-to-calculate-the-benchmark" class="level2">
<h2 class="anchored" data-anchor-id="how-to-calculate-the-benchmark">How to calculate the benchmark</h2>
<p>In the dynamic world of artificial intelligence, benchmarks play a pivotal role in gauging the prowess of AI models. A notable platform that has garnered widespread attention for its comprehensive leaderboard is Hugging Face. Here, benchmarks such as <em>Average</em>, <em>ARC</em>, <em>HellaSwag</em>, <em>MMLU</em>, <em>TruthfulQA</em>, <em>Winogrande</em>, and <em>GSM8K</em> offer a bird’s-eye view of an AI model’s capabilities. To demystify the process of benchmark calculation, let’s delve into a practical example using the <strong>TruthfulQA</strong> benchmark.</p>
<section id="discovering-truthfulqa" class="level3">
<h3 class="anchored" data-anchor-id="discovering-truthfulqa">Discovering TruthfulQA</h3>
<p>The <strong>TruthfulQA</strong> dataset, accessible on Hugging Face (<a href="https://huggingface.co/datasets/truthful_qa/viewer/multiple_choice">view dataset</a>), serves as an excellent starting point. This benchmark is designed to evaluate an AI’s ability to not only generate accurate answers but also ensure they align with factual correctness.</p>
</section>
<section id="unified-framework-for-evaluation" class="level3">
<h3 class="anchored" data-anchor-id="unified-framework-for-evaluation">Unified Framework for Evaluation</h3>
<p>Thankfully, the complexity of working across different benchmarks is significantly reduced with tools like the <a href="https://github.com/EleutherAI/lm-evaluation-harness">lm-evaluation-harness</a> repository. This unified framework simplifies the evaluation process, allowing for a streamlined approach to assessing AI models across various benchmarks.</p>
</section>
<section id="tailoring-evaluation-to-learning-scenarios" class="level3">
<h3 class="anchored" data-anchor-id="tailoring-evaluation-to-learning-scenarios">Tailoring Evaluation to Learning Scenarios</h3>
<p>The evaluation process varies significantly depending on the learning scenario—be it zero-shot or few-shot learning. In few-shot learning, where the model is primed with examples, prompts such as <code>thus, the choice is:</code> can guide the model to the correct answer format (e.g., <code>A, B, C</code>). For zero-shot scenarios, where the model lacks prior examples, multiple prompts may be necessary. The first prompt elicits a raw response, while subsequent prompts refine this into a final, decisive choice.</p>
</section>
<section id="navigating-dataset-splits" class="level3">
<h3 class="anchored" data-anchor-id="navigating-dataset-splits">Navigating Dataset Splits</h3>
<p>A critical aspect of benchmark evaluation is understanding the dataset structure, particularly the splits: <code>train</code>, <code>val</code>, and <code>test</code>. For benchmarks like <strong>HellaSwag</strong>, it’s crucial to fine-tune models on the <code>train</code> and <code>val</code> datasets before evaluating them on the <code>test</code> set. This approach ensures the model is not unfairly advantaged by exposure to test data during training, maintaining the integrity of the evaluation process.</p>
</section>
<section id="conclusion" class="level3">
<h3 class="anchored" data-anchor-id="conclusion">Conclusion</h3>
<p>Benchmarks like TruthfulQA are indispensable for advancing AI research, providing a clear benchmark for evaluating the nuanced capabilities of AI models. By leveraging unified frameworks and adapting to the specific demands of different learning scenarios, researchers can efficiently and accurately assess their models. Remember, the key to a successful evaluation lies in understanding the dataset, choosing the right learning scenario, and meticulously following the evaluation protocol to ensure fair and accurate results.</p>


</section>
</section>

 ]]></description>
  <category>Large Language Models</category>
  <guid>https://alexchen4ai.github.io/blog/notes/Large Language Model/llm_eval.html</guid>
  <pubDate>Thu, 22 Feb 2024 08:00:00 GMT</pubDate>
</item>
<item>
  <title>Mixture of expert</title>
  <dc:creator>Alex Chen</dc:creator>
  <link>https://alexchen4ai.github.io/blog/notes/Large Language Model/moe.html</link>
  <description><![CDATA[ 





<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Tip
</div>
</div>
<div class="callout-body-container callout-body">
<p>MoE means the mixture of expert. In this blog, we will introduce the type of MoE used in the <code>mixtral8x7B</code> model. The blog will assume that you already understand the llama2 model. Thus, we will not revisit any knowledge already inside the llama2 model. Another focus of this blog will combine the code implementation to delve into the arc of MoE.</p>
</div>
</div>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Tip
</div>
</div>
<div class="callout-body-container callout-body">
<p>📝 <strong>Paper</strong>: <a href="https://arxiv.org/abs/2101.03961">Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity</a></p>
</div>
</div>
<section id="difference-from-the-llama2" class="level2">
<h2 class="anchored" data-anchor-id="difference-from-the-llama2">Difference from the llama2</h2>
<p>In causal LM, we know that the decoder model can be decomposed into multiple small parts. Let’s visit the architecture of the llama2 in the Figure&nbsp;1.</p>
<div id="fig-llama2" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-llama2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="https://alexchen4ai.github.io/blog/images/llama2.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-llama2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1: The architecture of the llama2 model
</figcaption>
</figure>
</div>
<p>There are many components inside the llama2 architecure, like the attention layer, positional embedding, RMS norm, FC etc. The only difference here is the block of <strong>FF SwiGLU</strong> (<code>FF</code> is the feedforward network and the implementation is add the hidden embedding dimension first, through the activation function and finally decrease the dimension). Instead of using only one FF function, we use the mixture of expert, which export is the actually a <strong>FF</strong>. In llama2, the code implementation would be as simple as:</p>
<div class="sourceCode" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">class</span> LlamaMLP(nn.Module):</span>
<span id="cb1-2">    <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">def</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">__init__</span>(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>, config):</span>
<span id="cb1-3">        <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">super</span>().<span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">__init__</span>()</span>
<span id="cb1-4">        <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.config <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> config</span>
<span id="cb1-5">        <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.hidden_size <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> config.hidden_size</span>
<span id="cb1-6">        <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.intermediate_size <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> config.intermediate_size</span>
<span id="cb1-7">        <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.gate_proj <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> nn.Linear(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.hidden_size, <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.intermediate_size, bias<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">False</span>)</span>
<span id="cb1-8">        <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.up_proj <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> nn.Linear(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.hidden_size, <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.intermediate_size, bias<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">False</span>)</span>
<span id="cb1-9">        <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.down_proj <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> nn.Linear(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.intermediate_size, <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.hidden_size, bias<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">False</span>)</span>
<span id="cb1-10">        <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.act_fn <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> ACT2FN[config.hidden_act]</span>
<span id="cb1-11"></span>
<span id="cb1-12">    <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">def</span> forward(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>, x):</span>
<span id="cb1-13">        down_proj <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.down_proj(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.act_fn(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.gate_proj(x)) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.up_proj(x))</span>
<span id="cb1-14">        <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">return</span> down_proj</span></code></pre></div>
<p>OK, now we will add more complexity to this function. ::: {.callout-tip} Note that all other parts are the same except the feedforward block. :::</p>
</section>
<section id="mathematical-insights" class="level2">
<h2 class="anchored" data-anchor-id="mathematical-insights">Mathematical insights</h2>
<p>The FF model is actually the expert! For the llama2 model, we only have one expert, therefore, in the case of llama2, we have</p>
<p><img src="https://latex.codecogs.com/png.latex?%0Ay%20=%20E(x),%0A"></p>
<p>where <img src="https://latex.codecogs.com/png.latex?x,%20y"> are the value before and after the FF block. In the MoE, we actually prepare multiple trainable experts, so there are multiple <code>E</code> layers. A gating network is thus introduced to decide which network should be used. And now the expression becomes:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0Ay%20=%20%5Csum_%7Bi=1%7D%5En%20G(x)_i%20E_i(x).%0A"></p>
<p>It is special to choose the network <code>G</code>. Here, we just introduce the found research work from the paper <a href="https://arxiv.org/abs/2101.03961">Switch Transformer</a> which is also the famous <a href="https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1">Mixtral-8x7B</a>.</p>
<p>Specially for this model, <code>n=8</code>. Suppose the last dimension of <code>x</code> is <code>d</code>, we will include the dimension of each variables for following up explanation.</p>
<p>The steps to construct the <code>G(x)</code> are:</p>
<ol type="1">
<li>Set trainable linear layer <img src="https://latex.codecogs.com/png.latex?W_g"> of size <code>d*n</code>. <img src="https://latex.codecogs.com/png.latex?%0AH(x)=%20x%20%5Ccdot%20W_%7B%5Cmathrm%7Bg%7D%7D%0A"> and we know the dimension of <img src="https://latex.codecogs.com/png.latex?H(x)"> is <img src="https://latex.codecogs.com/png.latex?n=8">.</li>
<li>Only pick the top K experts: <img src="https://latex.codecogs.com/png.latex?%0A%5Coperatorname%7BKeepTopK%7D(v,%20k)_i=%20%5Cbegin%7Bcases%7Dv_i%20&amp;%20%5Ctext%20%7B%20if%20%7D%20v_i%20%5Ctext%20%7B%20is%20in%20the%20top%20%7D%20k%20%5Ctext%20%7B%20elements%20of%20%7D%20v%20%5C%5C%20-%5Cinfty%20&amp;%20%5Ctext%20%7B%20otherwise.%20%7D%5Cend%7Bcases%7D%0A"></li>
<li>Apply the softmax to get the final <code>G(x)</code> <img src="https://latex.codecogs.com/png.latex?%0AG(x)=%5Coperatorname%7BNorm%7D(%5Coperatorname%7BSoftmax%7D(%5Coperatorname%7BKeepTopK%7D(H(x),%20k)))%0A"></li>
</ol>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Tip
</div>
</div>
<div class="callout-body-container callout-body">
<p>Set to <img src="https://latex.codecogs.com/png.latex?-%5Cinfty"> so that it becomes zero during softmax. Thus, for the final output, we only have <img src="https://latex.codecogs.com/png.latex?k"> experts which are really used.</p>
</div>
</div>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Tip
</div>
</div>
<div class="callout-body-container callout-body">
<p>For different tokens (tokens in the same sequence and batch could be routed to different experts), they will choose different experts! For example the <code>batch[0]</code> will choose the expert 0 and 3, while the <code>batch[1]</code> will choose the expert 4 and 7.</p>
</div>
</div>
</section>
<section id="load-balancing-loss" class="level2">
<h2 class="anchored" data-anchor-id="load-balancing-loss">Load balancing loss</h2>
<p>Since different portion of total tokens will enter different experts, like the unbalanced dataset problem, we need to add a load balancing loss. Given <img src="https://latex.codecogs.com/png.latex?N"> experts indexed by <img src="https://latex.codecogs.com/png.latex?i=1"> to <img src="https://latex.codecogs.com/png.latex?N"> and a batch <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BB%7D"> with <img src="https://latex.codecogs.com/png.latex?T"> tokens, the auxiliary loss is computed as the scaled dot-product between vectors <img src="https://latex.codecogs.com/png.latex?f"> and <img src="https://latex.codecogs.com/png.latex?P">, <img src="https://latex.codecogs.com/png.latex?%0A%5Ctext%20%7B%20loss%20%7D=%5Calpha%20%5Ccdot%20N%20%5Ccdot%20%5Csum_%7Bi=1%7D%5EN%20f_i%20%5Ccdot%20P_i%0A"> where <img src="https://latex.codecogs.com/png.latex?f_i"> is the fraction of tokens dispatched to expert <img src="https://latex.codecogs.com/png.latex?i">, <img src="https://latex.codecogs.com/png.latex?%0Af_i=%5Cfrac%7B1%7D%7BT%7D%20%5Csum_%7Bx%20%5Cin%20%5Cmathcal%7BB%7D%7D%20%5Cmathbb%7B1%7D%5C%7B%5Coperatorname%7Bargmax%7D%20p(x)=i%5C%7D%0A"> and <img src="https://latex.codecogs.com/png.latex?P_i"> is the fraction of the router probability allocated for expert <img src="https://latex.codecogs.com/png.latex?i,%7B%20%7D%5E2"> <img src="https://latex.codecogs.com/png.latex?%0AP_i=%5Cfrac%7B1%7D%7BT%7D%20%5Csum_%7Bx%20%5Cin%20%5Cmathcal%7BB%7D%7D%20p_i(x)%0A"></p>
<p>We add this loss since we want to encourages uniform routing since the loss is minimized when <img src="https://latex.codecogs.com/png.latex?%0Af_i%20=%20P_i%20=%20%5Cfrac%7B1%7D%7BN%7D.%0A"></p>
<div id="thm-policy-gradient-theorem" class="theorem">
<p><span class="theorem-title"><strong>Theorem 1</strong></span> To prove that the minimum of the objective function <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Ba%7D%20%5Ccdot%20%5Cmathbf%7Bb%7D=%5Csum_%7Bi=1%7D%5EN%20a_i%20b_i"> is achieved when <img src="https://latex.codecogs.com/png.latex?a_i="> <img src="https://latex.codecogs.com/png.latex?b_i=%5Cfrac%7B1%7D%7BN%7D"> under the given constraints, we use the method of Lagrange multipliers for the constraints:</p>
<p>Given constraints:</p>
<ol type="1">
<li><img src="https://latex.codecogs.com/png.latex?%5Csum_%7Bi=1%7D%5EN%20a_i=1,%20a_i%20%5Cgeq%200">.</li>
<li><img src="https://latex.codecogs.com/png.latex?%5Csum_%7Bi=1%7D%5EN%20b_i=1,%20b_i%20%5Cgeq%200">.</li>
</ol>
<p>Objective Function to minimize:</p>
<ul>
<li><img src="https://latex.codecogs.com/png.latex?L=%5Cmathbf%7Ba%7D%20%5Ccdot%20%5Cmathbf%7Bb%7D-%5Clambda%5Cleft(%5Csum_%7Bi=1%7D%5EN%20a_i-1%5Cright)-%5Cmu%5Cleft(%5Csum_%7Bi=1%7D%5EN%20b_i-1%5Cright)">, where <img src="https://latex.codecogs.com/png.latex?%5Clambda"> and <img src="https://latex.codecogs.com/png.latex?%5Cmu"> are Lagrange multipliers.</li>
</ul>
<p>Taking partial derivatives of <img src="https://latex.codecogs.com/png.latex?L"> with respect to <img src="https://latex.codecogs.com/png.latex?a_i,%20b_i,%20%5Clambda">, and <img src="https://latex.codecogs.com/png.latex?%5Cmu"> and setting them to zero gives:</p>
<ol type="1">
<li><img src="https://latex.codecogs.com/png.latex?%5Cfrac%7B%5Cpartial%20L%7D%7B%5Cpartial%20a_i%7D=b_i-%5Clambda=0%20%5CRightarrow%20b_i=%5Clambda">.</li>
<li><img src="https://latex.codecogs.com/png.latex?%5Cfrac%7B%5Cpartial%20L%7D%7B%5Cpartial%20b_i%7D=a_i-%5Cmu=0%20%5CRightarrow%20a_i=%5Cmu">.</li>
<li><img src="https://latex.codecogs.com/png.latex?%5Cfrac%7B%5Cpartial%20L%7D%7B%5Cpartial%20%5Clambda%7D=%5Csum_%7Bi=1%7D%5EN%20a_i-1=0">.</li>
<li><img src="https://latex.codecogs.com/png.latex?%5Cfrac%7B%5Cpartial%20L%7D%7B%5Cpartial%20%5Cmu%7D=%5Csum_%7Bi=1%7D%5EN%20b_i-1=0">.</li>
</ol>
<p>From equations 1 and 2 , all <img src="https://latex.codecogs.com/png.latex?a_i"> and <img src="https://latex.codecogs.com/png.latex?b_i"> must be constant for all <img src="https://latex.codecogs.com/png.latex?i">, because they equal the respective Lagrange multipliers <img src="https://latex.codecogs.com/png.latex?%5Clambda"> and <img src="https://latex.codecogs.com/png.latex?%5Cmu">, which are constants. Thus, <img src="https://latex.codecogs.com/png.latex?a_i=a"> and <img src="https://latex.codecogs.com/png.latex?b_i=b"> for some constants <img src="https://latex.codecogs.com/png.latex?a"> and <img src="https://latex.codecogs.com/png.latex?b"> for all <img src="https://latex.codecogs.com/png.latex?i">.</p>
<p>Given the constraints <img src="https://latex.codecogs.com/png.latex?%5Csum_%7Bi=1%7D%5EN%20a_i=1"> and <img src="https://latex.codecogs.com/png.latex?%5Csum_%7Bi=1%7D%5EN%20b_i=1">, and knowing that <img src="https://latex.codecogs.com/png.latex?a_i=a"> and <img src="https://latex.codecogs.com/png.latex?b_i=b"> for all <img src="https://latex.codecogs.com/png.latex?i">, we have:</p>
<ul>
<li><img src="https://latex.codecogs.com/png.latex?%5Csum_%7Bi=1%7D%5EN%20a=N%20%5Ccdot%20a=1%20%5CRightarrow%20a=%5Cfrac%7B1%7D%7BN%7D">.</li>
<li><img src="https://latex.codecogs.com/png.latex?%5Csum_%7Bi=1%7D%5EN%20b=N%20%5Ccdot%20b=1%20%5CRightarrow%20b=%5Cfrac%7B1%7D%7BN%7D">.</li>
</ul>
<p>Therefore, setting <img src="https://latex.codecogs.com/png.latex?a_i=b_i=%5Cfrac%7B1%7D%7BN%7D"> for all <img src="https://latex.codecogs.com/png.latex?i"> satisfies the constraints and minimizes the objective function <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Ba%7D%20%5Ccdot%20%5Cmathbf%7Bb%7D=%5Csum_%7Bi=1%7D%5EN%20a_i%20b_i">, as any local minimum in a convex function over a convex set is a global minimum.</p>
</div>


</section>

 ]]></description>
  <category>Large Language Models</category>
  <guid>https://alexchen4ai.github.io/blog/notes/Large Language Model/moe.html</guid>
  <pubDate>Wed, 21 Feb 2024 08:00:00 GMT</pubDate>
</item>
<item>
  <title>Scalable diffusion models with transformers</title>
  <dc:creator>Alex Chen</dc:creator>
  <link>https://alexchen4ai.github.io/blog/notes/Diffusion Model/sd.html</link>
  <description><![CDATA[ 





<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Tip
</div>
</div>
<div class="callout-body-container callout-body">
<p>A text to image generation model from the diffusion architecture.</p>
</div>
</div>
<p>📝 <strong>Paper</strong>: <a href="https://arxiv.org/abs/2212.09748">https://arxiv.org/abs/2212.09748</a></p>
<section id="introduction" class="level2">
<h2 class="anchored" data-anchor-id="introduction">Introduction</h2>
<p>Coming soom</p>


</section>

 ]]></description>
  <category>Diffusion Model</category>
  <guid>https://alexchen4ai.github.io/blog/notes/Diffusion Model/sd.html</guid>
  <pubDate>Mon, 19 Feb 2024 08:00:00 GMT</pubDate>
</item>
<item>
  <title>Reinforcement learning for large language model</title>
  <dc:creator>Alex Chen</dc:creator>
  <link>https://alexchen4ai.github.io/blog/notes/Large Language Model/rl_llm.html</link>
  <description><![CDATA[ 





<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Tip
</div>
</div>
<div class="callout-body-container callout-body">
<p>Reinforment is a common technique, which can be applied to the large language model area.</p>
</div>
</div>
<section id="background-of-reinforcement-learning" class="level2">
<h2 class="anchored" data-anchor-id="background-of-reinforcement-learning">Background of reinforcement learning</h2>
In the first section, we will review the fundamental concept of the reinforcement learning. The fundamental part of the reinforcement learning includes the <strong>agent</strong> and <strong>environment</strong>. The process is as the following:
<p align="center">
<img src="https://alexchen4ai.github.io/blog/images/RL_basic.png" width="60%">
</p>
<p>At each iteration step, we have the state of the environement marked as <img src="https://latex.codecogs.com/png.latex?S">, the action <img src="https://latex.codecogs.com/png.latex?A"> and the reward <img src="https://latex.codecogs.com/png.latex?R">. Below, we list the step at the time step <img src="https://latex.codecogs.com/png.latex?t">:</p>
<ol type="1">
<li>Based on the current state <img src="https://latex.codecogs.com/png.latex?S_t">, the agent make the action <img src="https://latex.codecogs.com/png.latex?A_t">;</li>
<li>The environment react to the action and transit to the state <img src="https://latex.codecogs.com/png.latex?S_%7Bt+1%7D"> and reward <img src="https://latex.codecogs.com/png.latex?R_%7Bt+1%7D">.</li>
</ol>
<p>Therefore, related to each action, we will have a state of <img src="https://latex.codecogs.com/png.latex?S_t,%20A_t,%20S_%7Bt+1%7D,%20R_%7Bt+1%7D">. And these four variables will be the critical data used for the reinforcement learning! Now, let me introduce more about the <strong>glossary</strong> of the reinforcement learning terms.</p>
<ul>
<li><em>Markov chain</em>: The Markov chain means that the action taken by the agent is only dependent on the most recent state/present state, and is independent of past states.</li>
<li><em>Observation/State</em>: The state is the complete description while the observation is just the partial description. The partial description means part of the state.</li>
<li><em>policy</em>: The policy is usually denoted as <img src="https://latex.codecogs.com/png.latex?%5Cpi"> and it is used to decide which action <img src="https://latex.codecogs.com/png.latex?a"> to take. According to the Markov chain, we have <img src="https://latex.codecogs.com/png.latex?%5Cpi(s)=a">.</li>
<li><em>reward</em>: Reward is the value that we can get immediately after we take a new action. For example, in cartpole example, we get every positive feedback if the cartpole doesn’t fail.</li>
<li><em>Value</em>: The value function to calculate the discounted sum of all future rewards! Thus, the values are different from the reward.</li>
</ul>
<p>These are some basic concepts in the reinforcement learning! We will introduce more advanced concept along with more topics involved below. We revisit the fundamental part of the RL: The agent can repeated to take actions and get feedback (rewards/values) from the environment so that it can update the agent itself to behave better to get best reward or values. The deep learning and pytorch is not designed for the RL, and RL is more a mathematically which may not naturally suited for the deep learning. Rather, we design some equation to apply the deep learning. Thus, when we design the RL, we need to think from the fundamental math, and deep learning is just a method to solve a math problem.</p>
</section>
<section id="the-classification-of-rl" class="level2">
<h2 class="anchored" data-anchor-id="the-classification-of-rl">The classification of RL</h2>
<p>To solve the RL problem, we have various methods! The detailed is concluded in the figure below. We will study more about the policy based method, the value based method. And for SOTA, the LLM usuaully use a combined method. When we consider how to train the RL, we should first think about how to use the pretrained model. We wish the model to guide us to get the best action to take at every step! Thus, we need a great policy <img src="https://latex.codecogs.com/png.latex?%5Cpi%5E*">!.</p>
<p align="center">
<img src="https://alexchen4ai.github.io/blog/images/RL_classification.png" width="100%">
</p>
<section id="the-value-based-method" class="level3">
<h3 class="anchored" data-anchor-id="the-value-based-method">The value based method</h3>
<p>The famous <img src="https://latex.codecogs.com/png.latex?Q"> learning is a typical value-based method. The original paper can be accessed <a href="https://link.springer.com/content/pdf/10.1007/BF00992698.pdf"><strong>here</strong></a>. The <img src="https://latex.codecogs.com/png.latex?Q"> is the abbreviate of <em>quality</em>. The value based method has two submethods called the state-value function and the action-value function. Usually, we use <img src="https://latex.codecogs.com/png.latex?V"> to represent the value, which is</p>
<p><img src="https://latex.codecogs.com/png.latex?%0AV_%7B%5Cpi%7D(s)%20=%20%5Cmathbb%7BE%7D_%7B%5Cpi%7D%5Cleft%5B%20R_%7Bt+1%7D+%5Cgamma%20R_%7Bt+2%7D%20+%20%5Cgamma%5E2R_%7Bt+3%7D+...%20%7C%20S_t=s%20%5Cright%5D%0A"></p>
<p>Let me clarify the equation above in a probability. The <img src="https://latex.codecogs.com/png.latex?%5Cpi"> is like a distribution, and we may express the value as</p>
<p><img src="https://latex.codecogs.com/png.latex?%0AV_%7B%5Cpi%7D(s)%20=%20%5Cmathbb%7BE%7D_%7B%5Ctau%5Csim%5Cpi%7D%5Cleft%5B%20R_%7Bt+1%7D+%5Cgamma%20R_%7Bt+2%7D%20+%20%5Cgamma%5E2R_%7Bt+3%7D+...%20%7C%20S_t=s%20%5Cright%5D%0A"></p>
<p>since we have <img src="https://latex.codecogs.com/png.latex?a%5Csim%20%5Cpi(s)">. And <img src="https://latex.codecogs.com/png.latex?a"> is directly relevant to the trajectory <img src="https://latex.codecogs.com/png.latex?%5Ctau"> which can be used for comprehensive rewards. Now, we have known the value function, this is a value that can evaluate the current confidence to get the best reward based on the current state! Another better and granular method is not just the current state, but also the action. And we introduce the <img src="https://latex.codecogs.com/png.latex?Q"> value. However, fundamentally, we have <img src="https://latex.codecogs.com/png.latex?Q"> and <img src="https://latex.codecogs.com/png.latex?V"> to express the same meaning, the confidence or the estimated quality of the current condition. The only difference is that the <img src="https://latex.codecogs.com/png.latex?Q"> function also count in the actions.</p>
<p>The comparison would be <img src="https://latex.codecogs.com/png.latex?V_%5Cpi%20(s)=%5Cmathbb%7BE%7D_%5Cpi%20%5BG_t%7CS_t=s%5D"> vs.&nbsp;<img src="https://latex.codecogs.com/png.latex?Q_%7B%5Cpi%7D(s,%20a)=%5Cmathbb%7BE_%5Cpi%7D%5BG_t%7CS_t=s,%20A_t=a%5D">. The <img src="https://latex.codecogs.com/png.latex?G_t"> here represent the ending state. Then, as stated above how do we get the best policy? We can use</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cpi%5E*%20=%20%5Ctext%7Barg%7D%5Cmax_a%20Q%5E*(s,%20a)%0A"></p>
<p>To simulate the RL, we usually need to simulate the whole episode, like a cartpole example would continue until it fails. However, there are ways to simplify the process by <strong>Bellman equation</strong>: <img src="https://latex.codecogs.com/png.latex?%0AV_%5Cpi(s)%20=%20%5Cmathbb%7BE%7D_%7B%5Cpi%7D%20%5BR_%7Bt+1%7D+%5Cgamma%20*%20V_%7B%5Cpi%7D(S_%7Bt+1%7D)%7CS_t=s%5D.%0A"> And we can update the value function by <strong>Monte Carlo</strong> or the <strong>Temporary Difference</strong> method. The <img src="https://latex.codecogs.com/png.latex?Q"> learning is an off-policy (when updating the value function choose a different way to sample the action) value-based method that uses a TD approach to train its action-value function.</p>
<p>Before move on, we explain the off-policy. In RL, we usually use <img src="https://latex.codecogs.com/png.latex?%5Cepsilon"> greedy policy to choose the actions. That is for a given state <img src="https://latex.codecogs.com/png.latex?s">, we take the action by sample <img src="https://latex.codecogs.com/png.latex?p%5Cin%20%5B0,1%5D">: <img src="https://latex.codecogs.com/png.latex?%0Af(x)%20=%0A%5Cbegin%7Bcases%7D%0A%5Cpi%5E*(s)%20&amp;%20%5Ctext%7B$p%5Cleq%5Cepsilon$%7D,%20%5C%5C%0A%5Ctext%7Brandom%20action%7D%20&amp;%20%5Ctext%7Botherwise%7D.%0A%5Cend%7Bcases%7D%0A"> This is a combination of exploration and eploitation. And each time, when we train the <img src="https://latex.codecogs.com/png.latex?Q"> function, we update it like <span id="eq-q-update"><img src="https://latex.codecogs.com/png.latex?%0AQ(S_t,%20A_t)%20%5Cleftarrow%20Q(S_t,%20A_t)%20+%5Calpha%20(R_%7Bt+1%7D+%5Cgamma%20%5Cmax_a%20Q(S_%7Bt+1%7D,%20a)%20-Q(S_t,%20A_t))%0A%5Ctag%7B1%7D"></span></p>
<p>For certain case with finite number of state and actions, we can easily use a table to record the <img src="https://latex.codecogs.com/png.latex?Q"> function. However, for some infinite number of states and actions, we need more complicated expression. For example, a math function, and abstract that function we can get the deep neural network <img src="https://latex.codecogs.com/png.latex?Q">. This is how we can infer the <a href="https://www.nature.com/articles/nature14236">DQN, a nature paper</a>. This basically tell us the value of <img src="https://latex.codecogs.com/png.latex?Q_%5Ctheta%20(s,%20a)">.</p>
<p>A DQN algorithm is:</p>
<ol type="1">
<li>Initialize replay memory <img src="https://latex.codecogs.com/png.latex?D"> to capacity <img src="https://latex.codecogs.com/png.latex?N"></li>
<li>Initialize action-value function <img src="https://latex.codecogs.com/png.latex?Q"> with random weights <img src="https://latex.codecogs.com/png.latex?%5Ctheta"></li>
<li>Initialize target action-value function <img src="https://latex.codecogs.com/png.latex?%5Chat%7BQ%7D"> with weights <img src="https://latex.codecogs.com/png.latex?%5Ctheta%5E%7B-%7D%20=%20%5Ctheta"></li>
<li>For episode = <img src="https://latex.codecogs.com/png.latex?1,%20M"> do
<ul>
<li>Initialize sequence <img src="https://latex.codecogs.com/png.latex?s_1%20=%20%5C%7Bx_1%5C%7D"> and preprocessed sequence <img src="https://latex.codecogs.com/png.latex?%5Cphi_1%20=%20%5Cphi(s_1)"></li>
<li>For <img src="https://latex.codecogs.com/png.latex?t%20=%201,%20T"> do
<ul>
<li>With probability <img src="https://latex.codecogs.com/png.latex?%5Cvarepsilon"> select a random action <img src="https://latex.codecogs.com/png.latex?a_t"> otherwise select <img src="https://latex.codecogs.com/png.latex?a_t%20=%20%5Ctext%7Bargmax%7D_a%20Q(%5Cphi(s_t),%20a;%20%5Ctheta)"></li>
<li>Execute action <img src="https://latex.codecogs.com/png.latex?a_t"> in emulator and observe reward <img src="https://latex.codecogs.com/png.latex?r_t"> and image <img src="https://latex.codecogs.com/png.latex?x_%7Bt+1%7D"></li>
<li>Set <img src="https://latex.codecogs.com/png.latex?s_%7Bt+1%7D%20=%20s_t,%20a_t,%20x_%7Bt+1%7D"> and preprocess <img src="https://latex.codecogs.com/png.latex?%5Cphi_%7Bt+1%7D%20=%20%5Cphi(s_%7Bt+1%7D)"></li>
<li>Store transition <img src="https://latex.codecogs.com/png.latex?(%5Cphi_t,%20a_t,%20r_t,%20%5Cphi_%7Bt+1%7D)"> in <img src="https://latex.codecogs.com/png.latex?D"></li>
<li>Sample random minibatch of transitions <img src="https://latex.codecogs.com/png.latex?(%5Cphi_j,%20a_j,%20r_j,%20%5Cphi_%7Bj+1%7D)"> from <img src="https://latex.codecogs.com/png.latex?D"></li>
<li>Set <img src="https://latex.codecogs.com/png.latex?y_j%20=%20%5Cleft%5C%7B%5Cbegin%7Barray%7D%7Bll%7D%0Ar_j%20&amp;%20%5Ctext%7Bif%20episode%20terminates%20at%20step%20%7D%20j+1%20%5C%5C%0Ar_j%20+%20%5Cgamma%20%5Cmax_%7Ba'%7D%20%5Chat%7BQ%7D(%5Cphi_%7Bj+1%7D,%20a';%20%5Ctheta%5E%7B-%7D)%20&amp;%20%5Ctext%7Botherwise%7D%0A%5Cend%7Barray%7D%5Cright."></li>
<li>Perform a gradient descent step on <img src="https://latex.codecogs.com/png.latex?(y_j%20-%20Q(%5Cphi_j,%20a_j;%20%5Ctheta))%5E2"> with respect to the network parameters <img src="https://latex.codecogs.com/png.latex?%5Ctheta"></li>
<li>Every <img src="https://latex.codecogs.com/png.latex?C"> steps reset <img src="https://latex.codecogs.com/png.latex?%5Chat%7BQ%7D%20=%20Q"></li>
</ul></li>
</ul></li>
</ol>
<p>Here <img src="https://latex.codecogs.com/png.latex?%5Cphi"> represent some feature encoder! For example, if the state can be represented as image. Then, <img src="https://latex.codecogs.com/png.latex?%5Cphi"> is something like the RGB value extractor. From the DQN algorithm above, we notice that the gradient descent is applied on the loss term of <span id="eq-mse"><img src="https://latex.codecogs.com/png.latex?%0A(y_j%20-%20Q(%5Cphi_j,%20a_j;%20%5Ctheta))%5E2%0A%5Ctag%7B2%7D"></span></p>
<p>This is to make the learned <img src="https://latex.codecogs.com/png.latex?Q"> function to approximate the value of the predicted <img src="https://latex.codecogs.com/png.latex?Q"> value. If we revisit the Equation&nbsp;1, we notice that the original Q value update is to directly update the <img src="https://latex.codecogs.com/png.latex?Q(S_t,%20A_t)">, and the goal is to reduce the difference between <img src="https://latex.codecogs.com/png.latex?R_%7Bt+1%7D+%5Cgamma%20%5Cmax_a%20Q(S_%7Bt+1%7D,%20a)"> and the <img src="https://latex.codecogs.com/png.latex?Q(S_t,%20A_t)">. In the context of the DQN, we can direcly construct the Equation&nbsp;2 for it! One notation here is that in the Equation&nbsp;1, we set the <img src="https://latex.codecogs.com/png.latex?R_%7Bt+1%7D">, with the same subscript as <img src="https://latex.codecogs.com/png.latex?S_%7Bt+1%7D">, but in the algorithm described above, we have it expressed as <img src="https://latex.codecogs.com/png.latex?r_%7Bj%7D"> with the <code>-1</code> subscript compared to <img src="https://latex.codecogs.com/png.latex?%5Cphi_%7Bj+1%7D">. However, the two terms are the same, we use <img src="https://latex.codecogs.com/png.latex?t"> since it represents the time step. For the use of <img src="https://latex.codecogs.com/png.latex?j">, it is one step of generated rewards! It is just different notation.</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Tip
</div>
</div>
<div class="callout-body-container callout-body">
<p>Why do we use Equation&nbsp;2? Q is the quality value, and it is used to estimate the total expected rewards based on the current state and the action. Suppose we already have the best <img src="https://latex.codecogs.com/png.latex?Q">, then <img src="https://latex.codecogs.com/png.latex?Q(%5Cphi_j,%20a_j)"> should be equal to the reward after we take the action <img src="https://latex.codecogs.com/png.latex?a_j">, and then based on the state <img src="https://latex.codecogs.com/png.latex?%5Cphi_%7Bj+1%7D">, the best rewards we can expect, and we use a greedy algorithm here.</p>
</div>
</div>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Tip
</div>
</div>
<div class="callout-body-container callout-body">
<p>It is always good to visit the code implementation to make sure you understand the detail.</p>
</div>
</div>
<p>We can study a <a href="https://huggingface.co/learn/deep-rl-course/en/unit3/deep-q-algorithm">DQN example from the atari simulation</a>, and the full github code can be accessed there. Now, we will combine the algorithm and the code to introduce more about the DQN.</p>
<p>Firstly, we need to have data generation process, and we can use</p>
<div class="sourceCode" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1">next_obs, rewards, terminations, truncations, infos <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> envs.step(actions)</span></code></pre></div>
<p>to get the variables <img src="https://latex.codecogs.com/png.latex?R,%20S,%20A"> and so on. And we save the generated data to the replay</p>
<div class="sourceCode" id="cb2" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1">rb.add(obs, real_next_obs, actions, rewards, terminations, infos)</span></code></pre></div>
<p>And the update process is like</p>
<div class="sourceCode" id="cb3" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1">data <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> rb.sample(args.batch_size)</span>
<span id="cb3-2"><span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">with</span> torch.no_grad():</span>
<span id="cb3-3">   target_max, _ <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> target_network(data.next_observations).<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">max</span>(dim<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>)</span>
<span id="cb3-4">   <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># data.dones is 0 or 1.</span></span>
<span id="cb3-5">   td_target <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> data.rewards.flatten() <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> args.gamma <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> target_max <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> (<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> data.dones.flatten())</span>
<span id="cb3-6">old_val <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> q_network(data.observations).gather(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, data.actions).squeeze()</span>
<span id="cb3-7">loss <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> F.mse_loss(td_target, old_val)</span></code></pre></div>
</section>
<section id="the-policy-based-method" class="level3">
<h3 class="anchored" data-anchor-id="the-policy-based-method">The policy based method</h3>
<p>We can also train the policy directly <img src="https://latex.codecogs.com/png.latex?%5Cpi_%5Ctheta">. It is more intuitive. Compared to the value-based method, it has pros and cons.</p>
<p>For pros: (a) Can explore stochastic polify, no need for the exploration and exploitation effort; (b) More effective in high-dimensional action space, especially the continuous actions spaces; (c) Better convergence properties, the curve is smoother.</p>
<p>For the cons: (a) Often get suboptimal result; (b) Take longer time to train; (c) Policy gradient have high variance (<strong>The policy gradient in different step has really different result</strong>).</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Tip
</div>
</div>
<div class="callout-body-container callout-body">
<p>Note that Q learning method needs the <code>argmax</code> to get the best action. And if the action is a continuous space, we need to do some pretty complicated optimization to get the result!</p>
</div>
</div>
<p>Note that <img src="https://latex.codecogs.com/png.latex?%5Cpi_%5Ctheta%20(s)%20=%20%5Cmathbb%7BP%7D(A%7Cs;%5Ctheta)">. Thus, the training basically becomes that when we have postive reward, we should increase the proability of the state and action pair. Otherwise, decrease it. The objective function is still the total rewards! <img src="https://latex.codecogs.com/png.latex?%0AJ(%5Ctheta)%20=%20%5Cmathbb%7BE%7D_%7B%5Ctau%5Csim%20%5Cpi%7D%5BR(%5Ctau)%5D,%0A"> where <img src="https://latex.codecogs.com/png.latex?%5Ctau"> is a trajectory (a whole simulation process). We already have a theorem to update the policy:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cnabla_%5Ctheta%20J(%5Ctheta)=%5Cmathbb%7BE%7D_%7B%5Cpi_%5Ctheta%7D%5Cleft%5B%5Cnabla_%5Ctheta%20%5Clog%20%5Cpi_%5Ctheta%5Cleft(a_t%20%5Cmid%20s_t%5Cright)%20R(%5Ctau)%5Cright%5D%0A"></p>
<p>which is valid for any differentiable policy and for any policy objective function! To better understand the process, we introduce the <strong>Monte Carlo Reinforce</strong>. In a loop:</p>
<ul>
<li>Use the policy <img src="https://latex.codecogs.com/png.latex?%5Cpi_%5Ctheta"> to collect an episode <img src="https://latex.codecogs.com/png.latex?%5Ctau"></li>
<li>Use the episode to estimate the gradient <img src="https://latex.codecogs.com/png.latex?%5Chat%7Bg%7D=%5Cnabla_%5Ctheta%20J(%5Ctheta)"> <img src="https://latex.codecogs.com/png.latex?%0A%5Cnabla_%5Ctheta%20J(%5Ctheta)%20%5Capprox%20%5Chat%7Bg%7D=%5Csum_%7Bt=0%7D%20%5Cnabla_%5Ctheta%20%5Clog%20%5Cpi_%5Ctheta%5Cleft(a_t%20%5Cmid%20s_t%5Cright)%20R(%5Ctau)%0A"></li>
<li>Update the weights of the policy: <img src="https://latex.codecogs.com/png.latex?%5Ctheta%20%5Cleftarrow%20%5Ctheta+%5Calpha%20%5Chat%7Bg%7D">. (Gradient ascent)</li>
</ul>
<p>Alternatively, we can collect multiple trajectories (helpful to mitigate the variance), and the gradient becomes <img src="https://latex.codecogs.com/png.latex?%0A%5Cnabla_%5Ctheta%20J(%5Ctheta)%20%5Capprox%20%5Chat%7Bg%7D=%5Cfrac%7B1%7D%7Bm%7D%5Csum_%7Bi=1%7D%5Em%5Csum_%7Bt=0%7D%20%5Cnabla_%5Ctheta%20%5Clog%20%5Cpi_%5Ctheta%5Cleft(a_t%5E%7B(i)%7D%20%5Cmid%20s_t%5E%7B(i)%7D%5Cright)%20R(%5Ctau%5E%7B(i)%7D).%0A"></p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Tip
</div>
</div>
<div class="callout-body-container callout-body">
<p>We can treat the <img src="https://latex.codecogs.com/png.latex?%5Cnabla_%5Ctheta%20%5Clog%5Cpi_%5Ctheta(a_t%5Cmid%20s_t)"> is the direction of the steeppest increase of the log probability of selected action based on the <img src="https://latex.codecogs.com/png.latex?s_t">. This is because that we wish to maximize the objective here (rewards).</p>
</div>
</div>
<p>For the derivation of the policy gradient theorem, check the following:</p>
<div id="thm-policy-gradient-theorem" class="theorem">
<p><span class="theorem-title"><strong>Theorem 1 (policy-gradient-theorem)</strong></span> The derivation of the policy gradient theorem is as the following:</p>
<p><span id="eq-policy"><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Baligned%7D%0A%5Cnabla_%5Ctheta%20J(%5Ctheta)%20%20&amp;=%20%5Cmathbb%7BE%7D_%7B%5Ctau%5Csim%20%5Cpi%7D%5BR(%5Ctau)%5D%20%5C%5C%0A%20%20%20&amp;=%20%5Cnabla_%5Ctheta%20%5Csum_%7B%5Ctau%7DP(%5Ctau;%5Ctheta)R(%5Ctau)%20%5C%5C%0A%20%20%20&amp;=%20%5Csum_%7B%5Ctau%7D%20%5Cnabla_%5Ctheta%20P(%5Ctau;%5Ctheta)R(%5Ctau)%20%5C%5C%0A%20%20%20&amp;=%20%5Csum_%7B%5Ctau%7D%20P(%5Ctau;%5Ctheta)%20%5Cfrac%7B%5Cnabla_%5Ctheta%20P(%5Ctau;%5Ctheta)%7D%7BP(%5Ctau;%5Ctheta)%7DR(%5Ctau)%20%5C%5C%0A%20%20%20&amp;=%20%5Csum_%7B%5Ctau%7D%20P(%5Ctau;%5Ctheta)%20%5Cnabla_%5Ctheta%20%5Clog%20P(%5Ctau;%5Ctheta)R(%5Ctau)%20%5C%5C%0A%20%20%20&amp;=%20%5Csum_%7B%5Ctau%7D%20P(%5Ctau;%5Ctheta)%20%5Cnabla_%5Ctheta%5Clog%20%5B%5Cphi(s_0)%5Cprod_%7Bt=0%7D%5ET%20P(s_%7Bt+1%7D%7Cs_t,%20a_t)%5Cpi_%5Ctheta%20(a_t%5Cmid%20s_t)%5D%20R(%5Ctau)%5C%5C%0A%20%20%20&amp;=%20%5Csum_%7B%5Ctau%7D%20P(%5Ctau;%5Ctheta)%20%5Cnabla_%5Ctheta%5Cleft%5B%5Clog%5Cphi(s_0)%20+%20%5Clog%5Csum_%7Bt=0%7D%5ET%20P(s_%7Bt+1%7D%7Cs_t,%20a_t)%20+%5Clog%5Csum_%7Bt=0%7D%5ET%5Cpi_%5Ctheta%20(a_t%5Cmid%20s_t)%5Cright%5D%20R(%5Ctau)%5C%5C%0A%20%20%20&amp;=%20%5Csum_%7B%5Ctau%7D%20P(%5Ctau;%5Ctheta)%20%5Cnabla_%5Ctheta%5Cleft%5B%5Clog%5Csum_%7Bt=0%7D%5ET%5Cpi_%5Ctheta%20(a_t%5Cmid%20s_t)%5Cright%5D%20R(%5Ctau).%0A%5Cend%7Baligned%7D%0A%5Ctag%7B3%7D"></span></p>
</div>
<p>For the code part, using the cartpole as an example, the policy framework would be</p>
<div class="sourceCode" id="cb4" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">class</span> Policy(nn.Module):</span>
<span id="cb4-2">    <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">def</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">__init__</span>(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>, s_size, a_size, h_size):</span>
<span id="cb4-3">        <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">super</span>(Policy, <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>).<span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">__init__</span>()</span>
<span id="cb4-4">        <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.fc1 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> nn.Linear(s_size, h_size)</span>
<span id="cb4-5">        <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.fc2 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> nn.Linear(h_size, a_size)</span>
<span id="cb4-6"></span>
<span id="cb4-7">    <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">def</span> forward(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>, x):</span>
<span id="cb4-8">        x <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> F.relu(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.fc1(x))</span>
<span id="cb4-9">        x <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.fc2(x)</span>
<span id="cb4-10">        <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">return</span> F.softmax(x, dim<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>)</span>
<span id="cb4-11"></span>
<span id="cb4-12">    <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">def</span> act(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>, state):</span>
<span id="cb4-13">        state <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> torch.from_numpy(state).<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">float</span>().unsqueeze(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>).to(device)</span>
<span id="cb4-14">        probs <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.forward(state).cpu()</span>
<span id="cb4-15">        m <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> Categorical(probs)</span>
<span id="cb4-16">        action <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.argmax(m)</span>
<span id="cb4-17">        <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">return</span> action.item(), m.log_prob(action)</span></code></pre></div>
<p>Note that we need to use the <code>Categorical</code> from <code>torch.distributions</code> to enable the backpropagation. The reinforce process can be constructed according to the reinfoce algorithm introduced above.</p>
<div class="sourceCode" id="cb5" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">def</span> reinforce(policy, optimizer, n_training_episodes, max_t, gamma, print_every):</span>
<span id="cb5-2">    scores_deque <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> deque(maxlen<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">100</span>)</span>
<span id="cb5-3">    scores <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> []</span>
<span id="cb5-4">    <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">for</span> i_episode <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, n_training_episodes <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>):</span>
<span id="cb5-5">        saved_log_probs <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> []</span>
<span id="cb5-6">        rewards <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> []</span>
<span id="cb5-7">        state <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> env.reset()</span>
<span id="cb5-8">        <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">for</span> t <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(max_t):</span>
<span id="cb5-9">            action, log_prob <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> policy.act(state)</span>
<span id="cb5-10">            saved_log_probs.append(log_prob)</span>
<span id="cb5-11">            state, reward, done, _ <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> env.step(action)</span>
<span id="cb5-12">            rewards.append(reward)</span>
<span id="cb5-13">            <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">if</span> done:</span>
<span id="cb5-14">                <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">break</span></span>
<span id="cb5-15">        scores_deque.append(<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">sum</span>(rewards))</span>
<span id="cb5-16">        scores.append(<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">sum</span>(rewards))</span>
<span id="cb5-17"></span>
<span id="cb5-18">        returns <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> deque(maxlen<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>max_t)</span>
<span id="cb5-19">        n_steps <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">len</span>(rewards)</span>
<span id="cb5-20">        </span>
<span id="cb5-21">        <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">for</span> t <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(n_steps)[::<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>]:</span>
<span id="cb5-22">            disc_return_t <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> returns[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>] <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">if</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">len</span>(returns) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">&gt;</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span> <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">else</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span></span>
<span id="cb5-23">            returns.appendleft(gamma <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> disc_return_t <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> rewards[t])</span>
<span id="cb5-24"></span>
<span id="cb5-25">        <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">## standardization of the returns is employed to make training more stable</span></span>
<span id="cb5-26">        eps <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.finfo(np.float32).eps.item()</span>
<span id="cb5-27"></span>
<span id="cb5-28">        <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">## eps is the smallest representable float, which is</span></span>
<span id="cb5-29">        <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># added to the standard deviation of the returns to avoid numerical instabilities</span></span>
<span id="cb5-30">        returns <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> torch.tensor(returns)</span>
<span id="cb5-31">        returns <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> (returns <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> returns.mean()) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span> (returns.std() <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> eps)</span>
<span id="cb5-32"></span>
<span id="cb5-33">        policy_loss <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> []</span>
<span id="cb5-34">        <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">for</span> log_prob, disc_return <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">zip</span>(saved_log_probs, returns):</span>
<span id="cb5-35">            policy_loss.append(<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span>log_prob <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> disc_return)</span>
<span id="cb5-36">        policy_loss <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> torch.cat(policy_loss).<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">sum</span>()</span>
<span id="cb5-37"></span>
<span id="cb5-38">        optimizer.zero_grad()</span>
<span id="cb5-39">        policy_loss.backward()</span>
<span id="cb5-40">        optimizer.step()</span>
<span id="cb5-41"></span>
<span id="cb5-42">        <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">if</span> i_episode <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%</span> print_every <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">==</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>:</span>
<span id="cb5-43">            <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Episode </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{}</span><span class="ch" style="color: #20794D;
background-color: null;
font-style: inherit;">\t</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">Average Score: </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{:.2f}</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span>.<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">format</span>(i_episode, np.mean(scores_deque)))</span>
<span id="cb5-44">    <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">return</span> scores</span></code></pre></div>
</section>
<section id="the-actor-critic-method-and-ppo" class="level3">
<h3 class="anchored" data-anchor-id="the-actor-critic-method-and-ppo">The actor-critic method and PPO</h3>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Tip
</div>
</div>
<div class="callout-body-container callout-body">
<p>SOTA, we usually use a mixed method containing both policy based and value based methods.</p>
</div>
</div>
<p>The motivation of the actor-critic method is to lower the variation of the policy method. We can use a large number of the trajectories but it is not efficient. Therefore, we choose a new method called actor-critic method. That is to say, instead of giving rewards/feedback to the policy (actor) after many trajectories, we can use critic to give instant feedback to evaluate the actions taken by the policy. Now, we have two network to train:</p>
<ul>
<li>A policy function with parameters <img src="https://latex.codecogs.com/png.latex?%5Cpi_%5Ctheta(s)">;</li>
<li>A value function with parameters <img src="https://latex.codecogs.com/png.latex?%7Bq%7D_w(s,%20a)"></li>
</ul>
<p>This is a combined methods of the policy-based and value-based methods. For one step of time <img src="https://latex.codecogs.com/png.latex?t"></p>
<ol type="1">
<li>At time step <img src="https://latex.codecogs.com/png.latex?t">, we have the state <img src="https://latex.codecogs.com/png.latex?s_t">;</li>
<li>We have the policy <img src="https://latex.codecogs.com/png.latex?%5Cpi_%5Ctheta(s_t)%20=%20a_t">;</li>
<li>Now, we can compute the Q-value by the value function directly as <img src="https://latex.codecogs.com/png.latex?Q_t=%7Bq%7D_w(s,%20a)">;</li>
<li>Execute the action <img src="https://latex.codecogs.com/png.latex?a_t"> and get the new state <img src="https://latex.codecogs.com/png.latex?s_%7Bt+1%7D"> and new reward <img src="https://latex.codecogs.com/png.latex?r_%7Bt+1%7D">.</li>
<li>Update the policy parameters using the Q value;</li>
<li>Using the updated parameters to get the next action <img src="https://latex.codecogs.com/png.latex?a_%7Bt+1%7D">, and use the new action to update critic parameters.</li>
</ol>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Tip
</div>
</div>
<div class="callout-body-container callout-body">
<p>In policy based function, Equation&nbsp;3 needs to use <img src="https://latex.codecogs.com/png.latex?R(%5Ctau)">, and <img src="https://latex.codecogs.com/png.latex?R(%5Ctau)"> is obtained by iterative experiments. Now, we can use Q value since they represent the same meaning. Also, when we update the Q parameters, we use <code>argmax</code> to get the best action, now we use the updated policy to calculate the best action. This actor-critic is somewhat like the iterative-optimization methods seen in many math problems.</p>
</div>
</div>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Tip
</div>
</div>
<div class="callout-body-container callout-body">
<p>To stabilize the training, now we tend to use the advantage function to replace the Q value.</p>
</div>
</div>
<p>PPO is an algorithm based on the actor-critic method, and it is to clip the ratio which indicates the difference of policy to [<img src="https://latex.codecogs.com/png.latex?1-%5Cepsilon">, <img src="https://latex.codecogs.com/png.latex?1+%5Cepsilon">].</p>
<p>To do so, we just need to the change the policy objection function (with advantage function) from <img src="https://latex.codecogs.com/png.latex?%0AJ(%5Ctheta)%20=%20%5Cmathbb%7BE%7D_t%5Cleft%5B%20%5Clog%5Cpi_%5Ctheta%20(a_t%5Cmid%20s_t)*A_t%20%5Cright%5D%0A"></p>
<p>to <img src="https://latex.codecogs.com/png.latex?%0AJ(%5Ctheta)=%5Chat%7B%5Cmathbb%7BE%7D%7D_t%5Cleft%5B%5Cmin%20%5Cleft(r_t(%5Ctheta)%20%5Chat%7BA%7D_t,%20%5Coperatorname%7Bclip%7D%5Cleft(r_t(%5Ctheta),%201-%5Cepsilon,%201+%5Cepsilon%5Cright)%20%5Chat%7BA%7D_t%5Cright)%5Cright%5D%0A"></p>
<p>where the ratio is <img src="https://latex.codecogs.com/png.latex?%0Ar_t(%5Ctheta)=%5Cfrac%7B%5Cpi_%5Ctheta%5Cleft(a_t%20%5Cmid%20s_t%5Cright)%7D%7B%5Cpi_%7B%5Ctheta_%7B%5Ctext%20%7Bold%20%7D%7D%7D%5Cleft(a_t%20%5Cmid%20s_t%5Cright)%7D.%0A"></p>
<p>Now, we use a PPO implementation to better study the algorithm above. The <a href="https://github.com/vwxyzjn/cleanrl/blob/master/cleanrl/ppo.py">full code implementation can be found here</a>. There is also <a href="https://iclr-blog-track.github.io/2022/03/25/ppo-implementation-details/">another wonderful post about PPO implementation</a>. Let’s study the code now.</p>
<section id="define-both-the-actor-and-critic" class="level4">
<h4 class="anchored" data-anchor-id="define-both-the-actor-and-critic">Define both the actor and critic</h4>
<p>We usually define the network directly!</p>
<div class="sourceCode" id="cb6" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">class</span> Agent(nn.Module):</span>
<span id="cb6-2">    <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">def</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">__init__</span>(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>, envs):</span>
<span id="cb6-3">        <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">super</span>().<span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">__init__</span>()</span>
<span id="cb6-4">        <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.critic <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> nn.Sequential(</span>
<span id="cb6-5">            layer_init(nn.Linear(np.array(envs.single_observation_space.shape).prod(), <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">64</span>)),</span>
<span id="cb6-6">            nn.Tanh(),</span>
<span id="cb6-7">            layer_init(nn.Linear(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">64</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">64</span>)),</span>
<span id="cb6-8">            nn.Tanh(),</span>
<span id="cb6-9">            layer_init(nn.Linear(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">64</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>), std<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">1.0</span>),</span>
<span id="cb6-10">        )</span>
<span id="cb6-11">        <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.actor <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> nn.Sequential(</span>
<span id="cb6-12">            layer_init(nn.Linear(np.array(envs.single_observation_space.shape).prod(), <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">64</span>)),</span>
<span id="cb6-13">            nn.Tanh(),</span>
<span id="cb6-14">            layer_init(nn.Linear(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">64</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">64</span>)),</span>
<span id="cb6-15">            nn.Tanh(),</span>
<span id="cb6-16">            layer_init(nn.Linear(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">64</span>, envs.single_action_space.n), std<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.01</span>),</span>
<span id="cb6-17">        )</span>
<span id="cb6-18"></span>
<span id="cb6-19">    <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">def</span> get_value(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>, x):</span>
<span id="cb6-20">        <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">return</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.critic(x)</span>
<span id="cb6-21"></span>
<span id="cb6-22">    <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">def</span> get_action_and_value(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>, x, action<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span>):</span>
<span id="cb6-23">        logits <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.actor(x)</span>
<span id="cb6-24">        probs <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> Categorical(logits<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>logits)</span>
<span id="cb6-25">        <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">if</span> action <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">is</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span>:</span>
<span id="cb6-26">            action <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> probs.sample()</span>
<span id="cb6-27">        <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">return</span> action, probs.log_prob(action), probs.entropy(), <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.critic(x)</span></code></pre></div>
<p>Note that we don’t have q value here since the PPO uses the advantage value which means we don’t need the Q value anymore! And you may observe that critic output is a single dim value.</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Tip
</div>
</div>
<div class="callout-body-container callout-body">
<p>In pytorch, the forward process is not necessarily defined in <code>forward()</code> function. We often use it since it has customization so that <code>model(**params)</code> is equal to <code>model.forward(**params)</code>.</p>
</div>
</div>
</section>
<section id="deal-with-the-advantage-values" class="level4">
<h4 class="anchored" data-anchor-id="deal-with-the-advantage-values">Deal with the advantage values</h4>
<p>The action value is simply as <img src="https://latex.codecogs.com/png.latex?%0AA(s_t,%20a_t)%20=%20Q(s_t,%20a_t)%20-%20V(s_t)%20=%20r%20+%20%5Cgamma%20V(s_%7Bt+1%7D)%20-%20V(s)%0A"> Here, we use <img src="https://latex.codecogs.com/png.latex?r%20+%20%5Cgamma%20V(s_%7Bt+1%7D)"> to appriximate the <img src="https://latex.codecogs.com/png.latex?Q"> value, but recall in the DQN algorithm, we use it as well!</p>
</section>
</section>
<section id="apply-ppo-to-llm" class="level3">
<h3 class="anchored" data-anchor-id="apply-ppo-to-llm">Apply PPO to LLM</h3>
<p>Now, we discuss the pivotal topic of this blog. How to consider the LLM training as a PPO.</p>
<p>We use the concept of RL, and explain how LLM can be used here.</p>
<ul>
<li><strong>environment</strong>: The language world, when you output a new word, it will be added as the context of the conversation. The observation/state is the existing generation and the initial language;</li>
<li><strong>state</strong>: The existing generation and the initial language;</li>
<li><strong>agent</strong>: The LLM model it self. We have <code>LLM(curr_words) = next_token</code>. Here <img src="https://latex.codecogs.com/png.latex?%5Cpi_%5Ctheta"> = LLM;</li>
<li><strong>reward</strong>: Can be customized, and we usually choose to add a linear layer (two-heads output) to the last embedding layer of the LLM as the reward function.</li>
</ul>
<p>The step of the PPO can be formulated as the following:</p>
<ol type="1">
<li>Given the preference pair (<img src="https://latex.codecogs.com/png.latex?y_%7BY%7D">, <img src="https://latex.codecogs.com/png.latex?y_%7BN%7D">), we train a reward model. The reward model can be trained using the following loss: <img src="https://latex.codecogs.com/png.latex?%0A%5Cmathcal%7BL%7D_R%5Cleft(r_%5Cphi,%20%5Cmathcal%7BD%7D%5Cright)=-%5Cmathbb%7BE%7D_%7B%5Cleft(x,%20y_Y,%20y_N%5Cright)%20%5Csim%20%5Cmathcal%7BD%7D%7D%5Cleft%5B%5Clog%20%5Csigma%5Cleft(r_%5Cphi%5Cleft(x,%20y_Y%5Cright)-r_%5Cphi%5Cleft(x,%20y_N%5Cright)%5Cright)%5Cright%5D%0A"></li>
<li>After the have the reward function, we freeze the parameters <img src="https://latex.codecogs.com/png.latex?%5Cphi"> and train the <img src="https://latex.codecogs.com/png.latex?%5Ctheta"> by optimization of <img src="https://latex.codecogs.com/png.latex?%0A%5Cmax%20_%7B%5Cpi_%5Ctheta%7D%20%5Cmathbb%7BE%7D_%7Bx%20%5Csim%20%5Cmathcal%7BD%7D,%20y%20%5Csim%20%5Cpi_%5Ctheta(y%20%5Cmid%20x)%7D%5Cleft%5Br_%5Cphi(x,%20y)%5Cright%5D-%5Cbeta%20%5Cmathbb%7BD%7D_%7B%5Cmathrm%7BKL%7D%7D%5Cleft%5B%5Cpi_%5Ctheta(y%20%5Cmid%20x)%20%5C%7C%20%5Cpi_%7B%5Cmathrm%7Bref%7D%7D(y%20%5Cmid%20x)%5Cright%5D%0A"></li>
</ol>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Tip
</div>
</div>
<div class="callout-body-container callout-body">
<p>The here should be perceived as a probability function. Thus, <img src="https://latex.codecogs.com/png.latex?%5Cpi_%5Ctheta(y%7Cx)"> will output a probability!</p>
<p>Actually, in the case of LLM, we have <img src="https://latex.codecogs.com/png.latex?%0A%5Cpi_%5Ctheta(y%7Cx)%20=%20p(y%7Cx;%20%5Ctext%7BLLM%7D)%20=%20p(y_%7B0%7D%7Cx,%20y)%5Cprod_%7Bi=1%7D%5ETp(y_%7B1%7D%7Cx,%20y_%7B0,...,i-1%7D;%20%5Ctext%7BLLM%7D)%0A"></p>
</div>
</div>
</section>
<section id="dpo" class="level3">
<h3 class="anchored" data-anchor-id="dpo">DPO</h3>
<p>DPO is another method inspired by the limitation of the PPO. In the case of direct preference of choosing from two results. The human preference distribution <img src="https://latex.codecogs.com/png.latex?p%5E*"> can be expressed with reward function: <img src="https://latex.codecogs.com/png.latex?%0Ap%5E*%5Cleft(y_1%20%5Csucc%20y_2%20%5Cmid%20x%5Cright)=%5Cfrac%7B%5Cexp%20%5Cleft(r%5E*%5Cleft(x,%20y_1%5Cright)%5Cright)%7D%7B%5Cexp%20%5Cleft(r%5E*%5Cleft(x,%20y_1%5Cright)%5Cright)+%5Cexp%20%5Cleft(r%5E*%5Cleft(x,%20y_2%5Cright)%5Cright)%7D%0A"></p>
<p>The DPO paper indicate that we can express the probability under the policy <img src="https://latex.codecogs.com/png.latex?%5Cpi%5E*"> with</p>
<p><img src="https://latex.codecogs.com/png.latex?%0Ap%5E*%5Cleft(y_1%20%5Csucc%20y_2%20%5Cmid%20x%5Cright)=%5Cfrac%7B1%7D%7B1+%5Cexp%20%5Cleft(%5Cbeta%20%5Clog%20%5Cfrac%7B%5Cpi%5E*%5Cleft(y_2%20%5Cmid%20x%5Cright)%7D%7B%5Cpi_%7B%5Cmathrm%7Bref%7D%7D%5Cleft(y_2%20%5Cmid%20x%5Cright)%7D-%5Cbeta%20%5Clog%20%5Cfrac%7B%5Cpi%5E*%5Cleft(y_1%20%5Cmid%20x%5Cright)%7D%7B%5Cpi_%7B%5Cmathrm%7Bref%7D%7D%5Cleft(y_1%20%5Cmid%20x%5Cright)%7D%5Cright)%7D%0A"></p>
<p>Therefore, we don’t need the real PPO now. And we just need to do something like a SFT with a different loss function: <img src="https://latex.codecogs.com/png.latex?%0A%5Cmathcal%7BL%7D_%7B%5Cmathrm%7BDPO%7D%7D%5Cleft(%5Cpi_%5Ctheta%20;%20%5Cpi_%7B%5Cmathrm%7Bref%7D%7D%5Cright)=-%5Cmathbb%7BE%7D_%7B%5Cleft(x,%20y_w,%20y_l%5Cright)%20%5Csim%20%5Cmathcal%7BD%7D%7D%5Cleft%5B%5Clog%20%5Csigma%5Cleft(%5Cbeta%20%5Clog%20%5Cfrac%7B%5Cpi_%5Ctheta%5Cleft(y_w%20%5Cmid%20x%5Cright)%7D%7B%5Cpi_%7B%5Ctext%20%7Bref%20%7D%7D%5Cleft(y_w%20%5Cmid%20x%5Cright)%7D-%5Cbeta%20%5Clog%20%5Cfrac%7B%5Cpi_%5Ctheta%5Cleft(y_l%20%5Cmid%20x%5Cright)%7D%7B%5Cpi_%7B%5Ctext%20%7Bref%20%7D%7D%5Cleft(y_l%20%5Cmid%20x%5Cright)%7D%5Cright)%5Cright%5D%20.%0A"></p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Tip
</div>
</div>
<div class="callout-body-container callout-body">
<p>During training, the <img src="https://latex.codecogs.com/png.latex?%5Cpi_%7Bref%7D"> is freezed!</p>
</div>
</div>


</section>
</section>

 ]]></description>
  <category>Large Language Models</category>
  <guid>https://alexchen4ai.github.io/blog/notes/Large Language Model/rl_llm.html</guid>
  <pubDate>Thu, 15 Feb 2024 08:00:00 GMT</pubDate>
</item>
</channel>
</rss>
