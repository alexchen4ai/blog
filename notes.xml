<?xml version="1.0" encoding="UTF-8"?>
<rss  xmlns:atom="http://www.w3.org/2005/Atom" 
      xmlns:media="http://search.yahoo.com/mrss/" 
      xmlns:content="http://purl.org/rss/1.0/modules/content/" 
      xmlns:dc="http://purl.org/dc/elements/1.1/" 
      version="2.0">
<channel>
<title>Alex Chen&#39;s Blog</title>
<link>https://alexchen4ai.github.io/blog/notes.html</link>
<atom:link href="https://alexchen4ai.github.io/blog/notes.xml" rel="self" type="application/rss+xml"/>
<description>Personal summaries and insights gathered from reading various research papers and articles.</description>
<generator>quarto-1.4.549</generator>
<lastBuildDate>Thu, 15 Feb 2024 08:00:00 GMT</lastBuildDate>
<item>
  <title>Reinforcement learning for large language model</title>
  <dc:creator>Alex Chen</dc:creator>
  <link>https://alexchen4ai.github.io/blog/notes/Large Language Model/orca.html</link>
  <description><![CDATA[ 





<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Tip
</div>
</div>
<div class="callout-body-container callout-body">
<p>Reinforment is a common technique, which can be applied to the large language model area.</p>
</div>
</div>
<section id="background-of-reinforcement-learning" class="level2">
<h2 class="anchored" data-anchor-id="background-of-reinforcement-learning">Background of reinforcement learning</h2>
In the first section, we will review the fundamental concept of the reinforcement learning. The fundamental part of the reinforcement learning includes the <strong>agent</strong> and <strong>environment</strong>. The process is as the following:
<p align="center">
<img src="https://alexchen4ai.github.io/blog/images/RL_basic.png" width="60%">
</p>
<p>At each iteration step, we have the state of the environement marked as <img src="https://latex.codecogs.com/png.latex?S">, the action <img src="https://latex.codecogs.com/png.latex?A"> and the reward <img src="https://latex.codecogs.com/png.latex?R">. Below, we list the step at the time step <img src="https://latex.codecogs.com/png.latex?t">:</p>
<ol type="1">
<li>Based on the current state <img src="https://latex.codecogs.com/png.latex?S_t">, the agent make the action <img src="https://latex.codecogs.com/png.latex?A_t">;</li>
<li>The environment react to the action and transit to the state <img src="https://latex.codecogs.com/png.latex?S_%7Bt+1%7D"> and reward <img src="https://latex.codecogs.com/png.latex?R_%7Bt+1%7D">.</li>
</ol>
<p>Therefore, related to each action, we will have a state of <img src="https://latex.codecogs.com/png.latex?S_t,%20A_t,%20S_%7Bt+1%7D,%20R_%7Bt+1%7D">. And these four variables will be the critical data used for the reinforcement learning! Now, let me introduce more about the <strong>glossary</strong> of the reinforcement learning terms.</p>
<ul>
<li><em>Markov chain</em>: The Markov chain means that the action taken by the agent is only dependent on the most recent state/present state, and is independent of past states.</li>
<li><em>Observation/State</em>: The state is the complete description while the observation is just the partial description. The partial description means part of the state.</li>
<li><em>policy</em>: The policy is usually denoted as <img src="https://latex.codecogs.com/png.latex?%5Cpi"> and it is used to decide which action <img src="https://latex.codecogs.com/png.latex?a"> to take. According to the Markov chain, we have <img src="https://latex.codecogs.com/png.latex?%5Cpi(s)=a">.</li>
<li><em>reward</em>: Reward is the value that we can get immediately after we take a new action. For example, in cartpole example, we get every positive feedback if the cartpole doesn‚Äôt fail.</li>
<li><em>Value</em>: The value function to calculate the discounted sum of all future rewards! Thus, the values are different from the reward.</li>
</ul>
<p>These are some basic concepts in the reinforcement learning! We will introduce more advanced concept along with more topics involved below. We revisit the fundamental part of the RL: The agent can repeated to take actions and get feedback (rewards/values) from the environment so that it can update the agent itself to behave better to get best reward or values. The deep learning and pytorch is not designed for the RL, and RL is more a mathematically which may not naturally suited for the deep learning. Rather, we design some equation to apply the deep learning. Thus, when we design the RL, we need to think from the fundamental math, and deep learning is just a method to solve a math problem.</p>
</section>
<section id="the-classification-of-rl" class="level2">
<h2 class="anchored" data-anchor-id="the-classification-of-rl">The classification of RL</h2>
<p>To solve the RL problem, we have various methods! The detailed is concluded in the figure below. We will study more about the policy based method, the value based method. And for SOTA, the LLM usuaully use a combined method. When we consider how to train the RL, we should first think about how to use the pretrained model. We wish the model to guide us to get the best action to take at every step! Thus, we need a great policy <img src="https://latex.codecogs.com/png.latex?%5Cpi%5E*">!.</p>
<p align="center">
<img src="https://alexchen4ai.github.io/blog/images/RL_classification.png" width="100%">
</p>
<section id="the-value-based-method" class="level3">
<h3 class="anchored" data-anchor-id="the-value-based-method">The value based method</h3>
<p>The famous <img src="https://latex.codecogs.com/png.latex?Q"> learning is a typical value-based method. The original paper can be accessed <a href="https://link.springer.com/content/pdf/10.1007/BF00992698.pdf"><strong>here</strong></a>. The <img src="https://latex.codecogs.com/png.latex?Q"> is the abbreviate of <em>quality</em>. The value based method has two submethods called the state-value function and the action-value function. Usually, we use <img src="https://latex.codecogs.com/png.latex?V"> to represent the value, which is</p>
<p><img src="https://latex.codecogs.com/png.latex?%0AV_%7B%5Cpi%7D(s)%20=%20%5Cmathbb%7BE%7D_%7B%5Cpi%7D%5Cleft%5B%20R_%7Bt+1%7D+%5Cgamma%20R_%7Bt+2%7D%20+%20%5Cgamma%5E2R_%7Bt+3%7D+...%20%7C%20S_t=s%20%5Cright%5D%0A"></p>
<p>Let me clarify the equation above in a probability. The <img src="https://latex.codecogs.com/png.latex?%5Cpi"> is like a distribution, and we may express the value as</p>
<p><img src="https://latex.codecogs.com/png.latex?%0AV_%7B%5Cpi%7D(s)%20=%20%5Cmathbb%7BE%7D_%7B%5Ctau%5Csim%5Cpi%7D%5Cleft%5B%20R_%7Bt+1%7D+%5Cgamma%20R_%7Bt+2%7D%20+%20%5Cgamma%5E2R_%7Bt+3%7D+...%20%7C%20S_t=s%20%5Cright%5D%0A"></p>
<p>since we have <img src="https://latex.codecogs.com/png.latex?a%5Csim%20%5Cpi(s)">. And <img src="https://latex.codecogs.com/png.latex?a"> is directly relevant to the trajectory <img src="https://latex.codecogs.com/png.latex?%5Ctau"> which can be used for comprehensive rewards. Now, we have known the value function, this is a value that can evaluate the current confidence to get the best reward based on the current state! Another better and granular method is not just the current state, but also the action. And we introduce the <img src="https://latex.codecogs.com/png.latex?Q"> value. However, fundamentally, we have <img src="https://latex.codecogs.com/png.latex?Q"> and <img src="https://latex.codecogs.com/png.latex?V"> to express the same meaning, the confidence or the estimated quality of the current condition. The only difference is that the <img src="https://latex.codecogs.com/png.latex?Q"> function also count in the actions.</p>
<p>The comparison would be <img src="https://latex.codecogs.com/png.latex?V_%5Cpi%20(s)=%5Cmathbb%7BE%7D_%5Cpi%20%5BG_t%7CS_t=s%5D"> vs.&nbsp;<img src="https://latex.codecogs.com/png.latex?Q_%7B%5Cpi%7D(s,%20a)=%5Cmathbb%7BE_%5Cpi%7D%5BG_t%7CS_t=s,%20A_t=a%5D">. The <img src="https://latex.codecogs.com/png.latex?G_t"> here represent the ending state. Then, as stated above how do we get the best policy? We can use</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cpi%5E*%20=%20%5Ctext%7Barg%7D%5Cmax_a%20Q%5E*(s,%20a)%0A"></p>
<p>To simulate the RL, we usually need to simulate the whole episode, like a cartpole example would continue until it fails. However, there are ways to simplify the process by <strong>Bellman equation</strong>: <img src="https://latex.codecogs.com/png.latex?%0AV_%5Cpi(s)%20=%20%5Cmathbb%7BE%7D_%7B%5Cpi%7D%20%5BR_%7Bt+1%7D+%5Cgamma%20*%20V_%7B%5Cpi%7D(S_%7Bt+1%7D)%7CS_t=s%5D.%0A"> And we can update the value function by <strong>Monte Carlo</strong> or the <strong>Temporary Difference</strong> method. The <img src="https://latex.codecogs.com/png.latex?Q"> learning is an off-policy (when updating the value function choose a different way to sample the action) value-based method that uses a TD approach to train its action-value function.</p>
<p>Before move on, we explain the off-policy. In RL, we usually use <img src="https://latex.codecogs.com/png.latex?%5Cepsilon"> greedy policy to choose the actions. That is for a given state <img src="https://latex.codecogs.com/png.latex?s">, we take the action by sample <img src="https://latex.codecogs.com/png.latex?p%5Cin%20%5B0,1%5D">: <img src="https://latex.codecogs.com/png.latex?%0Af(x)%20=%0A%5Cbegin%7Bcases%7D%0A%5Cpi%5E*(s)%20&amp;%20%5Ctext%7B$p%5Cleq%5Cepsilon$%7D,%20%5C%5C%0A%5Ctext%7Brandom%20action%7D%20&amp;%20%5Ctext%7Botherwise%7D.%0A%5Cend%7Bcases%7D%0A"> This is a combination of exploration and eploitation. And each time, when we train the <img src="https://latex.codecogs.com/png.latex?Q"> function, we update it like <img src="https://latex.codecogs.com/png.latex?%0AQ(S_t,%20A_t)%20%5Cleftarrow%20Q(S_t,%20A_t)%20+%5Calpha%20(R_%7Bt+1%7D+%5Cgamma%20%5Cmax_a%20Q(S_%7Bt+1%7D,%20a)%20-Q(S_t,%20A_t))%0A"></p>
<p>For certain case with finite number of state and actions, we can easily use a table to record the <img src="https://latex.codecogs.com/png.latex?Q"> function. However, for some infinite number of states and actions, we need more complicated expression. For example, a math function, and abstract that function we can get the deep neural network <img src="https://latex.codecogs.com/png.latex?Q">. This is how we can infer the <a href="https://www.nature.com/articles/nature14236">DQN, a natural paper</a>. This basically tell us the value of <img src="https://latex.codecogs.com/png.latex?Q_%5Ctheta%20(s,%20a)">.</p>
<p>A DQN algorithm is:</p>
<ol type="1">
<li>Initialize replay memory <img src="https://latex.codecogs.com/png.latex?D"> to capacity <img src="https://latex.codecogs.com/png.latex?N"></li>
<li>Initialize action-value function <img src="https://latex.codecogs.com/png.latex?Q"> with random weights <img src="https://latex.codecogs.com/png.latex?%5Ctheta"></li>
<li>Initialize target action-value function <img src="https://latex.codecogs.com/png.latex?%5Chat%7BQ%7D"> with weights <img src="https://latex.codecogs.com/png.latex?%5Ctheta%5E%7B-%7D%20=%20%5Ctheta"></li>
<li>For episode = <img src="https://latex.codecogs.com/png.latex?1,%20M"> do
<ul>
<li>Initialize sequence <img src="https://latex.codecogs.com/png.latex?s_1%20=%20%5C%7Bx_1%5C%7D"> and preprocessed sequence <img src="https://latex.codecogs.com/png.latex?%5Cphi_1%20=%20%5Cphi(s_1)"></li>
<li>For <img src="https://latex.codecogs.com/png.latex?t%20=%201,%20T"> do
<ul>
<li>With probability <img src="https://latex.codecogs.com/png.latex?%5Cvarepsilon"> select a random action <img src="https://latex.codecogs.com/png.latex?a_t"> otherwise select <img src="https://latex.codecogs.com/png.latex?a_t%20=%20%5Ctext%7Bargmax%7D_a%20Q(%5Cphi(s_t),%20a;%20%5Ctheta)"></li>
<li>Execute action <img src="https://latex.codecogs.com/png.latex?a_t"> in emulator and observe reward <img src="https://latex.codecogs.com/png.latex?r_t"> and image <img src="https://latex.codecogs.com/png.latex?x_%7Bt+1%7D"></li>
<li>Set <img src="https://latex.codecogs.com/png.latex?s_%7Bt+1%7D%20=%20s_t,%20a_t,%20x_%7Bt+1%7D"> and preprocess <img src="https://latex.codecogs.com/png.latex?%5Cphi_%7Bt+1%7D%20=%20%5Cphi(s_%7Bt+1%7D)"></li>
<li>Store transition <img src="https://latex.codecogs.com/png.latex?(%5Cphi_t,%20a_t,%20r_t,%20%5Cphi_%7Bt+1%7D)"> in <img src="https://latex.codecogs.com/png.latex?D"></li>
<li>Sample random minibatch of transitions <img src="https://latex.codecogs.com/png.latex?(%5Cphi_j,%20a_j,%20r_j,%20%5Cphi_%7Bj+1%7D)"> from <img src="https://latex.codecogs.com/png.latex?D"></li>
<li>Set <img src="https://latex.codecogs.com/png.latex?y_j%20=%20%5Cleft%5C%7B%5Cbegin%7Barray%7D%7Bll%7D%0Ar_j%20&amp;%20%5Ctext%7Bif%20episode%20terminates%20at%20step%20%7D%20j+1%20%5C%5C%0Ar_j%20+%20%5Cgamma%20%5Cmax_%7Ba'%7D%20%5Chat%7BQ%7D(%5Cphi_%7Bj+1%7D,%20a';%20%5Ctheta%5E%7B-%7D)%20&amp;%20%5Ctext%7Botherwise%7D%0A%5Cend%7Barray%7D%5Cright."></li>
<li>Perform a gradient descent step on <img src="https://latex.codecogs.com/png.latex?(y_j%20-%20Q(%5Cphi_j,%20a_j;%20%5Ctheta))%5E2"> with respect to the network parameters <img src="https://latex.codecogs.com/png.latex?%5Ctheta"></li>
<li>Every <img src="https://latex.codecogs.com/png.latex?C"> steps reset <img src="https://latex.codecogs.com/png.latex?%5Chat%7BQ%7D%20=%20Q"></li>
</ul></li>
</ul></li>
</ol>
<p>Here <img src="https://latex.codecogs.com/png.latex?%5Cphi"> represent some feature encoder! For example, if the state can be represented as image. Then, <img src="https://latex.codecogs.com/png.latex?%5Cphi"> is something like the RGB value extractor.</p>
</section>
<section id="the-policy-based-method" class="level3">
<h3 class="anchored" data-anchor-id="the-policy-based-method">The policy based method</h3>
<p>We can also train the policy directly <img src="https://latex.codecogs.com/png.latex?%5Cpi_%5Ctheta"> and set the loss as the rewarding function! It is more intuitive but it becomes hard to converge and takes really long time to train.</p>
<p>Note that <img src="https://latex.codecogs.com/png.latex?%5Cpi_%5Ctheta%20(s)%20=%20%5Cmathbb%7BP%7D(A%7Cs;%5Ctheta)">. Thus, the training basically becomes that when we have postive reward, we should increase the proability of the state and action pair. Otherwise, decrease it. The objective function is still the total rewards! <img src="https://latex.codecogs.com/png.latex?%0AJ(%5Ctheta)%20=%20%5Cmathbb%7BE%7D_%7B%5Ctau%5Csim%20%5Cpi%7D%5BR(%5Ctau)%5D,%0A"> where <img src="https://latex.codecogs.com/png.latex?%5Ctau"> is a trajectory (a whole simulation process). We already have a theorem to update the policy:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cnabla_%5Ctheta%20J(%5Ctheta)=%5Cmathbb%7BE%7D_%7B%5Cpi_%5Ctheta%7D%5Cleft%5B%5Cnabla_%5Ctheta%20%5Clog%20%5Cpi_%5Ctheta%5Cleft(a_t%20%5Cmid%20s_t%5Cright)%20R(%5Ctau)%5Cright%5D%0A"></p>
<p>which is valid for any differentiable policy and for any policy objective function! To better understand the process, we introduce the <strong>Monte Carlo Reinforce</strong>. In a loop:</p>
<ul>
<li>Use the policy <img src="https://latex.codecogs.com/png.latex?%5Cpi_%5Ctheta"> to collect an episode <img src="https://latex.codecogs.com/png.latex?%5Ctau"></li>
<li>Use the episode to estimate the gradient <img src="https://latex.codecogs.com/png.latex?%5Chat%7Bg%7D=%5Cnabla_%5Ctheta%20J(%5Ctheta)"> <img src="https://latex.codecogs.com/png.latex?%0A%5Cnabla_%5Ctheta%20J(%5Ctheta)%20%5Capprox%20%5Chat%7Bg%7D=%5Csum_%7Bt=0%7D%20%5Cnabla_%5Ctheta%20%5Clog%20%5Cpi_%5Ctheta%5Cleft(a_t%20%5Cmid%20s_t%5Cright)%20R(%5Ctau)%0A"></li>
<li>Update the weights of the policy: <img src="https://latex.codecogs.com/png.latex?%5Ctheta%20%5Cleftarrow%20%5Ctheta+%5Calpha%20%5Chat%7Bg%7D">. (Gradient descent)</li>
</ul>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Tip
</div>
</div>
<div class="callout-body-container callout-body">
<p>Now, we usually use a mixed method containing both policy based and value based methods.</p>
</div>
</div>
</section>
<section id="the-actor-critic-method-and-ppo" class="level3">
<h3 class="anchored" data-anchor-id="the-actor-critic-method-and-ppo">The actor-critic method and PPO</h3>
<p>TODO‚Ä¶</p>


</section>
</section>

 ]]></description>
  <category>Large Language Models</category>
  <guid>https://alexchen4ai.github.io/blog/notes/Large Language Model/orca.html</guid>
  <pubDate>Thu, 15 Feb 2024 08:00:00 GMT</pubDate>
</item>
<item>
  <title>Stable diffusion model</title>
  <dc:creator>Alex Chen</dc:creator>
  <link>https://alexchen4ai.github.io/blog/notes/Diffusion Model/sd.html</link>
  <description><![CDATA[ 





<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Tip
</div>
</div>
<div class="callout-body-container callout-body">
<p>A text to image generation model from the diffusion architecture.</p>
</div>
</div>
<p>üìù <strong>Paper</strong>: <a href="https://arxiv.org/abs/2112.10752">https://arxiv.org/abs/2112.10752</a></p>



 ]]></description>
  <category>Diffusion Model</category>
  <guid>https://alexchen4ai.github.io/blog/notes/Diffusion Model/sd.html</guid>
  <pubDate>Sat, 03 Feb 2024 08:00:00 GMT</pubDate>
</item>
</channel>
</rss>
