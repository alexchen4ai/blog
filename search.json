[
  {
    "objectID": "notes/Diffusion Model/sd.html",
    "href": "notes/Diffusion Model/sd.html",
    "title": "Scalable diffusion models with transformers",
    "section": "",
    "text": "Tip\n\n\n\nA text to image generation model from the diffusion architecture.\n\n\nüìù Paper: https://arxiv.org/abs/2212.09748",
    "crumbs": [
      "Home",
      "üí° **Diffusion models**",
      "Scalable diffusion models with transformers"
    ]
  },
  {
    "objectID": "notes/Large Language Model/moe.html",
    "href": "notes/Large Language Model/moe.html",
    "title": "Mixture of expert",
    "section": "",
    "text": "Tip\n\n\n\nMoE means the mixture of expert. In this blog, we will introduce the type of MoE used in the mixtral8x7B model. The blog will assume that you already understand the llama2 model. Thus, we will not revisit any knowledge already inside the llama2 model. Another focus of this blog will combine the code implementation to delve into the arc of MoE.",
    "crumbs": [
      "Home",
      "üó£Ô∏è **Large language models**",
      "Mixture of expert"
    ]
  },
  {
    "objectID": "notes/Large Language Model/moe.html#difference-from-the-llama2",
    "href": "notes/Large Language Model/moe.html#difference-from-the-llama2",
    "title": "Mixture of expert",
    "section": "Difference from the llama2",
    "text": "Difference from the llama2\nIn causal LM, we know that the decoder model can be decomposed into multiple small parts. Let‚Äôs visit the architecture of the llama2 in the Figure¬†1.\n\n\n\n\n\n\nFigure¬†1: The architecture of the llama2 model\n\n\n\nThere are many components inside the llama2 architecure, like the attention layer, positional embedding, RMS norm, FC etc. The only difference here is the block of FF SwiGLU (FF is the feedforward network and the implementation is add the hidden embedding dimension first, through the activation function and finally decrease the dimension). Instead of using only one FF function, we use the mixture of expert, which export is the actually a FF. In llama2, the code implementation would be as simple as:\nclass LlamaMLP(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.config = config\n        self.hidden_size = config.hidden_size\n        self.intermediate_size = config.intermediate_size\n        self.gate_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)\n        self.up_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)\n        self.down_proj = nn.Linear(self.intermediate_size, self.hidden_size, bias=False)\n        self.act_fn = ACT2FN[config.hidden_act]\n\n    def forward(self, x):\n        down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n        return down_proj\nOK, now we will add more complexity to this function. ::: {.callout-tip} Note that all other parts are the same except the feedforward block. :::",
    "crumbs": [
      "Home",
      "üó£Ô∏è **Large language models**",
      "Mixture of expert"
    ]
  },
  {
    "objectID": "notes/Large Language Model/moe.html#mathematical-insights",
    "href": "notes/Large Language Model/moe.html#mathematical-insights",
    "title": "Mixture of expert",
    "section": "Mathematical insights",
    "text": "Mathematical insights\nThe FF model is actually the expert! For the llama2 model, we only have one expert, therefore, in the case of llama2, we have\n\\[\ny = E(x),\n\\]\nwhere \\(x, y\\) are the value before and after the FF block. In the MoE, we actually prepare multiple trainable experts, so there are multiple E layers. A gating network is thus introduced to decide which network should be used. And now the expression becomes:\n\\[\ny = \\sum_{i=1}^n G(x)_i E_i(x).\n\\]\nIt is special to choose the network G. Here, we just introduce the found research work from the paper Switch Transformer which is also the famous Mixtral-8x7B.\nSpecially for this model, n=8. Suppose the last dimension of x is d, we will include the dimension of each variables for following up explanation.\nThe steps to construct the G(x) are:\n\nSet trainable linear layer \\(W_g\\) of size d*n. \\[\nH(x)= x \\cdot W_{\\mathrm{g}}\n\\] and we know the dimension of \\(H(x)\\) is \\(n=8\\).\nOnly pick the top K experts: \\[\n\\operatorname{KeepTopK}(v, k)_i= \\begin{cases}v_i & \\text { if } v_i \\text { is in the top } k \\text { elements of } v \\\\ -\\infty & \\text { otherwise. }\\end{cases}\n\\]\nApply the softmax to get the final G(x) \\[\nG(x)=\\operatorname{Norm}(\\operatorname{Softmax}(\\operatorname{KeepTopK}(H(x), k)))\n\\]\n\n\n\n\n\n\n\nTip\n\n\n\nSet to \\(-\\infty\\) so that it becomes zero during softmax. Thus, for the final output, we only have \\(k\\) experts which are really used.\n\n\n\n\n\n\n\n\nTip\n\n\n\nFor different tokens (tokens in the same sequence and batch could be routed to different experts), they will choose different experts! For example the batch[0] will choose the expert 0 and 3, while the batch[1] will choose the expert 4 and 7.",
    "crumbs": [
      "Home",
      "üó£Ô∏è **Large language models**",
      "Mixture of expert"
    ]
  },
  {
    "objectID": "notes/Large Language Model/moe.html#load-balancing-loss",
    "href": "notes/Large Language Model/moe.html#load-balancing-loss",
    "title": "Mixture of expert",
    "section": "Load balancing loss",
    "text": "Load balancing loss\nSince different portion of total tokens will enter different experts, like the unbalanced dataset problem, we need to add a load balancing loss. Given \\(N\\) experts indexed by \\(i=1\\) to \\(N\\) and a batch \\(\\mathcal{B}\\) with \\(T\\) tokens, the auxiliary loss is computed as the scaled dot-product between vectors \\(f\\) and \\(P\\), \\[\n\\text { loss }=\\alpha \\cdot N \\cdot \\sum_{i=1}^N f_i \\cdot P_i\n\\] where \\(f_i\\) is the fraction of tokens dispatched to expert \\(i\\), \\[\nf_i=\\frac{1}{T} \\sum_{x \\in \\mathcal{B}} \\mathbb{1}\\{\\operatorname{argmax} p(x)=i\\}\n\\] and \\(P_i\\) is the fraction of the router probability allocated for expert \\(i,{ }^2\\) \\[\nP_i=\\frac{1}{T} \\sum_{x \\in \\mathcal{B}} p_i(x)\n\\]\nWe add this loss since we want to encourages uniform routing since the loss is minimized when \\[\nf_i = P_i = \\frac{1}{N}.\n\\]\n\nTheorem 1 To prove that the minimum of the objective function \\(\\mathbf{a} \\cdot \\mathbf{b}=\\sum_{i=1}^N a_i b_i\\) is achieved when \\(a_i=\\) \\(b_i=\\frac{1}{N}\\) under the given constraints, we use the method of Lagrange multipliers for the constraints:\nGiven constraints:\n\n\\(\\sum_{i=1}^N a_i=1, a_i \\geq 0\\).\n\\(\\sum_{i=1}^N b_i=1, b_i \\geq 0\\).\n\nObjective Function to minimize:\n\n\\(L=\\mathbf{a} \\cdot \\mathbf{b}-\\lambda\\left(\\sum_{i=1}^N a_i-1\\right)-\\mu\\left(\\sum_{i=1}^N b_i-1\\right)\\), where \\(\\lambda\\) and \\(\\mu\\) are Lagrange multipliers.\n\nTaking partial derivatives of \\(L\\) with respect to \\(a_i, b_i, \\lambda\\), and \\(\\mu\\) and setting them to zero gives:\n\n\\(\\frac{\\partial L}{\\partial a_i}=b_i-\\lambda=0 \\Rightarrow b_i=\\lambda\\).\n\\(\\frac{\\partial L}{\\partial b_i}=a_i-\\mu=0 \\Rightarrow a_i=\\mu\\).\n\\(\\frac{\\partial L}{\\partial \\lambda}=\\sum_{i=1}^N a_i-1=0\\).\n\\(\\frac{\\partial L}{\\partial \\mu}=\\sum_{i=1}^N b_i-1=0\\).\n\nFrom equations 1 and 2 , all \\(a_i\\) and \\(b_i\\) must be constant for all \\(i\\), because they equal the respective Lagrange multipliers \\(\\lambda\\) and \\(\\mu\\), which are constants. Thus, \\(a_i=a\\) and \\(b_i=b\\) for some constants \\(a\\) and \\(b\\) for all \\(i\\).\nGiven the constraints \\(\\sum_{i=1}^N a_i=1\\) and \\(\\sum_{i=1}^N b_i=1\\), and knowing that \\(a_i=a\\) and \\(b_i=b\\) for all \\(i\\), we have:\n\n\\(\\sum_{i=1}^N a=N \\cdot a=1 \\Rightarrow a=\\frac{1}{N}\\).\n\\(\\sum_{i=1}^N b=N \\cdot b=1 \\Rightarrow b=\\frac{1}{N}\\).\n\nTherefore, setting \\(a_i=b_i=\\frac{1}{N}\\) for all \\(i\\) satisfies the constraints and minimizes the objective function \\(\\mathbf{a} \\cdot \\mathbf{b}=\\sum_{i=1}^N a_i b_i\\), as any local minimum in a convex function over a convex set is a global minimum.",
    "crumbs": [
      "Home",
      "üó£Ô∏è **Large language models**",
      "Mixture of expert"
    ]
  },
  {
    "objectID": "notes/Large Language Model/llm_eval.html",
    "href": "notes/Large Language Model/llm_eval.html",
    "title": "Large language model evaluation",
    "section": "",
    "text": "Tip\n\n\n\nToday, the landscape of large language models (LLMs) is rich with diverse evaluation benchmarks. In this blog post, we‚Äôll explore the various benchmarks used to assess language models and guide you through the process of obtaining these benchmarks after running a language model.",
    "crumbs": [
      "Home",
      "üó£Ô∏è **Large language models**",
      "Large language model evaluation"
    ]
  },
  {
    "objectID": "notes/Large Language Model/llm_eval.html#llm-leaderboard",
    "href": "notes/Large Language Model/llm_eval.html#llm-leaderboard",
    "title": "Large language model evaluation",
    "section": "LLM leaderboard",
    "text": "LLM leaderboard\nNumerous leaderboards exist for Large Language Models (LLMs), each compiled based on the benchmarks of these models. By examining these leaderboards, we can identify which benchmarks are particularly effective and informative for evaluating the capabilities of LLMs.\n\nHuggingface LLM Leaderboard;\nStreamlit Leaderboard;\nLMSYS Leaderboard;\nCan AI code leaderboard.\n\nAlso, we can get more benchmark from the papers for sure. üìù Paper: Evaluating Large Language Models: A Comprehensive Survey can provide a full explanation about it.",
    "crumbs": [
      "Home",
      "üó£Ô∏è **Large language models**",
      "Large language model evaluation"
    ]
  },
  {
    "objectID": "notes/Large Language Model/llm_eval.html#classification-of-llm-evaluation",
    "href": "notes/Large Language Model/llm_eval.html#classification-of-llm-evaluation",
    "title": "Large language model evaluation",
    "section": "Classification of LLM evaluation",
    "text": "Classification of LLM evaluation\nThere are different benchmarks for the LLM evaluation. The general classification can be found in Figure¬†1.\n\n\n\n\n\n\n\n\nFigure¬†1: The classification of LLM evaluation.\n\n\n\nFor each aspect of the model, we will have different methods to evaluate it.\nThe knowledge and capability evaluation can be seen in Figure¬†2.\n\n\n\n\n\n\n\n\nFigure¬†2: The progress of the LLM knowledge capability evaluation.\n\n\n\nThe commonsense reasoning datasets can be seen below:\n\nThe details of commonsense reasoning datasets.\n\n\n\n\n\n\n\n\n\nDataset\nDomain\nSize\nSource\nTask\n\n\n\n\nARC\nscience\n7,787\na variety of sources\nmultiple-choice QA\n\n\nQASC\nscience\n9,980\nhuman-authored\nmultiple-choice QA\n\n\nMCTACO\ntemporal\n1,893\nMultiRC\nmultiple-choice QA\n\n\nTRACIE\ntemporal\n-\nROCStories, Wikipedia\nmultiple-choice QA\n\n\nTIMEDIAL\ntemporal\n1.1K\nDailyDialog\nmultiple-choice QA\n\n\nHellaSWAG\nevent\n20K\nActivityNet, WikiHow\nmultiple-choice QA\n\n\nPIQA\nphysical\n21K\nhuman-authored\n2-choice QA\n\n\nPep-3k\nphysical\n3,062\nhuman-authored\n2-choice QA\n\n\nSocial IQA\nsocial\n38K\nhuman-authored\nmultiple-choice QA\n\n\nCommonsenseQA\ngeneric\n12,247\nCONCEPTNET, human-authored\nmultiple-choice QA\n\n\nOpenBookQA\ngeneric\n6K\nWorldTree\nmultiple-choice QA\n\n\n\nAnd the multi-hop reasoning dataset is:\n\n\n\n\n\n\n\n\n\n\n\nDataset\nDomain\nSize\n# hops\nSource\nAnswer type\n\n\n\n\nHotpotQA\ngeneric\n112,779\n1/2/3\nWikipedia\nspan\n\n\nHybridQA\ngeneric\n69,611\n2/3\nWikitables, Wikipedia\nspan\n\n\nMultiRC\ngeneric\n9,872\n2.37\nMultiple\nMCQ\n\n\nNarrativeQA\nfiction\n46,765\n-\nMultiple\ngenerative\n\n\nMedhop\nmedline\n2,508\n-\nMedline\nMCQ\n\n\nWikihop\ngeneric\n51,318\n-\nWikipedia\nMCQ\n\n\n\nLike the knowledge and capability, there are datasets prepared for other benchmark as well.",
    "crumbs": [
      "Home",
      "üó£Ô∏è **Large language models**",
      "Large language model evaluation"
    ]
  },
  {
    "objectID": "notes/Large Language Model/llm_eval.html#benchmarks",
    "href": "notes/Large Language Model/llm_eval.html#benchmarks",
    "title": "Large language model evaluation",
    "section": "Benchmarks",
    "text": "Benchmarks\nOnce we‚Äôve acquired the dataset to assess the Large Language Model (LLM), we introduce a crucial concept known as a benchmark‚Äîa tool that quantitatively evaluates the LLM‚Äôs performance. Let‚Äôs delve deeper into the benchmarks and their significance.\n\nBenchmarks for Knowledge and Reasoning\n\n\n\n\n\n\n\n\n\nBenchmarks\n# Tasks\nLanguage\n# Instances\nEvaluation Form\n\n\n\n\nMMLU\n57\nEnglish\n15,908\nLocal\n\n\nMMCU\n51\nChinese\n11,900\nLocal\n\n\nC-Eval\n52\nChinese\n13,948\nOnline\n\n\nAGIEval\n20\nEnglish, Chinese\n8,062\nLocal\n\n\nM3KE\n71\nChinese\n20,477\nLocal\n\n\nM3Exam\n4\nEnglish and others\n12,317\nLocal\n\n\nCMMLU\n67\nChinese\n11,528\nLocal\n\n\nLucyEval\n55\nChinese\n11,000\nOnline\n\n\n\nAlso, there are some benchmark for the holistic evaluation.\n\nHolistic benchmarks\n\n\n\n\n\n\n\n\n\nBenchmarks\nLanguage\nbenchmark\nEvaluation Form\nExpandability\n\n\n\n\nHELM\nEnglish\nAutomatic\nLocal\nSupported\n\n\nBIG-bench\nEnglish and others\nAutomatic\nLocal\nSupported\n\n\nOpenCompass\nEnglish and others\nAutomatic and LLMs-based\nLocal\nSupported\n\n\nHuggingface\nEnglish\nAutomatic\nLocal\nUnsupported\n\n\nFlagEval\nEnglish and others\nAutomatic and Manual\nLocal and Online\nUnsupported\n\n\nOpenEval\nChinese\nAutomatic\nLocal\nSupported\n\n\nChatbot Arena\nEnglish and others\nManual\nOnline\nSupported",
    "crumbs": [
      "Home",
      "üó£Ô∏è **Large language models**",
      "Large language model evaluation"
    ]
  },
  {
    "objectID": "notes/Large Language Model/llm_eval.html#how-to-calculate-the-benchmark",
    "href": "notes/Large Language Model/llm_eval.html#how-to-calculate-the-benchmark",
    "title": "Large language model evaluation",
    "section": "How to calculate the benchmark",
    "text": "How to calculate the benchmark\nIn the dynamic world of artificial intelligence, benchmarks play a pivotal role in gauging the prowess of AI models. A notable platform that has garnered widespread attention for its comprehensive leaderboard is Hugging Face. Here, benchmarks such as Average, ARC, HellaSwag, MMLU, TruthfulQA, Winogrande, and GSM8K offer a bird‚Äôs-eye view of an AI model‚Äôs capabilities. To demystify the process of benchmark calculation, let‚Äôs delve into a practical example using the TruthfulQA benchmark.\n\nDiscovering TruthfulQA\nThe TruthfulQA dataset, accessible on Hugging Face (view dataset), serves as an excellent starting point. This benchmark is designed to evaluate an AI‚Äôs ability to not only generate accurate answers but also ensure they align with factual correctness.\n\n\nUnified Framework for Evaluation\nThankfully, the complexity of working across different benchmarks is significantly reduced with tools like the lm-evaluation-harness repository. This unified framework simplifies the evaluation process, allowing for a streamlined approach to assessing AI models across various benchmarks.\n\n\nTailoring Evaluation to Learning Scenarios\nThe evaluation process varies significantly depending on the learning scenario‚Äîbe it zero-shot or few-shot learning. In few-shot learning, where the model is primed with examples, prompts such as thus, the choice is: can guide the model to the correct answer format (e.g., A, B, C). For zero-shot scenarios, where the model lacks prior examples, multiple prompts may be necessary. The first prompt elicits a raw response, while subsequent prompts refine this into a final, decisive choice.\n\n\nNavigating Dataset Splits\nA critical aspect of benchmark evaluation is understanding the dataset structure, particularly the splits: train, val, and test. For benchmarks like HellaSwag, it‚Äôs crucial to fine-tune models on the train and val datasets before evaluating them on the test set. This approach ensures the model is not unfairly advantaged by exposure to test data during training, maintaining the integrity of the evaluation process.\n\n\nConclusion\nBenchmarks like TruthfulQA are indispensable for advancing AI research, providing a clear benchmark for evaluating the nuanced capabilities of AI models. By leveraging unified frameworks and adapting to the specific demands of different learning scenarios, researchers can efficiently and accurately assess their models. Remember, the key to a successful evaluation lies in understanding the dataset, choosing the right learning scenario, and meticulously following the evaluation protocol to ensure fair and accurate results.",
    "crumbs": [
      "Home",
      "üó£Ô∏è **Large language models**",
      "Large language model evaluation"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Alex Chen",
    "section": "",
    "text": "üëã Greetings! My name is Alex Chen, an AI researcher and startup founder in Silicon Valley. Previously, I pursued a PhD at Stanford University, delving into numerical simulation and artificial intelligence research. Now, I am building an AI agent UGC platform.\nIn this space, I aim to share my journey in academia and industry, providing insights into cutting-edge research and practical applications of AI and machine learning.\nIf you want to contact me, feel free to send a mail at this address.\n\nCredits:\n\nEmojis used in figures are designed by OpenMoji, the open-source emoji and icon project. License: CC BY-SA 4.0.\nVector icons are provided by Streamline (https://streamlinehq.com). License: CC BY-SA 4.0."
  },
  {
    "objectID": "notes.html",
    "href": "notes.html",
    "title": "Research notes",
    "section": "",
    "text": "Complex analysis for machine learning\n\n\n\n\n\n\nMath Theories\n\n\n\n\n\n\n\n\n\n1 min\n\n\n\n\n\n\n\n\n\n\n\n\nLarge language model evaluation\n\n\n\n\n\n\nLarge Language Models\n\n\n\n\n\n\n\n\n\n11 min\n\n\n\n\n\n\n\n\n\n\n\n\nMixture of expert\n\n\n\n\n\n\nLarge Language Models\n\n\n\n\n\n\n\n\n\n5 min\n\n\n\n\n\n\n\n\n\n\n\n\nScalable diffusion models with transformers\n\n\n\n\n\n\nDiffusion Model\n\n\n\n\n\n\n\n\n\n1 min\n\n\n\n\n\n\n\n\n\n\n\n\nReinforcement learning for large language model\n\n\n\n\n\n\nLarge Language Models\n\n\n\n\n\n\n\n\n\n19 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Wei Chen",
    "section": "",
    "text": "I am inside a Hawaii cafe\n\n\n\n\n   \n\n\nGreetings! My name is Alex Chen, an AI researcher and startup founder in Silicon Valley. Previously, I pursued a PhD at Stanford University, delving into numerical simulation and artificial intelligence research.\nIn this space, I aim to share my journey in academia and industry, providing insights into cutting-edge research and practical applications of AI and machine learning.\n\nFeel free to connect with me:\n\nEmail: weichen6@stanford.edu\nCheck out my startup: AI Tools by Nexa"
  },
  {
    "objectID": "notes/Large Language Model/rl_llm.html",
    "href": "notes/Large Language Model/rl_llm.html",
    "title": "Reinforcement learning for large language model",
    "section": "",
    "text": "Tip\n\n\n\nReinforment is a common technique, which can be applied to the large language model area.",
    "crumbs": [
      "Home",
      "üó£Ô∏è **Large language models**",
      "Reinforcement learning for large language model"
    ]
  },
  {
    "objectID": "notes/Large Language Model/rl_llm.html#background-of-reinforcement-learning",
    "href": "notes/Large Language Model/rl_llm.html#background-of-reinforcement-learning",
    "title": "Reinforcement learning for large language model",
    "section": "Background of reinforcement learning",
    "text": "Background of reinforcement learning\nIn the first section, we will review the fundamental concept of the reinforcement learning. The fundamental part of the reinforcement learning includes the agent and environment. The process is as the following:\n\n\n\nAt each iteration step, we have the state of the environement marked as \\(S\\), the action \\(A\\) and the reward \\(R\\). Below, we list the step at the time step \\(t\\):\n\nBased on the current state \\(S_t\\), the agent make the action \\(A_t\\);\nThe environment react to the action and transit to the state \\(S_{t+1}\\) and reward \\(R_{t+1}\\).\n\nTherefore, related to each action, we will have a state of \\(S_t, A_t, S_{t+1}, R_{t+1}\\). And these four variables will be the critical data used for the reinforcement learning! Now, let me introduce more about the glossary of the reinforcement learning terms.\n\nMarkov chain: The Markov chain means that the action taken by the agent is only dependent on the most recent state/present state, and is independent of past states.\nObservation/State: The state is the complete description while the observation is just the partial description. The partial description means part of the state.\npolicy: The policy is usually denoted as \\(\\pi\\) and it is used to decide which action \\(a\\) to take. According to the Markov chain, we have \\(\\pi(s)=a\\).\nreward: Reward is the value that we can get immediately after we take a new action. For example, in cartpole example, we get every positive feedback if the cartpole doesn‚Äôt fail.\nValue: The value function to calculate the discounted sum of all future rewards! Thus, the values are different from the reward.\n\nThese are some basic concepts in the reinforcement learning! We will introduce more advanced concept along with more topics involved below. We revisit the fundamental part of the RL: The agent can repeated to take actions and get feedback (rewards/values) from the environment so that it can update the agent itself to behave better to get best reward or values. The deep learning and pytorch is not designed for the RL, and RL is more a mathematically which may not naturally suited for the deep learning. Rather, we design some equation to apply the deep learning. Thus, when we design the RL, we need to think from the fundamental math, and deep learning is just a method to solve a math problem.",
    "crumbs": [
      "Home",
      "üó£Ô∏è **Large language models**",
      "Reinforcement learning for large language model"
    ]
  },
  {
    "objectID": "notes/Large Language Model/rl_llm.html#the-classification-of-rl",
    "href": "notes/Large Language Model/rl_llm.html#the-classification-of-rl",
    "title": "Reinforcement learning for large language model",
    "section": "The classification of RL",
    "text": "The classification of RL\nTo solve the RL problem, we have various methods! The detailed is concluded in the figure below. We will study more about the policy based method, the value based method. And for SOTA, the LLM usuaully use a combined method. When we consider how to train the RL, we should first think about how to use the pretrained model. We wish the model to guide us to get the best action to take at every step! Thus, we need a great policy \\(\\pi^*\\)!.\n\n\n\n\nThe value based method\nThe famous \\(Q\\) learning is a typical value-based method. The original paper can be accessed here. The \\(Q\\) is the abbreviate of quality. The value based method has two submethods called the state-value function and the action-value function. Usually, we use \\(V\\) to represent the value, which is\n\\[\nV_{\\pi}(s) = \\mathbb{E}_{\\pi}\\left[ R_{t+1}+\\gamma R_{t+2} + \\gamma^2R_{t+3}+... | S_t=s \\right]\n\\]\nLet me clarify the equation above in a probability. The \\(\\pi\\) is like a distribution, and we may express the value as\n\\[\nV_{\\pi}(s) = \\mathbb{E}_{\\tau\\sim\\pi}\\left[ R_{t+1}+\\gamma R_{t+2} + \\gamma^2R_{t+3}+... | S_t=s \\right]\n\\]\nsince we have \\(a\\sim \\pi(s)\\). And \\(a\\) is directly relevant to the trajectory \\(\\tau\\) which can be used for comprehensive rewards. Now, we have known the value function, this is a value that can evaluate the current confidence to get the best reward based on the current state! Another better and granular method is not just the current state, but also the action. And we introduce the \\(Q\\) value. However, fundamentally, we have \\(Q\\) and \\(V\\) to express the same meaning, the confidence or the estimated quality of the current condition. The only difference is that the \\(Q\\) function also count in the actions.\nThe comparison would be \\(V_\\pi (s)=\\mathbb{E}_\\pi [G_t|S_t=s]\\) vs.¬†\\(Q_{\\pi}(s, a)=\\mathbb{E_\\pi}[G_t|S_t=s, A_t=a]\\). The \\(G_t\\) here represent the ending state. Then, as stated above how do we get the best policy? We can use\n\\[\n\\pi^* = \\text{arg}\\max_a Q^*(s, a)\n\\]\nTo simulate the RL, we usually need to simulate the whole episode, like a cartpole example would continue until it fails. However, there are ways to simplify the process by Bellman equation: \\[\nV_\\pi(s) = \\mathbb{E}_{\\pi} [R_{t+1}+\\gamma * V_{\\pi}(S_{t+1})|S_t=s].\n\\] And we can update the value function by Monte Carlo or the Temporary Difference method. The \\(Q\\) learning is an off-policy (when updating the value function choose a different way to sample the action) value-based method that uses a TD approach to train its action-value function.\nBefore move on, we explain the off-policy. In RL, we usually use \\(\\epsilon\\) greedy policy to choose the actions. That is for a given state \\(s\\), we take the action by sample \\(p\\in [0,1]\\): \\[\nf(x) =\n\\begin{cases}\n\\pi^*(s) & \\text{$p\\leq\\epsilon$}, \\\\\n\\text{random action} & \\text{otherwise}.\n\\end{cases}\n\\] This is a combination of exploration and eploitation. And each time, when we train the \\(Q\\) function, we update it like \\[\nQ(S_t, A_t) \\leftarrow Q(S_t, A_t) +\\alpha (R_{t+1}+\\gamma \\max_a Q(S_{t+1}, a) -Q(S_t, A_t))\n\\tag{1}\\]\nFor certain case with finite number of state and actions, we can easily use a table to record the \\(Q\\) function. However, for some infinite number of states and actions, we need more complicated expression. For example, a math function, and abstract that function we can get the deep neural network \\(Q\\). This is how we can infer the DQN, a nature paper. This basically tell us the value of \\(Q_\\theta (s, a)\\).\nA DQN algorithm is:\n\nInitialize replay memory \\(D\\) to capacity \\(N\\)\nInitialize action-value function \\(Q\\) with random weights \\(\\theta\\)\nInitialize target action-value function \\(\\hat{Q}\\) with weights \\(\\theta^{-} = \\theta\\)\nFor episode = \\(1, M\\) do\n\nInitialize sequence \\(s_1 = \\{x_1\\}\\) and preprocessed sequence \\(\\phi_1 = \\phi(s_1)\\)\nFor \\(t = 1, T\\) do\n\nWith probability \\(\\varepsilon\\) select a random action \\(a_t\\) otherwise select \\(a_t = \\text{argmax}_a Q(\\phi(s_t), a; \\theta)\\)\nExecute action \\(a_t\\) in emulator and observe reward \\(r_t\\) and image \\(x_{t+1}\\)\nSet \\(s_{t+1} = s_t, a_t, x_{t+1}\\) and preprocess \\(\\phi_{t+1} = \\phi(s_{t+1})\\)\nStore transition \\((\\phi_t, a_t, r_t, \\phi_{t+1})\\) in \\(D\\)\nSample random minibatch of transitions \\((\\phi_j, a_j, r_j, \\phi_{j+1})\\) from \\(D\\)\nSet \\(y_j = \\left\\{\\begin{array}{ll}\nr_j & \\text{if episode terminates at step } j+1 \\\\\nr_j + \\gamma \\max_{a'} \\hat{Q}(\\phi_{j+1}, a'; \\theta^{-}) & \\text{otherwise}\n\\end{array}\\right.\\)\nPerform a gradient descent step on \\((y_j - Q(\\phi_j, a_j; \\theta))^2\\) with respect to the network parameters \\(\\theta\\)\nEvery \\(C\\) steps reset \\(\\hat{Q} = Q\\)\n\n\n\nHere \\(\\phi\\) represent some feature encoder! For example, if the state can be represented as image. Then, \\(\\phi\\) is something like the RGB value extractor. From the DQN algorithm above, we notice that the gradient descent is applied on the loss term of \\[\n(y_j - Q(\\phi_j, a_j; \\theta))^2\n\\tag{2}\\]\nThis is to make the learned \\(Q\\) function to approximate the value of the predicted \\(Q\\) value. If we revisit the Equation¬†1, we notice that the original Q value update is to directly update the \\(Q(S_t, A_t)\\), and the goal is to reduce the difference between \\(R_{t+1}+\\gamma \\max_a Q(S_{t+1}, a)\\) and the \\(Q(S_t, A_t)\\). In the context of the DQN, we can direcly construct the Equation¬†2 for it! One notation here is that in the Equation¬†1, we set the \\(R_{t+1}\\), with the same subscript as \\(S_{t+1}\\), but in the algorithm described above, we have it expressed as \\(r_{j}\\) with the -1 subscript compared to \\(\\phi_{j+1}\\). However, the two terms are the same, we use \\(t\\) since it represents the time step. For the use of \\(j\\), it is one step of generated rewards! It is just different notation.\n\n\n\n\n\n\nTip\n\n\n\nWhy do we use Equation¬†2? Q is the quality value, and it is used to estimate the total expected rewards based on the current state and the action. Suppose we already have the best \\(Q\\), then \\(Q(\\phi_j, a_j)\\) should be equal to the reward after we take the action \\(a_j\\), and then based on the state \\(\\phi_{j+1}\\), the best rewards we can expect, and we use a greedy algorithm here.\n\n\n\n\n\n\n\n\nTip\n\n\n\nIt is always good to visit the code implementation to make sure you understand the detail.\n\n\nWe can study a DQN example from the atari simulation, and the full github code can be accessed there. Now, we will combine the algorithm and the code to introduce more about the DQN.\nFirstly, we need to have data generation process, and we can use\nnext_obs, rewards, terminations, truncations, infos = envs.step(actions)\nto get the variables \\(R, S, A\\) and so on. And we save the generated data to the replay\nrb.add(obs, real_next_obs, actions, rewards, terminations, infos)\nAnd the update process is like\ndata = rb.sample(args.batch_size)\nwith torch.no_grad():\n   target_max, _ = target_network(data.next_observations).max(dim=1)\n   # data.dones is 0 or 1.\n   td_target = data.rewards.flatten() + args.gamma * target_max * (1 - data.dones.flatten())\nold_val = q_network(data.observations).gather(1, data.actions).squeeze()\nloss = F.mse_loss(td_target, old_val)\n\n\nThe policy based method\nWe can also train the policy directly \\(\\pi_\\theta\\). It is more intuitive. Compared to the value-based method, it has pros and cons.\nFor pros: (a) Can explore stochastic polify, no need for the exploration and exploitation effort; (b) More effective in high-dimensional action space, especially the continuous actions spaces; (c) Better convergence properties, the curve is smoother.\nFor the cons: (a) Often get suboptimal result; (b) Take longer time to train; (c) Policy gradient have high variance (The policy gradient in different step has really different result).\n\n\n\n\n\n\nTip\n\n\n\nNote that Q learning method needs the argmax to get the best action. And if the action is a continuous space, we need to do some pretty complicated optimization to get the result!\n\n\nNote that \\(\\pi_\\theta (s) = \\mathbb{P}(A|s;\\theta)\\). Thus, the training basically becomes that when we have postive reward, we should increase the proability of the state and action pair. Otherwise, decrease it. The objective function is still the total rewards! \\[\nJ(\\theta) = \\mathbb{E}_{\\tau\\sim \\pi}[R(\\tau)],\n\\] where \\(\\tau\\) is a trajectory (a whole simulation process). We already have a theorem to update the policy:\n\\[\n\\nabla_\\theta J(\\theta)=\\mathbb{E}_{\\pi_\\theta}\\left[\\nabla_\\theta \\log \\pi_\\theta\\left(a_t \\mid s_t\\right) R(\\tau)\\right]\n\\]\nwhich is valid for any differentiable policy and for any policy objective function! To better understand the process, we introduce the Monte Carlo Reinforce. In a loop:\n\nUse the policy \\(\\pi_\\theta\\) to collect an episode \\(\\tau\\)\nUse the episode to estimate the gradient \\(\\hat{g}=\\nabla_\\theta J(\\theta)\\) \\[\n\\nabla_\\theta J(\\theta) \\approx \\hat{g}=\\sum_{t=0} \\nabla_\\theta \\log \\pi_\\theta\\left(a_t \\mid s_t\\right) R(\\tau)\n\\]\nUpdate the weights of the policy: \\(\\theta \\leftarrow \\theta+\\alpha \\hat{g}\\). (Gradient ascent)\n\nAlternatively, we can collect multiple trajectories (helpful to mitigate the variance), and the gradient becomes \\[\n\\nabla_\\theta J(\\theta) \\approx \\hat{g}=\\frac{1}{m}\\sum_{i=1}^m\\sum_{t=0} \\nabla_\\theta \\log \\pi_\\theta\\left(a_t^{(i)} \\mid s_t^{(i)}\\right) R(\\tau^{(i)}).\n\\]\n\n\n\n\n\n\nTip\n\n\n\nWe can treat the \\(\\nabla_\\theta \\log\\pi_\\theta(a_t\\mid s_t)\\) is the direction of the steeppest increase of the log probability of selected action based on the \\(s_t\\). This is because that we wish to maximize the objective here (rewards).\n\n\nFor the derivation of the policy gradient theorem, check the following:\n\nTheorem 1 (policy-gradient-theorem) The derivation of the policy gradient theorem is as the following:\n\\[\n\\begin{aligned}\n\\nabla_\\theta J(\\theta)  &= \\mathbb{E}_{\\tau\\sim \\pi}[R(\\tau)] \\\\\n   &= \\nabla_\\theta \\sum_{\\tau}P(\\tau;\\theta)R(\\tau) \\\\\n   &= \\sum_{\\tau} \\nabla_\\theta P(\\tau;\\theta)R(\\tau) \\\\\n   &= \\sum_{\\tau} P(\\tau;\\theta) \\frac{\\nabla_\\theta P(\\tau;\\theta)}{P(\\tau;\\theta)}R(\\tau) \\\\\n   &= \\sum_{\\tau} P(\\tau;\\theta) \\nabla_\\theta \\log P(\\tau;\\theta)R(\\tau) \\\\\n   &= \\sum_{\\tau} P(\\tau;\\theta) \\nabla_\\theta\\log [\\phi(s_0)\\prod_{t=0}^T P(s_{t+1}|s_t, a_t)\\pi_\\theta (a_t\\mid s_t)] R(\\tau)\\\\\n   &= \\sum_{\\tau} P(\\tau;\\theta) \\nabla_\\theta\\left[\\log\\phi(s_0) + \\log\\sum_{t=0}^T P(s_{t+1}|s_t, a_t) +\\log\\sum_{t=0}^T\\pi_\\theta (a_t\\mid s_t)\\right] R(\\tau)\\\\\n   &= \\sum_{\\tau} P(\\tau;\\theta) \\nabla_\\theta\\left[\\log\\sum_{t=0}^T\\pi_\\theta (a_t\\mid s_t)\\right] R(\\tau).\n\\end{aligned}\n\\tag{3}\\]\n\nFor the code part, using the cartpole as an example, the policy framework would be\nclass Policy(nn.Module):\n    def __init__(self, s_size, a_size, h_size):\n        super(Policy, self).__init__()\n        self.fc1 = nn.Linear(s_size, h_size)\n        self.fc2 = nn.Linear(h_size, a_size)\n\n    def forward(self, x):\n        x = F.relu(self.fc1(x))\n        x = self.fc2(x)\n        return F.softmax(x, dim=1)\n\n    def act(self, state):\n        state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n        probs = self.forward(state).cpu()\n        m = Categorical(probs)\n        action = np.argmax(m)\n        return action.item(), m.log_prob(action)\nNote that we need to use the Categorical from torch.distributions to enable the backpropagation. The reinforce process can be constructed according to the reinfoce algorithm introduced above.\ndef reinforce(policy, optimizer, n_training_episodes, max_t, gamma, print_every):\n    scores_deque = deque(maxlen=100)\n    scores = []\n    for i_episode in range(1, n_training_episodes + 1):\n        saved_log_probs = []\n        rewards = []\n        state = env.reset()\n        for t in range(max_t):\n            action, log_prob = policy.act(state)\n            saved_log_probs.append(log_prob)\n            state, reward, done, _ = env.step(action)\n            rewards.append(reward)\n            if done:\n                break\n        scores_deque.append(sum(rewards))\n        scores.append(sum(rewards))\n\n        returns = deque(maxlen=max_t)\n        n_steps = len(rewards)\n        \n        for t in range(n_steps)[::-1]:\n            disc_return_t = returns[0] if len(returns) &gt; 0 else 0\n            returns.appendleft(gamma * disc_return_t + rewards[t])\n\n        ## standardization of the returns is employed to make training more stable\n        eps = np.finfo(np.float32).eps.item()\n\n        ## eps is the smallest representable float, which is\n        # added to the standard deviation of the returns to avoid numerical instabilities\n        returns = torch.tensor(returns)\n        returns = (returns - returns.mean()) / (returns.std() + eps)\n\n        policy_loss = []\n        for log_prob, disc_return in zip(saved_log_probs, returns):\n            policy_loss.append(-log_prob * disc_return)\n        policy_loss = torch.cat(policy_loss).sum()\n\n        optimizer.zero_grad()\n        policy_loss.backward()\n        optimizer.step()\n\n        if i_episode % print_every == 0:\n            print(\"Episode {}\\tAverage Score: {:.2f}\".format(i_episode, np.mean(scores_deque)))\n    return scores\n\n\nThe actor-critic method and PPO\n\n\n\n\n\n\nTip\n\n\n\nSOTA, we usually use a mixed method containing both policy based and value based methods.\n\n\nThe motivation of the actor-critic method is to lower the variation of the policy method. We can use a large number of the trajectories but it is not efficient. Therefore, we choose a new method called actor-critic method. That is to say, instead of giving rewards/feedback to the policy (actor) after many trajectories, we can use critic to give instant feedback to evaluate the actions taken by the policy. Now, we have two network to train:\n\nA policy function with parameters \\(\\pi_\\theta(s)\\);\nA value function with parameters \\({q}_w(s, a)\\)\n\nThis is a combined methods of the policy-based and value-based methods. For one step of time \\(t\\)\n\nAt time step \\(t\\), we have the state \\(s_t\\);\nWe have the policy \\(\\pi_\\theta(s_t) = a_t\\);\nNow, we can compute the Q-value by the value function directly as \\(Q_t={q}_w(s, a)\\);\nExecute the action \\(a_t\\) and get the new state \\(s_{t+1}\\) and new reward \\(r_{t+1}\\).\nUpdate the policy parameters using the Q value;\nUsing the updated parameters to get the next action \\(a_{t+1}\\), and use the new action to update critic parameters.\n\n\n\n\n\n\n\nTip\n\n\n\nIn policy based function, Equation¬†3 needs to use \\(R(\\tau)\\), and \\(R(\\tau)\\) is obtained by iterative experiments. Now, we can use Q value since they represent the same meaning. Also, when we update the Q parameters, we use argmax to get the best action, now we use the updated policy to calculate the best action. This actor-critic is somewhat like the iterative-optimization methods seen in many math problems.\n\n\n\n\n\n\n\n\nTip\n\n\n\nTo stabilize the training, now we tend to use the advantage function to replace the Q value.\n\n\nPPO is an algorithm based on the actor-critic method, and it is to clip the ratio which indicates the difference of policy to [\\(1-\\epsilon\\), \\(1+\\epsilon\\)].\nTo do so, we just need to the change the policy objection function (with advantage function) from \\[\nJ(\\theta) = \\mathbb{E}_t\\left[ \\log\\pi_\\theta (a_t\\mid s_t)*A_t \\right]\n\\]\nto \\[\nJ(\\theta)=\\hat{\\mathbb{E}}_t\\left[\\min \\left(r_t(\\theta) \\hat{A}_t, \\operatorname{clip}\\left(r_t(\\theta), 1-\\epsilon, 1+\\epsilon\\right) \\hat{A}_t\\right)\\right]\n\\]\nwhere the ratio is \\[\nr_t(\\theta)=\\frac{\\pi_\\theta\\left(a_t \\mid s_t\\right)}{\\pi_{\\theta_{\\text {old }}}\\left(a_t \\mid s_t\\right)}.\n\\]\nNow, we use a PPO implementation to better study the algorithm above. The full code implementation can be found here. There is also another wonderful post about PPO implementation. Let‚Äôs study the code now.\n\nDefine both the actor and critic\nWe usually define the network directly!\nclass Agent(nn.Module):\n    def __init__(self, envs):\n        super().__init__()\n        self.critic = nn.Sequential(\n            layer_init(nn.Linear(np.array(envs.single_observation_space.shape).prod(), 64)),\n            nn.Tanh(),\n            layer_init(nn.Linear(64, 64)),\n            nn.Tanh(),\n            layer_init(nn.Linear(64, 1), std=1.0),\n        )\n        self.actor = nn.Sequential(\n            layer_init(nn.Linear(np.array(envs.single_observation_space.shape).prod(), 64)),\n            nn.Tanh(),\n            layer_init(nn.Linear(64, 64)),\n            nn.Tanh(),\n            layer_init(nn.Linear(64, envs.single_action_space.n), std=0.01),\n        )\n\n    def get_value(self, x):\n        return self.critic(x)\n\n    def get_action_and_value(self, x, action=None):\n        logits = self.actor(x)\n        probs = Categorical(logits=logits)\n        if action is None:\n            action = probs.sample()\n        return action, probs.log_prob(action), probs.entropy(), self.critic(x)\nNote that we don‚Äôt have q value here since the PPO uses the advantage value which means we don‚Äôt need the Q value anymore! And you may observe that critic output is a single dim value.\n\n\n\n\n\n\nTip\n\n\n\nIn pytorch, the forward process is not necessarily defined in forward() function. We often use it since it has customization so that model(**params) is equal to model.forward(**params).\n\n\n\n\nDeal with the advantage values\nThe action value is simply as \\[\nA(s_t, a_t) = Q(s_t, a_t) - V(s_t) = r + \\gamma V(s_{t+1}) - V(s)\n\\] Here, we use \\(r + \\gamma V(s_{t+1})\\) to appriximate the \\(Q\\) value, but recall in the DQN algorithm, we use it as well!\n\n\n\nApply PPO to LLM\nNow, we discuss the pivotal topic of this blog. How to consider the LLM training as a PPO.\nWe use the concept of RL, and explain how LLM can be used here.\n\nenvironment: The language world, when you output a new word, it will be added as the context of the conversation. The observation/state is the existing generation and the initial language;\nstate: The existing generation and the initial language;\nagent: The LLM model it self. We have LLM(curr_words) = next_token. Here \\(\\pi_\\theta\\) = LLM;\nreward: Can be customized, and we usually choose to add a linear layer (two-heads output) to the last embedding layer of the LLM as the reward function.\n\nThe step of the PPO can be formulated as the following:\n\nGiven the preference pair (\\(y_{Y}\\), \\(y_{N}\\)), we train a reward model. The reward model can be trained using the following loss: \\[\n\\mathcal{L}_R\\left(r_\\phi, \\mathcal{D}\\right)=-\\mathbb{E}_{\\left(x, y_Y, y_N\\right) \\sim \\mathcal{D}}\\left[\\log \\sigma\\left(r_\\phi\\left(x, y_Y\\right)-r_\\phi\\left(x, y_N\\right)\\right)\\right]\n\\]\nAfter the have the reward function, we freeze the parameters \\(\\phi\\) and train the \\(\\theta\\) by optimization of \\[\n\\max _{\\pi_\\theta} \\mathbb{E}_{x \\sim \\mathcal{D}, y \\sim \\pi_\\theta(y \\mid x)}\\left[r_\\phi(x, y)\\right]-\\beta \\mathbb{D}_{\\mathrm{KL}}\\left[\\pi_\\theta(y \\mid x) \\| \\pi_{\\mathrm{ref}}(y \\mid x)\\right]\n\\]\n\n\n\n\n\n\n\nTip\n\n\n\nThe here should be perceived as a probability function. Thus, \\(\\pi_\\theta(y|x)\\) will output a probability!\nActually, in the case of LLM, we have \\[\n\\pi_\\theta(y|x) = p(y|x; \\text{LLM}) = p(y_{0}|x, y)\\prod_{i=1}^Tp(y_{1}|x, y_{0,...,i-1}; \\text{LLM})\n\\]\n\n\n\n\nDPO\nDPO is another method inspired by the limitation of the PPO. In the case of direct preference of choosing from two results. The human preference distribution \\(p^*\\) can be expressed with reward function: \\[\np^*\\left(y_1 \\succ y_2 \\mid x\\right)=\\frac{\\exp \\left(r^*\\left(x, y_1\\right)\\right)}{\\exp \\left(r^*\\left(x, y_1\\right)\\right)+\\exp \\left(r^*\\left(x, y_2\\right)\\right)}\n\\]\nThe DPO paper indicate that we can express the probability under the policy \\(\\pi^*\\) with\n\\[\np^*\\left(y_1 \\succ y_2 \\mid x\\right)=\\frac{1}{1+\\exp \\left(\\beta \\log \\frac{\\pi^*\\left(y_2 \\mid x\\right)}{\\pi_{\\mathrm{ref}}\\left(y_2 \\mid x\\right)}-\\beta \\log \\frac{\\pi^*\\left(y_1 \\mid x\\right)}{\\pi_{\\mathrm{ref}}\\left(y_1 \\mid x\\right)}\\right)}\n\\]\nTherefore, we don‚Äôt need the real PPO now. And we just need to do something like a SFT with a different loss function: \\[\n\\mathcal{L}_{\\mathrm{DPO}}\\left(\\pi_\\theta ; \\pi_{\\mathrm{ref}}\\right)=-\\mathbb{E}_{\\left(x, y_w, y_l\\right) \\sim \\mathcal{D}}\\left[\\log \\sigma\\left(\\beta \\log \\frac{\\pi_\\theta\\left(y_w \\mid x\\right)}{\\pi_{\\text {ref }}\\left(y_w \\mid x\\right)}-\\beta \\log \\frac{\\pi_\\theta\\left(y_l \\mid x\\right)}{\\pi_{\\text {ref }}\\left(y_l \\mid x\\right)}\\right)\\right] .\n\\]\n\n\n\n\n\n\nTip\n\n\n\nDuring training, the \\(\\pi_{ref}\\) is freezed!",
    "crumbs": [
      "Home",
      "üó£Ô∏è **Large language models**",
      "Reinforcement learning for large language model"
    ]
  },
  {
    "objectID": "notes/Math Theories/complexanalysis.html",
    "href": "notes/Math Theories/complexanalysis.html",
    "title": "Complex analysis for machine learning",
    "section": "",
    "text": "Tip\n\n\n\nThe real functional analysis is used a lot in the ML. There is also the case where the complex analysis is used. We are not talking about the very detail of the complexy analysis, we just mention its application in the era of ML.",
    "crumbs": [
      "Home",
      "‚ôæ **Math Theories**",
      "Complex analysis for machine learning"
    ]
  },
  {
    "objectID": "notes/Math Theories/complexanalysis.html#basics-or-formulas-required.",
    "href": "notes/Math Theories/complexanalysis.html#basics-or-formulas-required.",
    "title": "Complex analysis for machine learning",
    "section": "Basics or formulas required.",
    "text": "Basics or formulas required.\nIn this section, we just mention some critical formulas. In the case of complex number, we have\n\\[\ne^{i \\theta}=\\cos \\theta+i \\sin \\theta\n\\]\nThe equation can be proved if we use the Taylor series of \\(e^x\\), \\(\\operatorname{cos} x\\) and \\(\\operatorname{sin} x\\) to prove it. This formula will be highlighted when we use complex analysis in the ML.",
    "crumbs": [
      "Home",
      "‚ôæ **Math Theories**",
      "Complex analysis for machine learning"
    ]
  }
]