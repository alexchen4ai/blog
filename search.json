[
  {
    "objectID": "hello.html",
    "href": "hello.html",
    "title": "Quarto Basics",
    "section": "",
    "text": "For a demonstration of a line plot on a polar axis, see Figure¬†1.\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nr = np.arange(0, 2, 0.01)\ntheta = 2 * np.pi * r\nfig, ax = plt.subplots(\n  subplot_kw = {'projection': 'polar'} \n)\nax.plot(theta, r)\nax.set_rticks([0.5, 1, 1.5, 2])\nax.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\nFigure¬†1: A line plot on a polar axis\n\n\n\n\n\nOf course, we can also write equations here freely.\n\\[\\hat{m} = (w_1 A_1 + w_2 A_2 + \\dots + w_N A_N) (w_1 B_1 + w_2 B_2 + \\dots + w_N B_N).\\]"
  },
  {
    "objectID": "notes/Large Language Model/orca.html",
    "href": "notes/Large Language Model/orca.html",
    "title": "Orca ‚Äì Progressive Learning from Complex Explanation Traces of GPT-4",
    "section": "",
    "text": "Tip\n\n\n\nOrca is a 13B parameter LLM with ChatGPT level of performance thanks to a huge dataset of 5M samples with step-by-step explanations.\n\n\nüìù Paper: https://arxiv.org/abs/2306.02707",
    "crumbs": [
      "Home",
      "üó£Ô∏è **Large language models**",
      "Orca ‚Äì Progressive Learning from Complex Explanation Traces of GPT-4"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Alex Chen",
    "section": "",
    "text": "üëã Greetings! My name is Alex Chen, an AI researcher and startup founder in Silicon Valley. Previously, I pursued a PhD at Stanford University, delving into numerical simulation and artificial intelligence research. Now, I am building an AI agent UGC platform.\nIn this space, I aim to share my journey in academia and industry, providing insights into cutting-edge research and practical applications of AI and machine learning.\nIf you want to contact me, feel free to send a mail at this address.\n\nCredits:\n\nEmojis used in figures are designed by OpenMoji, the open-source emoji and icon project. License: CC BY-SA 4.0.\nVector icons are provided by Streamline (https://streamlinehq.com). License: CC BY-SA 4.0."
  },
  {
    "objectID": "notes.html",
    "href": "notes.html",
    "title": "Research notes",
    "section": "",
    "text": "Orca ‚Äì Progressive Learning from Complex Explanation Traces of GPT-4\n\n\n\n\n\n\nLarge Language Models\n\n\n\n\n\n\n\n\n\n1 min\n\n\n\n\n\n\n\n\n\n\n\n\nStable diffusion model\n\n\n\n\n\n\nDiffusion Model\n\n\n\n\n\n\n\n\n\n1 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "chat_model.html",
    "href": "chat_model.html",
    "title": "Chat model demo",
    "section": "",
    "text": "The chat models are very different from the other models. We should spend more time for the data, especially during training.\n\nfrom transformers import AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.1\")\n\nchat = [\n    {\"role\": \"user\", \"content\": \"Hello, how are you?\"},\n    {\"role\": \"assistant\", \"content\": \"I'm doing great. How can I help you today?\"},\n    {\"role\": \"user\", \"content\": \"I'd like to show off how chat templating works!\"},\n]\n\ntokenizer.apply_chat_template(chat, tokenize=False)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\"&lt;s&gt;[INST] Hello, how are you? [/INST]I'm doing great. How can I help you today?&lt;/s&gt; [INST] I'd like to show off how chat templating works! [/INST]\"\n\n\n\n# each model has different tokenizer! We can access the chat templates for each model\ntokenizer.chat_template\n\n\"{{ bos_token }}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if message['role'] == 'user' %}{{ '[INST] ' + message['content'] + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ message['content'] + eos_token + ' ' }}{% else %}{{ raise_exception('Only user and assistant roles are supported!') }}{% endif %}{% endfor %}\"\n\n\n\n# During training, we should also use the chat templates to rearrange the dataset\nfrom transformers import AutoTokenizer\nfrom datasets import Dataset\n\ntokenizer = AutoTokenizer.from_pretrained(\"HuggingFaceH4/zephyr-7b-beta\")\n\nchat1 = [\n    {\"role\": \"user\", \"content\": \"Which is bigger, the moon or the sun?\"},\n    {\"role\": \"assistant\", \"content\": \"The sun.\"},\n]\nchat2 = [\n    {\"role\": \"user\", \"content\": \"Which is bigger, a virus or a bacterium?\"},\n    {\"role\": \"assistant\", \"content\": \"A bacterium.\"},\n]\n\ndataset = Dataset.from_dict({\"chat\": [chat1, chat2]})\ndataset = dataset.map(\n    lambda x: {\n        \"formatted_chat\": tokenizer.apply_chat_template(\n            x[\"chat\"], tokenize=False, add_generation_prompt=False\n        )\n    }\n)\nprint(dataset[\"formatted_chat\"][0])\n\n\n\nWe can also fill the missing tokens in the middle of the sentence. We just need to tell the language model the content before and after the missing part. And the model will handle the missing part automatically.\n\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained(\n    \"stabilityai/stable-code-3b\", trust_remote_code=True\n)\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"stabilityai/stable-code-3b\",\n    trust_remote_code=True,\n    torch_dtype=\"auto\",\n    #   attn_implementation=\"flash_attention_2\",\n)\nmodel.cuda()\n\n# &lt;fim_prefix&gt; is the prefix code before the missiong part\n# &lt;fim_suffix&gt; is the suffix code after the missiong part\n# &lt;fim_middle&gt; is the missing part, and we add the token in the final part so that the model can predict it\ninputs = tokenizer(\n    \"&lt;fim_prefix&gt;def fib(n):&lt;fim_suffix&gt;    else:\\n        return fib(n - 2) + fib(n - 1)&lt;fim_middle&gt;\",\n    return_tensors=\"pt\",\n).to(model.device)\ntokens = model.generate(\n    **inputs,\n    max_new_tokens=48,\n    temperature=0.2,\n    do_sample=True,\n)\nprint(tokenizer.decode(tokens[0], skip_special_tokens=True))"
  },
  {
    "objectID": "chat_model.html#fill-in-middle-model",
    "href": "chat_model.html#fill-in-middle-model",
    "title": "Chat model demo",
    "section": "",
    "text": "We can also fill the missing tokens in the middle of the sentence. We just need to tell the language model the content before and after the missing part. And the model will handle the missing part automatically.\n\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained(\n    \"stabilityai/stable-code-3b\", trust_remote_code=True\n)\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"stabilityai/stable-code-3b\",\n    trust_remote_code=True,\n    torch_dtype=\"auto\",\n    #   attn_implementation=\"flash_attention_2\",\n)\nmodel.cuda()\n\n# &lt;fim_prefix&gt; is the prefix code before the missiong part\n# &lt;fim_suffix&gt; is the suffix code after the missiong part\n# &lt;fim_middle&gt; is the missing part, and we add the token in the final part so that the model can predict it\ninputs = tokenizer(\n    \"&lt;fim_prefix&gt;def fib(n):&lt;fim_suffix&gt;    else:\\n        return fib(n - 2) + fib(n - 1)&lt;fim_middle&gt;\",\n    return_tensors=\"pt\",\n).to(model.device)\ntokens = model.generate(\n    **inputs,\n    max_new_tokens=48,\n    temperature=0.2,\n    do_sample=True,\n)\nprint(tokenizer.decode(tokens[0], skip_special_tokens=True))"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Wei Chen",
    "section": "",
    "text": "I am inside a Hawaii cafe\n\n\n\n\n   \n\n\nGreetings! My name is Alex Chen, an AI researcher and startup founder in Silicon Valley. Previously, I pursued a PhD at Stanford University, delving into numerical simulation and artificial intelligence research.\nIn this space, I aim to share my journey in academia and industry, providing insights into cutting-edge research and practical applications of AI and machine learning.\n\nFeel free to connect with me:\n\nEmail: weichen6@stanford.edu\nCheck out my startup: AI Tools by Nexa"
  },
  {
    "objectID": "notes/Diffusion Model/sd.html",
    "href": "notes/Diffusion Model/sd.html",
    "title": "Stable diffusion model",
    "section": "",
    "text": "Tip\n\n\n\nA text to image generation model from the diffusion architecture.\n\n\nüìù Paper: https://arxiv.org/abs/2112.10752",
    "crumbs": [
      "Home",
      "üí° **Diffusion models**",
      "Stable diffusion model"
    ]
  }
]