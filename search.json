[
  {
    "objectID": "hello.html",
    "href": "hello.html",
    "title": "Quarto Basics",
    "section": "",
    "text": "For a demonstration of a line plot on a polar axis, see Figure¬†1.\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nr = np.arange(0, 2, 0.01)\ntheta = 2 * np.pi * r\nfig, ax = plt.subplots(\n  subplot_kw = {'projection': 'polar'} \n)\nax.plot(theta, r)\nax.set_rticks([0.5, 1, 1.5, 2])\nax.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\nFigure¬†1: A line plot on a polar axis\n\n\n\n\n\nOf course, we can also write equations here freely.\n\\[\\hat{m} = (w_1 A_1 + w_2 A_2 + \\dots + w_N A_N) (w_1 B_1 + w_2 B_2 + \\dots + w_N B_N).\\]"
  },
  {
    "objectID": "notes/Large Language Model/orca.html",
    "href": "notes/Large Language Model/orca.html",
    "title": "Reinforcement learning for large language model",
    "section": "",
    "text": "Tip\n\n\n\nReinforment is a common technique, which can be applied to the large language model area.",
    "crumbs": [
      "Home",
      "üó£Ô∏è **Large language models**",
      "Reinforcement learning for large language model"
    ]
  },
  {
    "objectID": "notes/Large Language Model/orca.html#background-of-reinforcement-learning",
    "href": "notes/Large Language Model/orca.html#background-of-reinforcement-learning",
    "title": "Reinforcement learning for large language model",
    "section": "Background of reinforcement learning",
    "text": "Background of reinforcement learning\nIn the first section, we will review the fundamental concept of the reinforcement learning. The fundamental part of the reinforcement learning includes the agent and environment. The process is as the following:\n\n\n\nAt each iteration step, we have the state of the environement marked as \\(S\\), the action \\(A\\) and the reward \\(R\\). Below, we list the step at the time step \\(t\\):\n\nBased on the current state \\(S_t\\), the agent make the action \\(A_t\\);\nThe environment react to the action and transit to the state \\(S_{t+1}\\) and reward \\(R_{t+1}\\).\n\nTherefore, related to each action, we will have a state of \\(S_t, A_t, S_{t+1}, R_{t+1}\\). And these four variables will be the critical data used for the reinforcement learning! Now, let me introduce more about the glossary of the reinforcement learning terms.\n\nMarkov chain: The Markov chain means that the action taken by the agent is only dependent on the most recent state/present state, and is independent of past states.\nObservation/State: The state is the complete description while the observation is just the partial description. The partial description means part of the state.\npolicy: The policy is usually denoted as \\(\\pi\\) and it is used to decide which action \\(a\\) to take. According to the Markov chain, we have \\(\\pi(s)=a\\).\nreward: Reward is the value that we can get immediately after we take a new action. For example, in cartpole example, we get every positive feedback if the cartpole doesn‚Äôt fail.\nValue: The value function to calculate the discounted sum of all future rewards! Thus, the values are different from the reward.\n\nThese are some basic concepts in the reinforcement learning! We will introduce more advanced concept along with more topics involved below. We revisit the fundamental part of the RL: The agent can repeated to take actions and get feedback (rewards/values) from the environment so that it can update the agent itself to behave better to get best reward or values. The deep learning and pytorch is not designed for the RL, and RL is more a mathematically which may not naturally suited for the deep learning. Rather, we design some equation to apply the deep learning. Thus, when we design the RL, we need to think from the fundamental math, and deep learning is just a method to solve a math problem.",
    "crumbs": [
      "Home",
      "üó£Ô∏è **Large language models**",
      "Reinforcement learning for large language model"
    ]
  },
  {
    "objectID": "notes/Large Language Model/orca.html#the-classification-of-rl",
    "href": "notes/Large Language Model/orca.html#the-classification-of-rl",
    "title": "Reinforcement learning for large language model",
    "section": "The classification of RL",
    "text": "The classification of RL\nTo solve the RL problem, we have various methods! The detailed is concluded in the figure below. We will study more about the policy based method, the value based method. And for SOTA, the LLM usuaully use a combined method. When we consider how to train the RL, we should first think about how to use the pretrained model. We wish the model to guide us to get the best action to take at every step! Thus, we need a great policy \\(\\pi^*\\)!.\n\n\n\n\nThe value based method\nThe famous \\(Q\\) learning is a typical value-based method. The original paper can be accessed here. The \\(Q\\) is the abbreviate of quality. The value based method has two submethods called the state-value function and the action-value function. Usually, we use \\(V\\) to represent the value, which is\n\\[\nV_{\\pi}(s) = \\mathbb{E}_{\\pi}\\left[ R_{t+1}+\\gamma R_{t+2} + \\gamma^2R_{t+3}+... | S_t=s \\right]\n\\]\nLet me clarify the equation above in a probability. The \\(\\pi\\) is like a distribution, and we may express the value as\n\\[\nV_{\\pi}(s) = \\mathbb{E}_{\\tau\\sim\\pi}\\left[ R_{t+1}+\\gamma R_{t+2} + \\gamma^2R_{t+3}+... | S_t=s \\right]\n\\]\nsince we have \\(a\\sim \\pi(s)\\). And \\(a\\) is directly relevant to the trajectory \\(\\tau\\) which can be used for comprehensive rewards. Now, we have known the value function, this is a value that can evaluate the current confidence to get the best reward based on the current state! Another better and granular method is not just the current state, but also the action. And we introduce the \\(Q\\) value. However, fundamentally, we have \\(Q\\) and \\(V\\) to express the same meaning, the confidence or the estimated quality of the current condition. The only difference is that the \\(Q\\) function also count in the actions.\nThe comparison would be \\(V_\\pi (s)=\\mathbb{E}_\\pi [G_t|S_t=s]\\) vs.¬†\\(Q_{\\pi}(s, a)=\\mathbb{E_\\pi}[G_t|S_t=s, A_t=a]\\). The \\(G_t\\) here represent the ending state. Then, as stated above how do we get the best policy? We can use\n\\[\n\\pi^* = \\text{arg}\\max_a Q^*(s, a)\n\\]\nTo simulate the RL, we usually need to simulate the whole episode, like a cartpole example would continue until it fails. However, there are ways to simplify the process by Bellman equation: \\[\nV_\\pi(s) = \\mathbb{E}_{\\pi} [R_{t+1}+\\gamma * V_{\\pi}(S_{t+1})|S_t=s].\n\\] And we can update the value function by Monte Carlo or the Temporary Difference method. The \\(Q\\) learning is an off-policy (when updating the value function choose a different way to sample the action) value-based method that uses a TD approach to train its action-value function.\nBefore move on, we explain the off-policy. In RL, we usually use \\(\\epsilon\\) greedy policy to choose the actions. That is for a given state \\(s\\), we take the action by sample \\(p\\in [0,1]\\): \\[\nf(x) =\n\\begin{cases}\n\\pi^*(s) & \\text{$p\\leq\\epsilon$}, \\\\\n\\text{random action} & \\text{otherwise}.\n\\end{cases}\n\\] This is a combination of exploration and eploitation. And each time, when we train the \\(Q\\) function, we update it like \\[\nQ(S_t, A_t) \\leftarrow Q(S_t, A_t) +\\alpha (R_{t+1}+\\gamma \\max_a Q(S_{t+1}, a) -Q(S_t, A_t))\n\\]\nFor certain case with finite number of state and actions, we can easily use a table to record the \\(Q\\) function. However, for some infinite number of states and actions, we need more complicated expression. For example, a math function, and abstract that function we can get the deep neural network \\(Q\\). This is how we can infer the DQN, a natural paper. This basically tell us the value of \\(Q_\\theta (s, a)\\).\nA DQN algorithm is:\n\nInitialize replay memory \\(D\\) to capacity \\(N\\)\nInitialize action-value function \\(Q\\) with random weights \\(\\theta\\)\nInitialize target action-value function \\(\\hat{Q}\\) with weights \\(\\theta^{-} = \\theta\\)\nFor episode = \\(1, M\\) do\n\nInitialize sequence \\(s_1 = \\{x_1\\}\\) and preprocessed sequence \\(\\phi_1 = \\phi(s_1)\\)\nFor \\(t = 1, T\\) do\n\nWith probability \\(\\varepsilon\\) select a random action \\(a_t\\) otherwise select \\(a_t = \\text{argmax}_a Q(\\phi(s_t), a; \\theta)\\)\nExecute action \\(a_t\\) in emulator and observe reward \\(r_t\\) and image \\(x_{t+1}\\)\nSet \\(s_{t+1} = s_t, a_t, x_{t+1}\\) and preprocess \\(\\phi_{t+1} = \\phi(s_{t+1})\\)\nStore transition \\((\\phi_t, a_t, r_t, \\phi_{t+1})\\) in \\(D\\)\nSample random minibatch of transitions \\((\\phi_j, a_j, r_j, \\phi_{j+1})\\) from \\(D\\)\nSet \\(y_j = \\left\\{\\begin{array}{ll}\nr_j & \\text{if episode terminates at step } j+1 \\\\\nr_j + \\gamma \\max_{a'} \\hat{Q}(\\phi_{j+1}, a'; \\theta^{-}) & \\text{otherwise}\n\\end{array}\\right.\\)\nPerform a gradient descent step on \\((y_j - Q(\\phi_j, a_j; \\theta))^2\\) with respect to the network parameters \\(\\theta\\)\nEvery \\(C\\) steps reset \\(\\hat{Q} = Q\\)\n\n\n\nHere \\(\\phi\\) represent some feature encoder! For example, if the state can be represented as image. Then, \\(\\phi\\) is something like the RGB value extractor.\n\n\nThe policy based method\nWe can also train the policy directly \\(\\pi_\\theta\\) and set the loss as the rewarding function! It is more intuitive but it becomes hard to converge and takes really long time to train.\nNote that \\(\\pi_\\theta (s) = \\mathbb{P}(A|s;\\theta)\\). Thus, the training basically becomes that when we have postive reward, we should increase the proability of the state and action pair. Otherwise, decrease it. The objective function is still the total rewards! \\[\nJ(\\theta) = \\mathbb{E}_{\\tau\\sim \\pi}[R(\\tau)],\n\\] where \\(\\tau\\) is a trajectory (a whole simulation process). We already have a theorem to update the policy:\n\\[\n\\nabla_\\theta J(\\theta)=\\mathbb{E}_{\\pi_\\theta}\\left[\\nabla_\\theta \\log \\pi_\\theta\\left(a_t \\mid s_t\\right) R(\\tau)\\right]\n\\]\nwhich is valid for any differentiable policy and for any policy objective function! To better understand the process, we introduce the Monte Carlo Reinforce. In a loop:\n\nUse the policy \\(\\pi_\\theta\\) to collect an episode \\(\\tau\\)\nUse the episode to estimate the gradient \\(\\hat{g}=\\nabla_\\theta J(\\theta)\\) \\[\n\\nabla_\\theta J(\\theta) \\approx \\hat{g}=\\sum_{t=0} \\nabla_\\theta \\log \\pi_\\theta\\left(a_t \\mid s_t\\right) R(\\tau)\n\\]\nUpdate the weights of the policy: \\(\\theta \\leftarrow \\theta+\\alpha \\hat{g}\\). (Gradient descent)\n\n\n\n\n\n\n\nTip\n\n\n\nNow, we usually use a mixed method containing both policy based and value based methods.\n\n\n\n\nThe actor-critic method and PPO\nTODO‚Ä¶",
    "crumbs": [
      "Home",
      "üó£Ô∏è **Large language models**",
      "Reinforcement learning for large language model"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Alex Chen",
    "section": "",
    "text": "üëã Greetings! My name is Alex Chen, an AI researcher and startup founder in Silicon Valley. Previously, I pursued a PhD at Stanford University, delving into numerical simulation and artificial intelligence research. Now, I am building an AI agent UGC platform.\nIn this space, I aim to share my journey in academia and industry, providing insights into cutting-edge research and practical applications of AI and machine learning.\nIf you want to contact me, feel free to send a mail at this address.\n\nCredits:\n\nEmojis used in figures are designed by OpenMoji, the open-source emoji and icon project. License: CC BY-SA 4.0.\nVector icons are provided by Streamline (https://streamlinehq.com). License: CC BY-SA 4.0."
  },
  {
    "objectID": "notes.html",
    "href": "notes.html",
    "title": "Research notes",
    "section": "",
    "text": "Reinforcement learning for large language model\n\n\n\n\n\n\nLarge Language Models\n\n\n\n\n\n\n\n\n\n8 min\n\n\n\n\n\n\n\n\n\n\n\n\nStable diffusion model\n\n\n\n\n\n\nDiffusion Model\n\n\n\n\n\n\n\n\n\n1 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "chat_model.html",
    "href": "chat_model.html",
    "title": "Chat model demo",
    "section": "",
    "text": "The chat models are very different from the other models. We should spend more time for the data, especially during training.\n\nfrom transformers import AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.1\")\n\nchat = [\n    {\"role\": \"user\", \"content\": \"Hello, how are you?\"},\n    {\"role\": \"assistant\", \"content\": \"I'm doing great. How can I help you today?\"},\n    {\"role\": \"user\", \"content\": \"I'd like to show off how chat templating works!\"},\n]\n\ntokenizer.apply_chat_template(chat, tokenize=False)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\"&lt;s&gt;[INST] Hello, how are you? [/INST]I'm doing great. How can I help you today?&lt;/s&gt; [INST] I'd like to show off how chat templating works! [/INST]\"\n\n\n\n# each model has different tokenizer! We can access the chat templates for each model\ntokenizer.chat_template\n\n\"{{ bos_token }}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if message['role'] == 'user' %}{{ '[INST] ' + message['content'] + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ message['content'] + eos_token + ' ' }}{% else %}{{ raise_exception('Only user and assistant roles are supported!') }}{% endif %}{% endfor %}\"\n\n\n\n# During training, we should also use the chat templates to rearrange the dataset\nfrom transformers import AutoTokenizer\nfrom datasets import Dataset\n\ntokenizer = AutoTokenizer.from_pretrained(\"HuggingFaceH4/zephyr-7b-beta\")\n\nchat1 = [\n    {\"role\": \"user\", \"content\": \"Which is bigger, the moon or the sun?\"},\n    {\"role\": \"assistant\", \"content\": \"The sun.\"},\n]\nchat2 = [\n    {\"role\": \"user\", \"content\": \"Which is bigger, a virus or a bacterium?\"},\n    {\"role\": \"assistant\", \"content\": \"A bacterium.\"},\n]\n\ndataset = Dataset.from_dict({\"chat\": [chat1, chat2]})\ndataset = dataset.map(\n    lambda x: {\n        \"formatted_chat\": tokenizer.apply_chat_template(\n            x[\"chat\"], tokenize=False, add_generation_prompt=False\n        )\n    }\n)\nprint(dataset[\"formatted_chat\"][0])\n\n\n\nWe can also fill the missing tokens in the middle of the sentence. We just need to tell the language model the content before and after the missing part. And the model will handle the missing part automatically.\n\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained(\n    \"stabilityai/stable-code-3b\", trust_remote_code=True\n)\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"stabilityai/stable-code-3b\",\n    trust_remote_code=True,\n    torch_dtype=\"auto\",\n    #   attn_implementation=\"flash_attention_2\",\n)\nmodel.cuda()\n\n# &lt;fim_prefix&gt; is the prefix code before the missiong part\n# &lt;fim_suffix&gt; is the suffix code after the missiong part\n# &lt;fim_middle&gt; is the missing part, and we add the token in the final part so that the model can predict it\ninputs = tokenizer(\n    \"&lt;fim_prefix&gt;def fib(n):&lt;fim_suffix&gt;    else:\\n        return fib(n - 2) + fib(n - 1)&lt;fim_middle&gt;\",\n    return_tensors=\"pt\",\n).to(model.device)\ntokens = model.generate(\n    **inputs,\n    max_new_tokens=48,\n    temperature=0.2,\n    do_sample=True,\n)\nprint(tokenizer.decode(tokens[0], skip_special_tokens=True))"
  },
  {
    "objectID": "chat_model.html#fill-in-middle-model",
    "href": "chat_model.html#fill-in-middle-model",
    "title": "Chat model demo",
    "section": "",
    "text": "We can also fill the missing tokens in the middle of the sentence. We just need to tell the language model the content before and after the missing part. And the model will handle the missing part automatically.\n\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained(\n    \"stabilityai/stable-code-3b\", trust_remote_code=True\n)\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"stabilityai/stable-code-3b\",\n    trust_remote_code=True,\n    torch_dtype=\"auto\",\n    #   attn_implementation=\"flash_attention_2\",\n)\nmodel.cuda()\n\n# &lt;fim_prefix&gt; is the prefix code before the missiong part\n# &lt;fim_suffix&gt; is the suffix code after the missiong part\n# &lt;fim_middle&gt; is the missing part, and we add the token in the final part so that the model can predict it\ninputs = tokenizer(\n    \"&lt;fim_prefix&gt;def fib(n):&lt;fim_suffix&gt;    else:\\n        return fib(n - 2) + fib(n - 1)&lt;fim_middle&gt;\",\n    return_tensors=\"pt\",\n).to(model.device)\ntokens = model.generate(\n    **inputs,\n    max_new_tokens=48,\n    temperature=0.2,\n    do_sample=True,\n)\nprint(tokenizer.decode(tokens[0], skip_special_tokens=True))"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Wei Chen",
    "section": "",
    "text": "I am inside a Hawaii cafe\n\n\n\n\n   \n\n\nGreetings! My name is Alex Chen, an AI researcher and startup founder in Silicon Valley. Previously, I pursued a PhD at Stanford University, delving into numerical simulation and artificial intelligence research.\nIn this space, I aim to share my journey in academia and industry, providing insights into cutting-edge research and practical applications of AI and machine learning.\n\nFeel free to connect with me:\n\nEmail: weichen6@stanford.edu\nCheck out my startup: AI Tools by Nexa"
  },
  {
    "objectID": "notes/Diffusion Model/sd.html",
    "href": "notes/Diffusion Model/sd.html",
    "title": "Stable diffusion model",
    "section": "",
    "text": "Tip\n\n\n\nA text to image generation model from the diffusion architecture.\n\n\nüìù Paper: https://arxiv.org/abs/2112.10752",
    "crumbs": [
      "Home",
      "üí° **Diffusion models**",
      "Stable diffusion model"
    ]
  }
]