<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.4.549">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Alex Chen">
<meta name="dcterms.date" content="2024-02-15">

<title>Alex Chen‚Äôs Blog - Reinforcement learning for large language model</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<link href="../../images/favicon.ico" rel="icon">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../../styles.css">
</head>

<body class="nav-sidebar floating nav-fixed fullcontent">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top quarto-banner">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">Alex Chen‚Äôs Blog</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../index.html"> 
<span class="menu-text">Home</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../notes.html"> 
<span class="menu-text">Notes</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../chat_model.html"> 
<span class="menu-text">LLM code example</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../hello.html"> 
<span class="menu-text">Hello</span></a>
  </li>  
</ul>
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/alexchen4ai"> <i class="bi bi-github" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://www.linkedin.com/in/wei-chen-stanford"> <i class="bi bi-linkedin" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
          <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <a class="flex-grow-1 no-decor" role="button" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
          <h1 class="quarto-secondary-nav-title">Reinforcement learning for large language model</h1>
        </a>     
    </div>
  </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title d-none d-lg-block">Reinforcement learning for large language model</h1>
                                <div class="quarto-categories">
                <div class="quarto-category">Large Language Models</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>Alex Chen </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">February 15, 2024</p>
      </div>
    </div>
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true">
 <span class="menu-text">üó£Ô∏è <strong>Large language models</strong></span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../notes/Large Language Model/rl_llm.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text">Reinforcement learning for large language model</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="px-0"><hr class="sidebar-divider hi "></li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="true">
 <span class="menu-text">üí° <strong>Diffusion models</strong></span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../notes/Diffusion Model/sd.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Scalable diffusion models with transformers</span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">





<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Tip
</div>
</div>
<div class="callout-body-container callout-body">
<p>Reinforment is a common technique, which can be applied to the large language model area.</p>
</div>
</div>
<section id="background-of-reinforcement-learning" class="level2">
<h2 class="anchored" data-anchor-id="background-of-reinforcement-learning">Background of reinforcement learning</h2>
In the first section, we will review the fundamental concept of the reinforcement learning. The fundamental part of the reinforcement learning includes the <strong>agent</strong> and <strong>environment</strong>. The process is as the following:
<p align="center">
<img src="../../images/RL_basic.png" width="60%">
</p>
<p>At each iteration step, we have the state of the environement marked as <span class="math inline">\(S\)</span>, the action <span class="math inline">\(A\)</span> and the reward <span class="math inline">\(R\)</span>. Below, we list the step at the time step <span class="math inline">\(t\)</span>:</p>
<ol type="1">
<li>Based on the current state <span class="math inline">\(S_t\)</span>, the agent make the action <span class="math inline">\(A_t\)</span>;</li>
<li>The environment react to the action and transit to the state <span class="math inline">\(S_{t+1}\)</span> and reward <span class="math inline">\(R_{t+1}\)</span>.</li>
</ol>
<p>Therefore, related to each action, we will have a state of <span class="math inline">\(S_t, A_t, S_{t+1}, R_{t+1}\)</span>. And these four variables will be the critical data used for the reinforcement learning! Now, let me introduce more about the <strong>glossary</strong> of the reinforcement learning terms.</p>
<ul>
<li><em>Markov chain</em>: The Markov chain means that the action taken by the agent is only dependent on the most recent state/present state, and is independent of past states.</li>
<li><em>Observation/State</em>: The state is the complete description while the observation is just the partial description. The partial description means part of the state.</li>
<li><em>policy</em>: The policy is usually denoted as <span class="math inline">\(\pi\)</span> and it is used to decide which action <span class="math inline">\(a\)</span> to take. According to the Markov chain, we have <span class="math inline">\(\pi(s)=a\)</span>.</li>
<li><em>reward</em>: Reward is the value that we can get immediately after we take a new action. For example, in cartpole example, we get every positive feedback if the cartpole doesn‚Äôt fail.</li>
<li><em>Value</em>: The value function to calculate the discounted sum of all future rewards! Thus, the values are different from the reward.</li>
</ul>
<p>These are some basic concepts in the reinforcement learning! We will introduce more advanced concept along with more topics involved below. We revisit the fundamental part of the RL: The agent can repeated to take actions and get feedback (rewards/values) from the environment so that it can update the agent itself to behave better to get best reward or values. The deep learning and pytorch is not designed for the RL, and RL is more a mathematically which may not naturally suited for the deep learning. Rather, we design some equation to apply the deep learning. Thus, when we design the RL, we need to think from the fundamental math, and deep learning is just a method to solve a math problem.</p>
</section>
<section id="the-classification-of-rl" class="level2">
<h2 class="anchored" data-anchor-id="the-classification-of-rl">The classification of RL</h2>
<p>To solve the RL problem, we have various methods! The detailed is concluded in the figure below. We will study more about the policy based method, the value based method. And for SOTA, the LLM usuaully use a combined method. When we consider how to train the RL, we should first think about how to use the pretrained model. We wish the model to guide us to get the best action to take at every step! Thus, we need a great policy <span class="math inline">\(\pi^*\)</span>!.</p>
<p align="center">
<img src="../../images/RL_classification.png" width="100%">
</p>
<section id="the-value-based-method" class="level3">
<h3 class="anchored" data-anchor-id="the-value-based-method">The value based method</h3>
<p>The famous <span class="math inline">\(Q\)</span> learning is a typical value-based method. The original paper can be accessed <a href="https://link.springer.com/content/pdf/10.1007/BF00992698.pdf"><strong>here</strong></a>. The <span class="math inline">\(Q\)</span> is the abbreviate of <em>quality</em>. The value based method has two submethods called the state-value function and the action-value function. Usually, we use <span class="math inline">\(V\)</span> to represent the value, which is</p>
<p><span class="math display">\[
V_{\pi}(s) = \mathbb{E}_{\pi}\left[ R_{t+1}+\gamma R_{t+2} + \gamma^2R_{t+3}+... | S_t=s \right]
\]</span></p>
<p>Let me clarify the equation above in a probability. The <span class="math inline">\(\pi\)</span> is like a distribution, and we may express the value as</p>
<p><span class="math display">\[
V_{\pi}(s) = \mathbb{E}_{\tau\sim\pi}\left[ R_{t+1}+\gamma R_{t+2} + \gamma^2R_{t+3}+... | S_t=s \right]
\]</span></p>
<p>since we have <span class="math inline">\(a\sim \pi(s)\)</span>. And <span class="math inline">\(a\)</span> is directly relevant to the trajectory <span class="math inline">\(\tau\)</span> which can be used for comprehensive rewards. Now, we have known the value function, this is a value that can evaluate the current confidence to get the best reward based on the current state! Another better and granular method is not just the current state, but also the action. And we introduce the <span class="math inline">\(Q\)</span> value. However, fundamentally, we have <span class="math inline">\(Q\)</span> and <span class="math inline">\(V\)</span> to express the same meaning, the confidence or the estimated quality of the current condition. The only difference is that the <span class="math inline">\(Q\)</span> function also count in the actions.</p>
<p>The comparison would be <span class="math inline">\(V_\pi (s)=\mathbb{E}_\pi [G_t|S_t=s]\)</span> vs.&nbsp;<span class="math inline">\(Q_{\pi}(s, a)=\mathbb{E_\pi}[G_t|S_t=s, A_t=a]\)</span>. The <span class="math inline">\(G_t\)</span> here represent the ending state. Then, as stated above how do we get the best policy? We can use</p>
<p><span class="math display">\[
\pi^* = \text{arg}\max_a Q^*(s, a)
\]</span></p>
<p>To simulate the RL, we usually need to simulate the whole episode, like a cartpole example would continue until it fails. However, there are ways to simplify the process by <strong>Bellman equation</strong>: <span class="math display">\[
V_\pi(s) = \mathbb{E}_{\pi} [R_{t+1}+\gamma * V_{\pi}(S_{t+1})|S_t=s].
\]</span> And we can update the value function by <strong>Monte Carlo</strong> or the <strong>Temporary Difference</strong> method. The <span class="math inline">\(Q\)</span> learning is an off-policy (when updating the value function choose a different way to sample the action) value-based method that uses a TD approach to train its action-value function.</p>
<p>Before move on, we explain the off-policy. In RL, we usually use <span class="math inline">\(\epsilon\)</span> greedy policy to choose the actions. That is for a given state <span class="math inline">\(s\)</span>, we take the action by sample <span class="math inline">\(p\in [0,1]\)</span>: <span class="math display">\[
f(x) =
\begin{cases}
\pi^*(s) &amp; \text{$p\leq\epsilon$}, \\
\text{random action} &amp; \text{otherwise}.
\end{cases}
\]</span> This is a combination of exploration and eploitation. And each time, when we train the <span class="math inline">\(Q\)</span> function, we update it like <span id="eq-q-update"><span class="math display">\[
Q(S_t, A_t) \leftarrow Q(S_t, A_t) +\alpha (R_{t+1}+\gamma \max_a Q(S_{t+1}, a) -Q(S_t, A_t))
\tag{1}\]</span></span></p>
<p>For certain case with finite number of state and actions, we can easily use a table to record the <span class="math inline">\(Q\)</span> function. However, for some infinite number of states and actions, we need more complicated expression. For example, a math function, and abstract that function we can get the deep neural network <span class="math inline">\(Q\)</span>. This is how we can infer the <a href="https://www.nature.com/articles/nature14236">DQN, a nature paper</a>. This basically tell us the value of <span class="math inline">\(Q_\theta (s, a)\)</span>.</p>
<p>A DQN algorithm is:</p>
<ol type="1">
<li>Initialize replay memory <span class="math inline">\(D\)</span> to capacity <span class="math inline">\(N\)</span></li>
<li>Initialize action-value function <span class="math inline">\(Q\)</span> with random weights <span class="math inline">\(\theta\)</span></li>
<li>Initialize target action-value function <span class="math inline">\(\hat{Q}\)</span> with weights <span class="math inline">\(\theta^{-} = \theta\)</span></li>
<li>For episode = <span class="math inline">\(1, M\)</span> do
<ul>
<li>Initialize sequence <span class="math inline">\(s_1 = \{x_1\}\)</span> and preprocessed sequence <span class="math inline">\(\phi_1 = \phi(s_1)\)</span></li>
<li>For <span class="math inline">\(t = 1, T\)</span> do
<ul>
<li>With probability <span class="math inline">\(\varepsilon\)</span> select a random action <span class="math inline">\(a_t\)</span> otherwise select <span class="math inline">\(a_t = \text{argmax}_a Q(\phi(s_t), a; \theta)\)</span></li>
<li>Execute action <span class="math inline">\(a_t\)</span> in emulator and observe reward <span class="math inline">\(r_t\)</span> and image <span class="math inline">\(x_{t+1}\)</span></li>
<li>Set <span class="math inline">\(s_{t+1} = s_t, a_t, x_{t+1}\)</span> and preprocess <span class="math inline">\(\phi_{t+1} = \phi(s_{t+1})\)</span></li>
<li>Store transition <span class="math inline">\((\phi_t, a_t, r_t, \phi_{t+1})\)</span> in <span class="math inline">\(D\)</span></li>
<li>Sample random minibatch of transitions <span class="math inline">\((\phi_j, a_j, r_j, \phi_{j+1})\)</span> from <span class="math inline">\(D\)</span></li>
<li>Set <span class="math inline">\(y_j = \left\{\begin{array}{ll}
r_j &amp; \text{if episode terminates at step } j+1 \\
r_j + \gamma \max_{a'} \hat{Q}(\phi_{j+1}, a'; \theta^{-}) &amp; \text{otherwise}
\end{array}\right.\)</span></li>
<li>Perform a gradient descent step on <span class="math inline">\((y_j - Q(\phi_j, a_j; \theta))^2\)</span> with respect to the network parameters <span class="math inline">\(\theta\)</span></li>
<li>Every <span class="math inline">\(C\)</span> steps reset <span class="math inline">\(\hat{Q} = Q\)</span></li>
</ul></li>
</ul></li>
</ol>
<p>Here <span class="math inline">\(\phi\)</span> represent some feature encoder! For example, if the state can be represented as image. Then, <span class="math inline">\(\phi\)</span> is something like the RGB value extractor. From the DQN algorithm above, we notice that the gradient descent is applied on the loss term of <span id="eq-mse"><span class="math display">\[
(y_j - Q(\phi_j, a_j; \theta))^2
\tag{2}\]</span></span></p>
<p>This is to make the learned <span class="math inline">\(Q\)</span> function to approximate the value of the predicted <span class="math inline">\(Q\)</span> value. If we revisit the <a href="#eq-q-update" class="quarto-xref">Equation&nbsp;1</a>, we notice that the original Q value update is to directly update the <span class="math inline">\(Q(S_t, A_t)\)</span>, and the goal is to reduce the difference between <span class="math inline">\(R_{t+1}+\gamma \max_a Q(S_{t+1}, a)\)</span> and the <span class="math inline">\(Q(S_t, A_t)\)</span>. In the context of the DQN, we can direcly construct the <a href="#eq-mse" class="quarto-xref">Equation&nbsp;2</a> for it! One notation here is that in the <a href="#eq-q-update" class="quarto-xref">Equation&nbsp;1</a>, we set the <span class="math inline">\(R_{t+1}\)</span>, with the same subscript as <span class="math inline">\(S_{t+1}\)</span>, but in the algorithm described above, we have it expressed as <span class="math inline">\(r_{j}\)</span> with the <code>-1</code> subscript compared to <span class="math inline">\(\phi_{j+1}\)</span>. However, the two terms are the same, we use <span class="math inline">\(t\)</span> since it represents the time step. For the use of <span class="math inline">\(j\)</span>, it is one step of generated rewards! It is just different notation.</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Tip
</div>
</div>
<div class="callout-body-container callout-body">
<p>Why do we use <a href="#eq-mse" class="quarto-xref">Equation&nbsp;2</a>? Q is the quality value, and it is used to estimate the total expected rewards based on the current state and the action. Suppose we already have the best <span class="math inline">\(Q\)</span>, then <span class="math inline">\(Q(\phi_j, a_j)\)</span> should be equal to the reward after we take the action <span class="math inline">\(a_j\)</span>, and then based on the state <span class="math inline">\(\phi_{j+1}\)</span>, the best rewards we can expect, and we use a greedy algorithm here.</p>
</div>
</div>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Tip
</div>
</div>
<div class="callout-body-container callout-body">
<p>It is always good to visit the code implementation to make sure you understand the detail.</p>
</div>
</div>
<p>We can study a <a href="https://huggingface.co/learn/deep-rl-course/en/unit3/deep-q-algorithm">DQN example from the atari simulation</a>, and the full github code can be accessed there. Now, we will combine the algorithm and the code to introduce more about the DQN.</p>
<p>Firstly, we need to have data generation process, and we can use</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a>next_obs, rewards, terminations, truncations, infos <span class="op">=</span> envs.step(actions)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>to get the variables <span class="math inline">\(R, S, A\)</span> and so on. And we save the generated data to the replay</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a>rb.add(obs, real_next_obs, actions, rewards, terminations, infos)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>And the update process is like</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a>data <span class="op">=</span> rb.sample(args.batch_size)</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> torch.no_grad():</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>   target_max, _ <span class="op">=</span> target_network(data.next_observations).<span class="bu">max</span>(dim<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>   <span class="co"># data.dones is 0 or 1.</span></span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>   td_target <span class="op">=</span> data.rewards.flatten() <span class="op">+</span> args.gamma <span class="op">*</span> target_max <span class="op">*</span> (<span class="dv">1</span> <span class="op">-</span> data.dones.flatten())</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>old_val <span class="op">=</span> q_network(data.observations).gather(<span class="dv">1</span>, data.actions).squeeze()</span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>loss <span class="op">=</span> F.mse_loss(td_target, old_val)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="the-policy-based-method" class="level3">
<h3 class="anchored" data-anchor-id="the-policy-based-method">The policy based method</h3>
<p>We can also train the policy directly <span class="math inline">\(\pi_\theta\)</span>. It is more intuitive. Compared to the value-based method, it has pros and cons.</p>
<p>For pros: (a) Can explore stochastic polify, no need for the exploration and exploitation effort; (b) More effective in high-dimensional action space, especially the continuous actions spaces; (c) Better convergence properties, the curve is smoother.</p>
<p>For the cons: (a) Often get suboptimal result; (b) Take longer time to train; (c) Policy gradient have high variance (<strong>The policy gradient in different step has really different result</strong>).</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Tip
</div>
</div>
<div class="callout-body-container callout-body">
<p>Note that Q learning method needs the <code>argmax</code> to get the best action. And if the action is a continuous space, we need to do some pretty complicated optimization to get the result!</p>
</div>
</div>
<p>Note that <span class="math inline">\(\pi_\theta (s) = \mathbb{P}(A|s;\theta)\)</span>. Thus, the training basically becomes that when we have postive reward, we should increase the proability of the state and action pair. Otherwise, decrease it. The objective function is still the total rewards! <span class="math display">\[
J(\theta) = \mathbb{E}_{\tau\sim \pi}[R(\tau)],
\]</span> where <span class="math inline">\(\tau\)</span> is a trajectory (a whole simulation process). We already have a theorem to update the policy:</p>
<p><span class="math display">\[
\nabla_\theta J(\theta)=\mathbb{E}_{\pi_\theta}\left[\nabla_\theta \log \pi_\theta\left(a_t \mid s_t\right) R(\tau)\right]
\]</span></p>
<p>which is valid for any differentiable policy and for any policy objective function! To better understand the process, we introduce the <strong>Monte Carlo Reinforce</strong>. In a loop:</p>
<ul>
<li>Use the policy <span class="math inline">\(\pi_\theta\)</span> to collect an episode <span class="math inline">\(\tau\)</span></li>
<li>Use the episode to estimate the gradient <span class="math inline">\(\hat{g}=\nabla_\theta J(\theta)\)</span> <span class="math display">\[
\nabla_\theta J(\theta) \approx \hat{g}=\sum_{t=0} \nabla_\theta \log \pi_\theta\left(a_t \mid s_t\right) R(\tau)
\]</span></li>
<li>Update the weights of the policy: <span class="math inline">\(\theta \leftarrow \theta+\alpha \hat{g}\)</span>. (Gradient ascent)</li>
</ul>
<p>Alternatively, we can collect multiple trajectories (helpful to mitigate the variance), and the gradient becomes <span class="math display">\[
\nabla_\theta J(\theta) \approx \hat{g}=\frac{1}{m}\sum_{i=1}^m\sum_{t=0} \nabla_\theta \log \pi_\theta\left(a_t^{(i)} \mid s_t^{(i)}\right) R(\tau^{(i)}).
\]</span></p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Tip
</div>
</div>
<div class="callout-body-container callout-body">
<p>We can treat the <span class="math inline">\(\nabla_\theta \log\pi_\theta(a_t\mid s_t)\)</span> is the direction of the steeppest increase of the log probability of selected action based on the <span class="math inline">\(s_t\)</span>. This is because that we wish to maximize the objective here (rewards).</p>
</div>
</div>
<p>For the derivation of the policy gradient theorem, check the following:</p>
<div id="thm-policy-gradient-theorem" class="theorem">
<p><span class="theorem-title"><strong>Theorem 1 (policy-gradient-theorem)</strong></span> The derivation of the policy gradient theorem is as the following:</p>
<p><span id="eq-policy"><span class="math display">\[
\begin{aligned}
\nabla_\theta J(\theta)  &amp;= \mathbb{E}_{\tau\sim \pi}[R(\tau)] \\
   &amp;= \nabla_\theta \sum_{\tau}P(\tau;\theta)R(\tau) \\
   &amp;= \sum_{\tau} \nabla_\theta P(\tau;\theta)R(\tau) \\
   &amp;= \sum_{\tau} P(\tau;\theta) \frac{\nabla_\theta P(\tau;\theta)}{P(\tau;\theta)}R(\tau) \\
   &amp;= \sum_{\tau} P(\tau;\theta) \nabla_\theta \log P(\tau;\theta)R(\tau) \\
   &amp;= \sum_{\tau} P(\tau;\theta) \nabla_\theta\log [\phi(s_0)\prod_{t=0}^T P(s_{t+1}|s_t, a_t)\pi_\theta (a_t\mid s_t)] R(\tau)\\
   &amp;= \sum_{\tau} P(\tau;\theta) \nabla_\theta\left[\log\phi(s_0) + \log\sum_{t=0}^T P(s_{t+1}|s_t, a_t) +\log\sum_{t=0}^T\pi_\theta (a_t\mid s_t)\right] R(\tau)\\
   &amp;= \sum_{\tau} P(\tau;\theta) \nabla_\theta\left[\log\sum_{t=0}^T\pi_\theta (a_t\mid s_t)\right] R(\tau).
\end{aligned}
\tag{3}\]</span></span></p>
</div>
<p>For the code part, using the cartpole as an example, the policy framework would be</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Policy(nn.Module):</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, s_size, a_size, h_size):</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>(Policy, <span class="va">self</span>).<span class="fu">__init__</span>()</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.fc1 <span class="op">=</span> nn.Linear(s_size, h_size)</span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.fc2 <span class="op">=</span> nn.Linear(h_size, a_size)</span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> F.relu(<span class="va">self</span>.fc1(x))</span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.fc2(x)</span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> F.softmax(x, dim<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> act(<span class="va">self</span>, state):</span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a>        state <span class="op">=</span> torch.from_numpy(state).<span class="bu">float</span>().unsqueeze(<span class="dv">0</span>).to(device)</span>
<span id="cb4-14"><a href="#cb4-14" aria-hidden="true" tabindex="-1"></a>        probs <span class="op">=</span> <span class="va">self</span>.forward(state).cpu()</span>
<span id="cb4-15"><a href="#cb4-15" aria-hidden="true" tabindex="-1"></a>        m <span class="op">=</span> Categorical(probs)</span>
<span id="cb4-16"><a href="#cb4-16" aria-hidden="true" tabindex="-1"></a>        action <span class="op">=</span> np.argmax(m)</span>
<span id="cb4-17"><a href="#cb4-17" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> action.item(), m.log_prob(action)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Note that we need to use the <code>Categorical</code> from <code>torch.distributions</code> to enable the backpropagation. The reinforce process can be constructed according to the reinfoce algorithm introduced above.</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> reinforce(policy, optimizer, n_training_episodes, max_t, gamma, print_every):</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>    scores_deque <span class="op">=</span> deque(maxlen<span class="op">=</span><span class="dv">100</span>)</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>    scores <span class="op">=</span> []</span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i_episode <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1</span>, n_training_episodes <span class="op">+</span> <span class="dv">1</span>):</span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a>        saved_log_probs <span class="op">=</span> []</span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a>        rewards <span class="op">=</span> []</span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a>        state <span class="op">=</span> env.reset()</span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> t <span class="kw">in</span> <span class="bu">range</span>(max_t):</span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a>            action, log_prob <span class="op">=</span> policy.act(state)</span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a>            saved_log_probs.append(log_prob)</span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a>            state, reward, done, _ <span class="op">=</span> env.step(action)</span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a>            rewards.append(reward)</span>
<span id="cb5-13"><a href="#cb5-13" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> done:</span>
<span id="cb5-14"><a href="#cb5-14" aria-hidden="true" tabindex="-1"></a>                <span class="cf">break</span></span>
<span id="cb5-15"><a href="#cb5-15" aria-hidden="true" tabindex="-1"></a>        scores_deque.append(<span class="bu">sum</span>(rewards))</span>
<span id="cb5-16"><a href="#cb5-16" aria-hidden="true" tabindex="-1"></a>        scores.append(<span class="bu">sum</span>(rewards))</span>
<span id="cb5-17"><a href="#cb5-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-18"><a href="#cb5-18" aria-hidden="true" tabindex="-1"></a>        returns <span class="op">=</span> deque(maxlen<span class="op">=</span>max_t)</span>
<span id="cb5-19"><a href="#cb5-19" aria-hidden="true" tabindex="-1"></a>        n_steps <span class="op">=</span> <span class="bu">len</span>(rewards)</span>
<span id="cb5-20"><a href="#cb5-20" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb5-21"><a href="#cb5-21" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> t <span class="kw">in</span> <span class="bu">range</span>(n_steps)[::<span class="op">-</span><span class="dv">1</span>]:</span>
<span id="cb5-22"><a href="#cb5-22" aria-hidden="true" tabindex="-1"></a>            disc_return_t <span class="op">=</span> returns[<span class="dv">0</span>] <span class="cf">if</span> <span class="bu">len</span>(returns) <span class="op">&gt;</span> <span class="dv">0</span> <span class="cf">else</span> <span class="dv">0</span></span>
<span id="cb5-23"><a href="#cb5-23" aria-hidden="true" tabindex="-1"></a>            returns.appendleft(gamma <span class="op">*</span> disc_return_t <span class="op">+</span> rewards[t])</span>
<span id="cb5-24"><a href="#cb5-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-25"><a href="#cb5-25" aria-hidden="true" tabindex="-1"></a>        <span class="co">## standardization of the returns is employed to make training more stable</span></span>
<span id="cb5-26"><a href="#cb5-26" aria-hidden="true" tabindex="-1"></a>        eps <span class="op">=</span> np.finfo(np.float32).eps.item()</span>
<span id="cb5-27"><a href="#cb5-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-28"><a href="#cb5-28" aria-hidden="true" tabindex="-1"></a>        <span class="co">## eps is the smallest representable float, which is</span></span>
<span id="cb5-29"><a href="#cb5-29" aria-hidden="true" tabindex="-1"></a>        <span class="co"># added to the standard deviation of the returns to avoid numerical instabilities</span></span>
<span id="cb5-30"><a href="#cb5-30" aria-hidden="true" tabindex="-1"></a>        returns <span class="op">=</span> torch.tensor(returns)</span>
<span id="cb5-31"><a href="#cb5-31" aria-hidden="true" tabindex="-1"></a>        returns <span class="op">=</span> (returns <span class="op">-</span> returns.mean()) <span class="op">/</span> (returns.std() <span class="op">+</span> eps)</span>
<span id="cb5-32"><a href="#cb5-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-33"><a href="#cb5-33" aria-hidden="true" tabindex="-1"></a>        policy_loss <span class="op">=</span> []</span>
<span id="cb5-34"><a href="#cb5-34" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> log_prob, disc_return <span class="kw">in</span> <span class="bu">zip</span>(saved_log_probs, returns):</span>
<span id="cb5-35"><a href="#cb5-35" aria-hidden="true" tabindex="-1"></a>            policy_loss.append(<span class="op">-</span>log_prob <span class="op">*</span> disc_return)</span>
<span id="cb5-36"><a href="#cb5-36" aria-hidden="true" tabindex="-1"></a>        policy_loss <span class="op">=</span> torch.cat(policy_loss).<span class="bu">sum</span>()</span>
<span id="cb5-37"><a href="#cb5-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-38"><a href="#cb5-38" aria-hidden="true" tabindex="-1"></a>        optimizer.zero_grad()</span>
<span id="cb5-39"><a href="#cb5-39" aria-hidden="true" tabindex="-1"></a>        policy_loss.backward()</span>
<span id="cb5-40"><a href="#cb5-40" aria-hidden="true" tabindex="-1"></a>        optimizer.step()</span>
<span id="cb5-41"><a href="#cb5-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-42"><a href="#cb5-42" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> i_episode <span class="op">%</span> print_every <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb5-43"><a href="#cb5-43" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="st">"Episode </span><span class="sc">{}</span><span class="ch">\t</span><span class="st">Average Score: </span><span class="sc">{:.2f}</span><span class="st">"</span>.<span class="bu">format</span>(i_episode, np.mean(scores_deque)))</span>
<span id="cb5-44"><a href="#cb5-44" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> scores</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="the-actor-critic-method-and-ppo" class="level3">
<h3 class="anchored" data-anchor-id="the-actor-critic-method-and-ppo">The actor-critic method and PPO</h3>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Tip
</div>
</div>
<div class="callout-body-container callout-body">
<p>SOTA, we usually use a mixed method containing both policy based and value based methods.</p>
</div>
</div>
<p>The motivation of the actor-critic method is to lower the variation of the policy method. We can use a large number of the trajectories but it is not efficient. Therefore, we choose a new method called actor-critic method. That is to say, instead of giving rewards/feedback to the policy (actor) after many trajectories, we can use critic to give instant feedback to evaluate the actions taken by the policy. Now, we have two network to train:</p>
<ul>
<li>A policy function with parameters <span class="math inline">\(\pi_\theta(s)\)</span>;</li>
<li>A value function with parameters <span class="math inline">\({q}_w(s, a)\)</span></li>
</ul>
<p>This is a combined methods of the policy-based and value-based methods. For one step of time <span class="math inline">\(t\)</span></p>
<ol type="1">
<li>At time step <span class="math inline">\(t\)</span>, we have the state <span class="math inline">\(s_t\)</span>;</li>
<li>We have the policy <span class="math inline">\(\pi_\theta(s_t) = a_t\)</span>;</li>
<li>Now, we can compute the Q-value by the value function directly as <span class="math inline">\(Q_t={q}_w(s, a)\)</span>;</li>
<li>Execute the action <span class="math inline">\(a_t\)</span> and get the new state <span class="math inline">\(s_{t+1}\)</span> and new reward <span class="math inline">\(r_{t+1}\)</span>.</li>
<li>Update the policy parameters using the Q value;</li>
<li>Using the updated parameters to get the next action <span class="math inline">\(a_{t+1}\)</span>, and use the new action to update critic parameters.</li>
</ol>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Tip
</div>
</div>
<div class="callout-body-container callout-body">
<p>In policy based function, <a href="#eq-policy" class="quarto-xref">Equation&nbsp;3</a> needs to use <span class="math inline">\(R(\tau)\)</span>, and <span class="math inline">\(R(\tau)\)</span> is obtained by iterative experiments. Now, we can use Q value since they represent the same meaning. Also, when we update the Q parameters, we use <code>argmax</code> to get the best action, now we use the updated policy to calculate the best action. This actor-critic is somewhat like the iterative-optimization methods seen in many math problems.</p>
</div>
</div>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Tip
</div>
</div>
<div class="callout-body-container callout-body">
<p>To stabilize the training, now we tend to use the advantage function to replace the Q value.</p>
</div>
</div>
<p>PPO is an algorithm based on the actor-critic method, and it is to clip the ratio which indicates the difference of policy to [<span class="math inline">\(1-\epsilon\)</span>, <span class="math inline">\(1+\epsilon\)</span>].</p>
<p>To do so, we just need to the change the policy objection function (with advantage function) from <span class="math display">\[
J(\theta) = \mathbb{E}_t\left[ \log\pi_\theta (a_t\mid s_t)*A_t \right]
\]</span></p>
<p>to <span class="math display">\[
J(\theta)=\hat{\mathbb{E}}_t\left[\min \left(r_t(\theta) \hat{A}_t, \operatorname{clip}\left(r_t(\theta), 1-\epsilon, 1+\epsilon\right) \hat{A}_t\right)\right]
\]</span></p>
<p>where the ratio is <span class="math display">\[
r_t(\theta)=\frac{\pi_\theta\left(a_t \mid s_t\right)}{\pi_{\theta_{\text {old }}}\left(a_t \mid s_t\right)}.
\]</span></p>
<p>Now, we use a PPO implementation to better study the algorithm above. The <a href="https://github.com/vwxyzjn/cleanrl/blob/master/cleanrl/ppo.py">full code implementation can be found here</a>. There is also <a href="https://iclr-blog-track.github.io/2022/03/25/ppo-implementation-details/">another wonderful post about PPO implementation</a>. Let‚Äôs study the code now.</p>
<section id="define-both-the-actor-and-critic" class="level4">
<h4 class="anchored" data-anchor-id="define-both-the-actor-and-critic">Define both the actor and critic</h4>
<p>We usually define the network directly!</p>
<div class="sourceCode" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Agent(nn.Module):</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, envs):</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.critic <span class="op">=</span> nn.Sequential(</span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a>            layer_init(nn.Linear(np.array(envs.single_observation_space.shape).prod(), <span class="dv">64</span>)),</span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a>            nn.Tanh(),</span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a>            layer_init(nn.Linear(<span class="dv">64</span>, <span class="dv">64</span>)),</span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a>            nn.Tanh(),</span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a>            layer_init(nn.Linear(<span class="dv">64</span>, <span class="dv">1</span>), std<span class="op">=</span><span class="fl">1.0</span>),</span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb6-11"><a href="#cb6-11" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.actor <span class="op">=</span> nn.Sequential(</span>
<span id="cb6-12"><a href="#cb6-12" aria-hidden="true" tabindex="-1"></a>            layer_init(nn.Linear(np.array(envs.single_observation_space.shape).prod(), <span class="dv">64</span>)),</span>
<span id="cb6-13"><a href="#cb6-13" aria-hidden="true" tabindex="-1"></a>            nn.Tanh(),</span>
<span id="cb6-14"><a href="#cb6-14" aria-hidden="true" tabindex="-1"></a>            layer_init(nn.Linear(<span class="dv">64</span>, <span class="dv">64</span>)),</span>
<span id="cb6-15"><a href="#cb6-15" aria-hidden="true" tabindex="-1"></a>            nn.Tanh(),</span>
<span id="cb6-16"><a href="#cb6-16" aria-hidden="true" tabindex="-1"></a>            layer_init(nn.Linear(<span class="dv">64</span>, envs.single_action_space.n), std<span class="op">=</span><span class="fl">0.01</span>),</span>
<span id="cb6-17"><a href="#cb6-17" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb6-18"><a href="#cb6-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-19"><a href="#cb6-19" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> get_value(<span class="va">self</span>, x):</span>
<span id="cb6-20"><a href="#cb6-20" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.critic(x)</span>
<span id="cb6-21"><a href="#cb6-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-22"><a href="#cb6-22" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> get_action_and_value(<span class="va">self</span>, x, action<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb6-23"><a href="#cb6-23" aria-hidden="true" tabindex="-1"></a>        logits <span class="op">=</span> <span class="va">self</span>.actor(x)</span>
<span id="cb6-24"><a href="#cb6-24" aria-hidden="true" tabindex="-1"></a>        probs <span class="op">=</span> Categorical(logits<span class="op">=</span>logits)</span>
<span id="cb6-25"><a href="#cb6-25" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> action <span class="kw">is</span> <span class="va">None</span>:</span>
<span id="cb6-26"><a href="#cb6-26" aria-hidden="true" tabindex="-1"></a>            action <span class="op">=</span> probs.sample()</span>
<span id="cb6-27"><a href="#cb6-27" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> action, probs.log_prob(action), probs.entropy(), <span class="va">self</span>.critic(x)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Note that we don‚Äôt have q value here since the PPO uses the advantage value which means we don‚Äôt need the Q value anymore! And you may observe that critic output is a single dim value.</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Tip
</div>
</div>
<div class="callout-body-container callout-body">
<p>In pytorch, the forward process is not necessarily defined in <code>forward()</code> function. We often use it since it has customization so that <code>model(**params)</code> is equal to <code>model.forward(**params)</code>.</p>
</div>
</div>
</section>
<section id="deal-with-the-advantage-values" class="level4">
<h4 class="anchored" data-anchor-id="deal-with-the-advantage-values">Deal with the advantage values</h4>
<p>The action value is simply as <span class="math display">\[
A(s_t, a_t) = Q(s_t, a_t) - V(s_t) = r + \gamma V(s_{t+1}) - V(s)
\]</span> Here, we use <span class="math inline">\(r + \gamma V(s_{t+1})\)</span> to appriximate the <span class="math inline">\(Q\)</span> value, but recall in the DQN algorithm, we use it as well!</p>
</section>
</section>
<section id="apply-ppo-to-llm" class="level3">
<h3 class="anchored" data-anchor-id="apply-ppo-to-llm">Apply PPO to LLM</h3>
<p>Now, we discuss the pivotal topic of this blog. How to consider the LLM training as a PPO.</p>
<p>We use the concept of RL, and explain how LLM can be used here.</p>
<ul>
<li><strong>environment</strong>: The language world, when you output a new word, it will be added as the context of the conversation. The observation/state is the existing generation and the initial language;</li>
<li><strong>state</strong>: The existing generation and the initial language;</li>
<li><strong>agent</strong>: The LLM model it self. We have <code>LLM(curr_words) = next_token</code>. Here <span class="math inline">\(\pi_\theta\)</span> = LLM;</li>
<li><strong>reward</strong>: Can be customized, and we usually choose to add a linear layer (two-heads output) to the last embedding layer of the LLM as the reward function.</li>
</ul>
<p>The step of the PPO can be formulated as the following:</p>
<ol type="1">
<li>Given the preference pair (<span class="math inline">\(y_{Y}\)</span>, <span class="math inline">\(y_{N}\)</span>), we train a reward model. The reward model can be trained using the following loss: <span class="math display">\[
\mathcal{L}_R\left(r_\phi, \mathcal{D}\right)=-\mathbb{E}_{\left(x, y_Y, y_N\right) \sim \mathcal{D}}\left[\log \sigma\left(r_\phi\left(x, y_Y\right)-r_\phi\left(x, y_N\right)\right)\right]
\]</span></li>
<li>After the have the reward function, we freeze the parameters <span class="math inline">\(\phi\)</span> and train the <span class="math inline">\(\theta\)</span> by optimization of <span class="math display">\[
\max _{\pi_\theta} \mathbb{E}_{x \sim \mathcal{D}, y \sim \pi_\theta(y \mid x)}\left[r_\phi(x, y)\right]-\beta \mathbb{D}_{\mathrm{KL}}\left[\pi_\theta(y \mid x) \| \pi_{\mathrm{ref}}(y \mid x)\right]
\]</span></li>
</ol>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Tip
</div>
</div>
<div class="callout-body-container callout-body">
<p>The here should be perceived as a probability function. Thus, <span class="math inline">\(\pi_\theta(y|x)\)</span> will output a probability!</p>
<p>Actually, in the case of LLM, we have <span class="math display">\[
\pi_\theta(y|x) = p(y|x; \text{LLM}) = p(y_{0}|x, y)\prod_{i=1}^Tp(y_{1}|x, y_{0,...,i-1}; \text{LLM})
\]</span></p>
</div>
</div>
</section>
<section id="dpo" class="level3">
<h3 class="anchored" data-anchor-id="dpo">DPO</h3>
<p>DPO is another method inspired by the limitation of the PPO. In the case of direct preference of choosing from two results. The human preference distribution <span class="math inline">\(p^*\)</span> can be expressed with reward function: <span class="math display">\[
p^*\left(y_1 \succ y_2 \mid x\right)=\frac{\exp \left(r^*\left(x, y_1\right)\right)}{\exp \left(r^*\left(x, y_1\right)\right)+\exp \left(r^*\left(x, y_2\right)\right)}
\]</span></p>
<p>The DPO paper indicate that we can express the probability under the policy <span class="math inline">\(\pi^*\)</span> with</p>
<p><span class="math display">\[
p^*\left(y_1 \succ y_2 \mid x\right)=\frac{1}{1+\exp \left(\beta \log \frac{\pi^*\left(y_2 \mid x\right)}{\pi_{\mathrm{ref}}\left(y_2 \mid x\right)}-\beta \log \frac{\pi^*\left(y_1 \mid x\right)}{\pi_{\mathrm{ref}}\left(y_1 \mid x\right)}\right)}
\]</span></p>
<p>Therefore, we don‚Äôt need the real PPO now. And we just need to do something like a SFT with a different loss function: <span class="math display">\[
\mathcal{L}_{\mathrm{DPO}}\left(\pi_\theta ; \pi_{\mathrm{ref}}\right)=-\mathbb{E}_{\left(x, y_w, y_l\right) \sim \mathcal{D}}\left[\log \sigma\left(\beta \log \frac{\pi_\theta\left(y_w \mid x\right)}{\pi_{\text {ref }}\left(y_w \mid x\right)}-\beta \log \frac{\pi_\theta\left(y_l \mid x\right)}{\pi_{\text {ref }}\left(y_l \mid x\right)}\right)\right] .
\]</span></p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Tip
</div>
</div>
<div class="callout-body-container callout-body">
<p>During training, the <span class="math inline">\(\pi_{ref}\)</span> is freezed!</p>
</div>
</div>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "Óßã";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>